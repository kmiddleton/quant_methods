---
title: 'Problem Set 10'
author: "Your Name Here: Group X"
date: 'Last updated: `r Sys.Date()`'
output:
  html_document:
    smart: no
    theme: flatly
    toc: true
    toc_float: true
---

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(cowplot)

# This option turn on cacheing of chunks. This will dramatically
# speed up knitting, because only chunks that have changed will
# be recompiled.
knitr::opts_chunk$set(cache = TRUE)
```

## General Framework for This Week's Problem Set

This week we will approach hypothesis testing for several of the models we have fit previously with generalized linear models in a randomization framework. For each, you should:

1. Fit the model to the observed data (feel free to copy your code from original problem set).
1. Decide on a test statistic to use for the randomization.
1. Perform the randomization:
    1. Calculate the test statistic for the *observed* data
    1. Randomly shuffle the observations
    1. Calculate the test statistic for that group
    1. Repeat thousands of times
1. Plot a histogram of the resulting set of test statistics and add a line denoting your observed value
1. Determine the proportion of random combinations resulting in a test statistic more extreme than the observed value ("empirical *P*")

Part of the challenge this week will be figuring out how to implement these steps without step by step guidance, using the tools you've developed so far and the lecture slides  and previous problem sets as a guide. 

Remember to start with a small number of iterations (10 or 100) until you get the code working correctly. Then increase to 10,000 for a final analysis.

## Stalk-eyed flies (PS 05)

Compare the mean eye span on two food sources in stalk-eyed flies. This model is a good candidate for randomization because the variances are likely unequal.

```{r message=FALSE}
# Read data
stalk <- read_csv("../data/Stalkies.csv")

# Some thoughtful plotting
ggplot(stalk, aes(x = food_source, y = eye_span)) +
  geom_point(position = position_jitter(width = 0.05), alpha = 0.5) +
  stat_summary(fun.y = mean, geom = "point", size = 3, color = "red") + 
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.1,
               color = "red", size = 0.7) +
  labs(x = "Food Source", y = "Eye Span")

set.seed(8987324)
nreps <- 1e4
t_stats <- numeric(nreps)
t_stats[1] <- t.test(eye_span ~ food_source, data = stalk)$statistic

for (ii in 2:nreps) {
  t_stats[ii] <- t.test(sample(eye_span) ~ food_source, data = stalk)$statistic
}

data_frame(t_stats) %>% 
  ggplot(aes(t_stats)) +
  geom_histogram() +
  geom_vline(xintercept = t_stats[1], color = "red") +
  labs(x = "t statistics", y = "Count")

2 * mean(t_stats >= t_stats[1])

t.test(eye_span ~ food_source, data = stalk)
```

10^5^ iterations took 124.7 seconds on a single processor. We don't need to run that many iterations to get a good sense that the empirical P-value is going to be very low. The analytical P is 6e-10, so we expect the empirical P to be close to that scale.

We can use parallel processing to speed things up considerably, taking advantage of the fact that we are really just doing the same kind of calculation over and over.

The code below implements a simple parallel loop using some straightforward R packages. Some of this code will look unfamiliar, but we have commented it for you, in case you want to try something like this with your own data. Basically we set things up as we normally do, setting up an empty vector to hold the output and assign the observed value to the first position. But rather than doing a for loop, we will use the combination of `foreach()` and `dopar()`, which handle the parallelization and collection of the results.

```{r}
library(parallel)
library(foreach)
library(doParallel)

t_obs <- t.test(eye_span ~ food_source, data = stalk)

nreps <- 1e4
t_stats <- numeric(nreps)
t_stats[1] <- t_obs$statistic

# Set up parallel processor
n_cores <- 2
cl <- makeCluster(n_cores)
registerDoParallel(cl, cores = n_cores)

# Function to do the randomized t-test and return the t-value only.
t_rand <- function() {
  rnd_t <- t.test(sample(eye_span) ~ food_source, data = stalk)$statistic
  return(as.numeric(rnd_t))
}

# Parallel randomization execute t_rand(), and combine the output into t_stats.
t_stats[2:nreps] <- foreach(i = 2:nreps, .combine = c) %dopar% t_rand()

2 * mean(t_stats >= t_stats[1])

# Close cluster
stopImplicitCluster()
```

The same number of iterations but using 2 processors takes 93 seconds.

```{r}
# Iterations per second, 1 processor
(p1 <- 124.7 / nreps)

# Iterations per second, 6 processors
(p6 <- 93 / nreps)

p1 / p6
```

We get a speed-up of about 1.34 times. There is some overhead in the parallelization, and there were other things going on on the computer, so we don't get double the speed.

## Earwigs (PS 05)

In PS 05, we studied the proportion of earwigs in a sample that have forceps and associated data on the population density of earwigs. Reanalyze these data.

```{r}
EW <- read_csv("../data/Earwigs.csv")

EW %>%
  ggplot(aes(Density, Proportion_Forceps)) +
  geom_point() +
  geom_smooth(method = "lm")

fm <- lm(Proportion_Forceps ~ Density, data = EW)
summary(fm)

set.seed(28476)
nreps <- 1e4
t_stats <- numeric(nreps)
t_stats[1] <- summary(fm)$coefficients[2, 3]

for (ii in 2:nreps) {
  fm_rnd <- lm(sample(Proportion_Forceps) ~ Density, data = EW)
  t_stats[ii] <- summary(fm_rnd)$coefficients[2, 3]
}

data_frame(t_stats) %>% 
  ggplot(aes(t_stats)) +
  geom_histogram() +
  geom_vline(xintercept = t_stats[1], color = "red") +
  labs(x = "t statistics", y = "Count")

2 * mean(t_stats >= t_stats[1])

factorial(7)
```

## Heart Transplant Survivorship (PS 07)

For the data on survivorship after heart transplant, we had to transform the data due to unequal variances. That is not necessary when we use randomization. Reanalyze these data without transformation.

## Bird abundance (PS 07)

We looked at the abundance of bird species in high-altitude "islands" in South America using multiple regression. This is the first model in which we have multiple predictors. In this case, it is usual that you will want to randomize only the outcome variable and retain the associations between the predictors. 

There are four predictors, you will have 5 tests:

1. The overall multple regression (F-test)
2. Area: "Island" size (thousands of square km)
3. Elevation: Elevation (thousands of meters)
4. Dist_to_Ecuador: Distance to Ecuador (km)
5. Dist_to_Island: Distance to nearest island (km)

You should do the randomization and collect the output test statistics using just one loop.

## Aspirin and cancer (PS 08)

In PS 08, we analyzed whether the odds of cancer differed in women, depending on whether they regularly took a low dose of aspirin. Reanalyze these data using randomization of a generalized linear model. This is more challenging, because you have to decide what test statistic to use.

## Correlation

Use the bird abundance data above to test for a significant correlation between `Area` and `Elevation`. Again, you have to decide what test statistic to use.
