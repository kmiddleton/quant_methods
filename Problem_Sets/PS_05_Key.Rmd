---
title: 'Problem Set 05'
author: "Your Name Here: Group X"
date: 'Last updated: `r Sys.Date()`'
output:
  html_document:
    smart: no
    theme: flatly
    toc: true
    toc_float: true
---

```{r setup, message=FALSE, warning=FALSE}
# FIXME
library(tidyverse)
library(readxl)
library(cowplot)
```

## Comparing Models: Differences between group means

Stalk-eyed flies are a clade of flies whose eyes are at the ends of long projections (stalks) on either side of their head. Shortly after emerging from a pupae, these flies ingest air through their mouth in order to pump it into these stalks to elongate them (watch this craziness here: https://www.youtube.com/watch?v=bGX7zZk0Eo4). Females prefer males with wider eye spans. Photo below:

<div style="width:350px">
![](https://i1.wp.com/invorma.com/wp-content/uploads/2015/06/The-Stalk-Eyed-Fly.jpg)
</div>

The `Stalkies.csv` data file contains data for an experiment in which male stalk-eyed flies were reared on different food sources (corn and cotton). The hypothesis is that food type affects eye span. In the next few exercises, you will try to answer this question using different approaches.

### Activity 1

Load the Stalkies.csv file.

```{r}
# FIXME
stalk <- read_csv("../data/Stalkies.csv")
```

Visualize the dataset in two ways:

1. Make a boxplot showing each food type on the x axis.
2. Plot the raw data (points) for the two groups. For the points, include jitter and transparency. Include means and standard errors plotted in a different color. Try a few different values for each of these elements to find a plot you feel best communicates the structure of the data. You will find that the number of data points you have will affect how much jitter & transparency is most appropriate. Change the axis labels to what you would expect on a publication ready figure.

```{r }
ggplot(stalk, aes(x = food_source, y = eye_span)) +
  geom_boxplot()

ggplot(stalk, aes(x = food_source, y = eye_span)) +
  geom_point(position = position_jitter(width = 0.05), alpha = 0.5) +
  stat_summary(fun.y = mean, geom = "point", size = 3, color = "red") +
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.1,
               color = "red", size = 0.7) +
  labs(x = "Food Source", y = "Eye Span")
```

Describe the distributions within each group and between groups. What do these plots tell you? Do you have a prediction for what your statistical tests will show?

> The mean eye span is higher for flies reared on corn than on cotton. The variance also appears to be greater for flies reared on cotton. Based on the standard errors, I predict there will be a significant difference between these groups.

We will first use the analytical method to determine if the group means are significantly different. In this case, a *t*-test is most appropriate.

By default, the `t.test` function will perform a test that does not assume equal variances (called "Welch's Correction"). You may specify equal variances with `var.equal = TRUE` if you decide that is appropriate here.

### Activity 2

Perform a *t*-test using the formula method to specify the groups to test (follow the example in lecture if you need to). Print the results.

```{r }
# FIXME
myT <- t.test(eye_span ~ food_source, data = stalk)
myT
```

What can you conclude from these results?

> The hypothesis that the means for the two groups are equal is very unlikely. There is a statistically significant difference between the groups.

### Activity 3

`t.test()` is a function that performs a very specific task: carrying out a *t*-test (by default with Welch's correction for unequal variances). `lm()` is a general function that fits many many different kinds of linear models. We will talk quite a lot about linear models in the coming weeks.

A *t*-test is a kind of linear model, so we can also fit the same model using `lm()`.

1. Use the code from above, but use `lm()` in place of `t.test()`. Assign this to a new object. Note that `lm()` always assumes equal variances (there is no `var.equal` argument).
2. Pass the object that you just created to the function `summary()`.

```{r}
# FIXME
fm <- lm(eye_span ~ food_source, data = stalk)
summary(fm)
```

Compare the results of the linear model with the results of the *t*-test above. Do you recognize any of the estimates?

> The `(Intercept)` term from `lm()` is equal to the mean for the Corn group. The `food_sourceCotton` estimate is the difference between the means for the Corn and Cotton groups.

### Activity 4

In lecture, we gave the equations for calculating the Bayes Factor (likelihood ratio) to compare two models. This type of comparison is applicable in situations when you have the model likelihoods and gives you an estimate of the relative support of one model to another. In the future, we will use variations of this test to compare models.

For now, we will give you most of the steps along the way. Study carefully, so that you understand what every block of code is doing.

```{r}
# Function for sum of squares
SS <- function(x, x_bar = NULL) {
  # If x_bar is not provided, use the mean of x
  if (is.null(x_bar)) x_bar <- mean(x)

  # Return sum of squares
  return(sum((x - x_bar) ^ 2))
}
```

#### Setup data

```{r}
n1 <- nrow(stalk[stalk$food_source == "Corn", ])
n2 <- nrow(stalk[stalk$food_source == "Cotton", ])

x <- stalk$eye_span
x1 <- stalk$eye_span[stalk$food_source == "Corn"]
x2 <- stalk$eye_span[stalk$food_source == "Cotton"]
```

#### Data drawn from two separate distributions

Assume equal variances but different means.

$$\hat{\sigma}^2_2 = \frac{1}{n_1 + n_2}\left(\sum^{n_1}_{i=1}\left(y_{1i} - \bar{y}_1\right)^2 +  \sum^{n_2}_{i=1} \left(y_{2i} - \bar{y}_2\right)^2 \right)$$

$$\mathcal{L}\left(\hat{\mu}_1, \hat{\mu}_2, \hat{\sigma}^2_2|x\right) = \frac{1}{\left(2\pi \hat{\sigma}^2_2\right)^\left(\frac{n_1 + n_2}{2}\right)}e^{\left(-\frac{n_1 + n_2}{2}\right)}$$

```{r}
s_hat_2 <- (1 / (n1 + n2)) * (SS(x1) + SS(x2))

L_2 <- 1 / (2 * pi * s_hat_2) ^ ((n1 + n2) / 2) * exp(-(n1 + n2) / 2)
log(L_2)
```

#### All observations come from the same distribution

$$\hat{\sigma}^2_1 = \frac{1}{n_1 + n_2}\left(\sum^{n_1}_{i=1}\left(y_{1i} - \bar{\bar{y}}\right)^2 +  \sum^{n_2}_{i=1} \left(y_{2i} - \bar{\bar{y}}\right)^2 \right)$$

where $\bar{\bar{y}}$ is the grand mean.

$$\mathcal{L}\left(\hat{\mu}, \hat{\sigma}^2_1|x\right) = \frac{1}{\left(2\pi \hat{\sigma}^2_1\right)^\left(\frac{n_1 + n_2}{2}\right)}e^{\left(-\frac{n_1 + n_2}{2}\right)}$$

```{r}
s_hat_1 <- (1 / (n1 + n2)) * (SS(x1, mean(x)) + SS(x2, mean(x)))
L_1 <- 1 / ((2 * pi * s_hat_1) ^ ((n1 + n2) / 2)) * exp(-((n1 + n2) / 2))
log(L_1)
```

#### Likelihood ratio (Bayes Factor)

The equation for Bayes Factor can be calculated in two ways:

$$BF = \frac{\mathcal{L}(Model~2)}{\mathcal{L}(Model~1)} = \left(\frac{\hat{\sigma}^2_2}{\hat{\sigma}^2_1}\right)^{-\frac{n_1 + n_2}{2}}$$

Use the calculations we have given you above to calculate the Bayes Factor for model 2 (the data come from distributions with different means) vs. model 1 (the data come from a single distribution with one mean).

```{r}
# FIXME
L_2 / L_1
(s_hat_2 / s_hat_1) ^ (-(n1 + n2) / 2)
```

What do you conclude based on the Bayes Factor comparison?

> There is overwhelming support for the two mean model.

We have actually violated one of the assumptions of the likelihood calculations that we did. What is it?

> The variances are likely not equal.

## Confidence intervals

We will use the same data to explore confidence intervals.

### Activity

Calculate a ~95% confidence interval for each mean (cotton & corn groups). You can use the approximate formula discussed in lecture mu +/- 2 * SEM.

```{r}
# FIXME
mean(stalk$eye_span[stalk$food_source == 'Cotton']) -
  2 * sd(stalk$eye_span[stalk$food_source == 'Cotton'] /
           sqrt(length(stalk$eye_span[stalk$food_source == 'Cotton'])))

mean(stalk$eye_span[stalk$food_source == 'Cotton']) +
  2 * sd(stalk$eye_span[stalk$food_source == 'Cotton'] /
           sqrt(length(stalk$eye_span[stalk$food_source == 'Cotton'])))

mean(stalk$eye_span[stalk$food_source == 'Corn']) -
  2 * sd(stalk$eye_span[stalk$food_source == 'Corn'] /
           sqrt(length(stalk$eye_span[stalk$food_source == 'Corn'])))

mean(stalk$eye_span[stalk$food_source == 'Corn']) +
  2 * sd(stalk$eye_span[stalk$food_source == 'Corn'] /
           sqrt(length(stalk$eye_span[stalk$food_source == 'Corn'])))

## We can write functions to do the SEM and approximate CI
## approx_CI returns a tibble with the lower and upper
## bounds of the CI. Notice that the "do" function can take
## the tibble from approx_CI and turn it into a tibble
## with both CI bounds. This is a general way to handle
## summarizing from functions that return multiple variables.
SEM <- function(x) return(sd(x) / sqrt(length(x)))

approx_CI <- function(x) {
  upper <- mean(x) + 2 * SEM(x)
  lower <- mean(x) - 2 * SEM(x)
  return(tibble(lower, upper))
}

stalk %>%
  group_by(food_source) %>%
  do(approx_CI(.$eye_span))
```

We can simulate what we would expect assuming the means and standard deviation are representative of the population parameters. Do the following:

1. Generate simulated data from a normal distribution matching the number of observations, mean, and standard deviation (note sd not SEM) for each group.
2. Calculate the mean for each group and the upper and lower bounds of the confidence interval.
3. Put these values into a `tibble`
4. Repeat this 1000 times
5. Calculate the proportion of times the mean for each group falls within the confidence interval


```{r}
set.seed(648273)
niter <- 1000

mu.corn <- mean(stalk$eye_span[stalk$food_source == 'Corn'])
mu.cotton <- mean(stalk$eye_span[stalk$food_source == 'Cotton'])
sd.corn <- sd(stalk$eye_span[stalk$food_source == 'Corn'])
sd.cotton <- sd(stalk$eye_span[stalk$food_source == 'Cotton'])
n.corn <- length(stalk$eye_span[stalk$food_source == 'Corn'])
n.cotton <- length(stalk$eye_span[stalk$food_source == 'Cotton'])

corn.cis <- tibble(mm = numeric(length = niter),
                       ll = numeric(length = niter),
                       uu = numeric(length = niter))
cotton.cis <- tibble(mm = numeric(length = niter),
                         ll = numeric(length = niter),
                         uu = numeric(length = niter))

for (jj in 1:niter) {
  corn.s <- rnorm(n.corn, mu.corn, sd.corn)
  cotton.s <- rnorm(n.cotton, mu.cotton, sd.cotton)

  corn.cis[jj, ] <- c(
    mean(corn.s),
    mean(corn.s) - 2 * (sd(corn.s) / sqrt(n.corn)),
    mean(corn.s) + 2 * (sd(corn.s) / sqrt(n.corn)))
  cotton.cis[jj, ] <- c(
    mean(cotton.s),
    mean(cotton.s) - 2 * (sd(cotton.s) / sqrt(n.cotton)),
    mean(cotton.s) + 2 * (sd(cotton.s) / sqrt(n.cotton)))
}

(sum(mu.corn < corn.cis$ll) + sum(mu.corn > corn.cis$uu)) / niter
(sum(mu.cotton < cotton.cis$ll) + sum(mu.cotton > cotton.cis$uu)) / niter
```

What do you conclude based on this simulation?

> The true mean is within about 95% of the resampled confidence intervals.

Is it correct to say that there is a 95% probability that the true mean is within a 95% confidence interval? Why or why not? Think about what you just showed with your simulation.

> No, it is not correct. CIs tell us about a range of values that we have some confidence in for future samples.

## OLS Regression

The file `Earwigs.csv` contains data for the proportion of earwigs in a sample that have forceps and associated data on the population density of earwigs. An earwig with forceps is shown below.

<div style="width:350px">
![](http://1.bp.blogspot.com/-n4ViM3mLJA4/Tps-JoQ4gJI/AAAAAAAACbw/aRFxy7vAM9o/s1600/IMG_0346.jpg)
</div>

The hypothesis is that earwigs living at higher densities will be more likely to have forceps, so that samples living at higher densities will have a higher proportion with forceps.

### Activity

Load the earwigs data:

```{r}
# FIXME
EW <- read_csv("../data/Earwigs.csv")
```

Make a scatterplot of the proportion of earwigs with forceps vs. density.

```{r}
# FIXME
EW %>%
  ggplot(aes(Density, Proportion_Forceps)) +
  geom_point()
```

Use the following code to add an OLS regression line through the points to your plot: `+ geom_smooth(method = "lm")`. Just add that code after `geom_point()` in your call to ggplot.

```{r}
# FIXME
EW %>%
  ggplot(aes(Density, Proportion_Forceps)) +
  geom_point() +
  geom_smooth(method = "lm")
```

Use `lm()` to fit a linear model in which the proportion of earwigs with forceps is modeled by the density of earwigs. Assign the linear model to an object, and then pass that object to `summary()` as you did above.

```{r}
fm <- lm(Proportion_Forceps ~ Density, data = EW)
summary(fm)
```

See if you can figure out what the coefficients represent. You will find estimates for the slope and intercept of the line, their respective standard errors,^[remember that standard errors tell us about the uncertainty in parameter estimates] and a *t*-test, which tests each parameter estimate vs. 0. Try to figure out how the *t*-value is calculated (divide one thing by another).

> The OLS line is Proportion_Forceps = 0.07 + 0.013(Density). The standard errors are 0.18 and 0.008. The *t*-statistics are the estimates divided by their SEs. Neither the intercept nor the slope is significantly different from 0.

The data we gave you here is a proportion. What is another format that the data could have been in? Think about the source of the portion values.

> A proportion could also be represented as a binomial (where "has forceps" is a "success").
