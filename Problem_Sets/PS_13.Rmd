---
title: 'Problem Set 13'
author: "Your Name Here"
date: 'Last updated: `r Sys.Date()`'
output:
  html_document:
    smart: no
    theme: flatly
---

```{r setup, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
```

## Exploring PCA 

The code below generates data from a multivariate normal distribution with a specified correlation coefficient (`rho`). You can specify means for the variables (`mu`), and their standard deviations `sigma`, as well as the number of points to generate (`n`). The function `mvrnorm()` generates the data, taking `n` and `Sigma` (the covariance matrix) as parameters. You should be able to follow the rest of the code. `mvrnorm()` returns a matrix, which, when converted to a `data.frame` gets variables `V1` and `V2`.

```{r}
set.seed(10)  # Set the random number generator seed
rho <- 0.5    # Desired correlation coefficient
mu <- c(0, 0) # Means for V1 and V2
sigma <- 1    # Standard deviations of V1 and V2
n <- 20       # Number of data points to simulate

# Construct the covariance matrix
Sigma <- matrix(c(sigma^2, rho*sigma^2, rho*sigma^2, sigma^2), 2)

# Simulate the data
x <- MASS::mvrnorm(n, mu, Sigma, empirical = TRUE)
ggplot(as.data.frame(x), aes(V1, V2)) + geom_point()

# Perform PCA
# Because x is a matrix, we don't need a formula
# Note center = TRUE is the default but scale. = TRUE is not
# to make the results compatible with S. You should always 
# use scale. = TRUE or manually scale your variables.
z <- prcomp(x, center = TRUE, scale. = TRUE)
summary(z)
```

### Activity

Rerun the code above, changing different parameters as you go, to answer the following questions, which are designed to allow you to explore the relationship between data and PCA. You should be able to just change values and then execute the whole chunk.

Keeping `rho`, `sigma`, and `n` constant, how does changing `mu` influence the proportion of variance accounted for by `V1` and `V2`? Briefly explain.

> 

Keeping `rho`, `mu`, and `sigma` constant, how does changing `n` influence the proportion of variance accounted for by `V1` and `V2`? Briefly explain.

> 

Keeping `rho`, `mu`, and `n` constant, how does changing `sigma` influence the proportion of variance accounted for by `V1` and `V2`? Briefly explain.

> 

Keeping `mu`, `sigma`, and `n` constant, how does changing `rho` influence the proportion of variance accounted for by `V1` and `V2`? Be sure to explore the full possible range of `rho`. Briefly explain.

> 

## Return of the Sole

In Week 8, we looked at presence/absence data for the common sole ([*Solea solea*](https://en.wikipedia.org/wiki/Common_sole)) predicted by different factors such as the salinity of the water and the season. This week we will focus on the composition of the substrate.  

### Activity

Load the sole data from the file `Sole.xlsx`. Several of the variables are categorical and should be converted into factors: season, month, area, and Solea_solea. Go ahead and do this now. As a final step, convert your object with the data into a `data.frame` with `as.data.frame()`.

```{r}

```

The columns `gravel`, `large_sand`, `med_fine_sand`, and `mud` represent their proportional concentrations in the substrate (they sum to 100). Calculate a composite score for "substrate". Use principal components analysis on these columns.

```{r}

```

Use the `summary()` and `print()` methods to determine the percent of variance accounted for by each PC and the relative loadings of each variable.

```{r}

```

Fit a logistic regression where presence of sole is predicted by the first PC from the PCA above. Refer to the lecture slides if you need to.

```{r}

```

Try to interpret the output of the summary of the fitted model from part d. Why kind of substrate do sole appear to prefer?

```{r}

```

> 

## Egyptian skulls

The file `Egyptian_Skulls.xlsx` contains 4 measurements of 150 skulls from 5 time periods of Egypt from Thompson and Randall-Maciver (1905). Here is an image of the measurements that we found online (with typos):

![](http://i.imgur.com/KrKDPGX.jpg)

We will explore this data set and try to predict some of the measurements.

Thompson, A. and Randall-Maciver, R. (1905) *Ancient races of the Thebaid: Being an Anthropometrical Study of the Inhabitants of Upper Egypt from the Earliest Prehistoric Times to the Mohammedan Conquest, Based Upon the Examination of Over 1,500 Crania*. Oxford University Press.

### Activity

Load the file. Convert `Time_Period` to a factor. Group by `Time_Period` and use `tally()` to confirm that there are 30 skulls per time period. Also, relevel the time periods so they go in chronological order (if you convert to factor with `forcats::fct_inorder()`, this will happen automagically).

```{r}

```

Use MANOVA to predict `Maximum_Breadth`, `Basibregmatic_Height` `Basialveolar_Length`, and `Nasal_Height` from `Time_Period` using the `lm()` function (see the lecture notes). 

```{r}

```

Generate the MANOVA table with `Anova()` using type III sums of squares.

```{r}

```

What is your interpretation of the results of the MANOVA?

> 

We can also flip the analysis around and try to classify skulls by time period by their measurements. Use discriminant function analysis (linear discriminant analysis) to classify skulls by time period. See the lecture notes.

```{r}

```

Calculate the percent correct classification for the DFA using the full dataset.

```{r}

```

Generate the confusion matrix for the classification. Are there any patterns as to which time periods are best or worst predicted?

```{r}

```

> 

Now use the `CV=TRUE` option to perform leave-one-out cross validation (see the help for `lda()`). 

```{r}

```

How does the percent correct compare to using the full data set to estimate this quantity? Explain why you observe a difference.

> 

## Pottery

The file `pottery.xslx` contains data on the chemical composition (oxides) of 45 samples of Romano-British pottery as well as a column (`kiln`) to indicate the source of the pottery.

A. Tubb and N. J. Parker and G. Nickless (1980), The analysis of Romano-British pottery by atomic absorption spectrophotometry. *Archaeometry*, 22, 153-171.

### Activity

Load the file `pottery.xslx` and check the structure, and create a new object having dropped the column `kiln`. This will leave you with 9 columns of data in your new object. We will return to the `data.frame` with `kiln` later.

```{r}

```

Calculate the principal components using centered and scaled data.

```{r}

```

Using the `summary()` output: (1) what percentage of the variance does PC1 account for? (2) What percentage of the variation do the first 5 PCs cumulatively account for?

```{r}

```

> 

Using the `print()` method for a `prcomp` object, (1) what variables load positively on PC1? (2) What variables load negatively on PC1? (3) What variables have minimal loading on PC1?

```{r}

```

> 

Make a biplot of PC2 vs. PC1. What variables have similar loadings on these two axes?

```{r}

```

> 

Return to the full dataset including the `kiln` column. Convert `kiln` to a factor.

```{r}

```

Perform discriminant function analysis on the kiln column predicted by the first 9 variables. Calculate the percent correct classification for the full set (as above).

```{r}

```

Calculate the confusion table for the classification.

```{r}

```

Use the `CV = TRUE` option to perform cross validation and calculate the % correct and confusion table as in question 2, part h. 

```{r}

```

A piece of pottery is found, but it's source is unknown. Chemical analysis reveals the following composition. Predict the source kiln it came from. Which kiln has the highest posterior probability? Use the `lda()` performed without `CV=TRUE` as that changes the structure of the output and won't work with `predict()`. 

- Al2O3: 12.1
- Fe2O3: 5.61
- MgO: 5.55
- CaO: 0.16
- Na2O: 0.19
- K2O: 4.45
- TiO2: 0.71
- MnO: 0.991
- BaO: 0.016

```{r}

```

> 
