---
title: 'Problem Set 11'
author: "Your Name Here: Group X"
date: 'Last updated: `r Sys.Date()`'
output:
  html_document:
    smart: no
    theme: flatly
    toc: true
    toc_float: true
---

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(cowplot)

theme_set(theme_cowplot())
```


## Repeatability: Intraclass correlation coefficient

The model you use to calculate the intraclass correlation coefficient *is* a multilevel model. You have repeated measurements for a single specimen. Those measurements are obviously not independent from one another. The major question you want to ask is: What is the relative between *measurement* vs. between *specimen* variation?

We will fit this model in two ways: using `aov()` and using the `ICC` package. 

The data we will be working with 

https://imgur.com/CVTtMkJ.png

### Activity

Load the data in `WalkingStickFemurs.csv` 

```{r echo=FALSE}
M <- read_csv("../data/WalkingStickFemurs.csv") %>% 
  mutate(Specimen = factor(Specimen))

M_wide <- M %>% 
  pivot_wider(id_cols = Specimen, names_from = Measurement,
              values_from = Femur_length, names_prefix = "Measure_")
```

```{r}
ggplot(M_wide, aes(Measure_1, Measure_2)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
  coord_equal()
```

Based on your plot, do you expect repeatability to be high or low? Give an estimate?

> The points are very close to the line, so repeatability should be large (> 0.9).

Follow the example from lecture to calculate repeatability of the femur length  measurement using `aov()`.

```{r}
fm <- summary(aov(Femur_length ~ Specimen, data = M))

var_a <- (0.002464 - 0.000356) / 2
var_a / (var_a + 0.000356)
```

Calculate repeatability of the femur length (`Femur_length`) measurement using `ICCest()` from the `ICC` package (you might need to install the package).

```{r}
ICCest(x = M$Specimen, y = M$Femur_length)
```

Compare the results

> The numbers are very similar.



```{r}
## FIXME
# Data preparation
set.seed(28462)

quant.norm <- function(y) {
  nn <- qqnorm(y, plot.it = FALSE)
  return(nn$x)
}

sid <- read_csv("../data/Butteryfly_IDs.csv",
                col_names = c("ID", "Treatment", "Sex", "Population"))
sid <- sid %>%
  mutate(Treatment = str_replace(Treatment, "treatment: ", ""),
         Sex = str_replace(Sex, "Sex: ", ""),
         Population = str_replace(Population, "population: ", ""),
         ID2 = str_split(ID, "_", simplify = TRUE)[, 2],
         ID = str_split(ID, "_", simplify = TRUE)[, 1],
         ID = str_replace_all(ID, "-", "_")) %>%
  filter(ID2 == 450) %>%
  dplyr::select(-ID2)

gg <- read_delim("../data/Butterfly_NormalizedCountsGenes.txt",
                 delim = "\t")
samp.names <- colnames(gg)[-1]
gene.names <- gg$gene
gg <- t(gg[, -1])
rr.temp <- row.names(gg)
gg <- apply(gg, 2, function(x) quant.norm(x))
colnames(gg) <- gene.names
rownames(gg) <- rr.temp
gg <- bind_cols(
  tibble(ID = str_replace_all(row.names(gg), "-", "_")),
  as.data.frame(gg))

ngenes <- 100
gg_samp <- gg[, c(1, sample(size = ngenes, x = 2:ncol(gg)))]

M <- left_join(sid, gg_samp)
M <- M[,-c(2,4)]

write_csv(M, path = "../data/Butterfly_Gene_Expression.csv")
```

```{r}
getP <- function(fm) {
  sum.set <- summary(fm)
  p.set <- lapply(sum.set, function(x) x[['coefficients']][2, 4])
  return(unlist(p.set))
}
```

## RNAseq (gene expression) on males and females of the Glanville Fritillary

Glanville Fritillary butterflies are a model organism for studying dispersal and metapopulation dynamics, because they live in isolated meadows.

<div style="width:350px">
![](http://i1.treknature.com/photos/1289/melitaea_cinxia.jpg)
</div>

### Activities

Load in the Butterfly Gene Expression data (`Butterfly_Gene_Expression.csv`). This is a `data.frame` of sample id, sex, and 100 normalized gene expression measures (this is trimmed down from an original over 8,000 expression measures). We will perform 100 regressions using `lm()`, predicting each gene expression measure from the sex variable. The `lm()` function can fit multiple Y's at once. See the example code below and follow the same procedure to fit all 100 models at once.

```{r}
ex.dat <- data.frame('treatment' = rep(c('a', 'b'), each = 20),
                     'yy1' = rnorm(40),
                     'yy2' = rnorm(40))

treat <- ex.dat$treatment
YYs <- as.matrix(ex.dat[, 2:3])

mods <- lm(YYs ~ treat)
summary(mods)

#FIXME
M <- read_csv("../data/Butterfly_Gene_Expression.csv")
Ys <- as.matrix(M[, 3:ncol(M)])
fm <- lm(Ys ~ Sex, data = M)
```

Note this method produces a list of `lm()` results. We have written a function (`getP()` see above) to extract the relevant P-value from a list of this type. Do this for your data and assign it to an object, `obsP`. Visualize your P-values by plotting a histogram and a q-q plot. For the q-q plot, transform your P-values to -log10(P-values) and sort them from smallest to largest. Then, generate expected values from the uniform distribution using `runif()` and again transform and sort them. Now plot observed versus expected values and add a 1:1 line (intercept = 0, slope = 1).

```{r}
ex.P <- getP(mods)

#FIXME
set.seed(273645)
obsP <- data.frame('P' = getP(fm))

ggplot(obsP, aes(P)) +
  geom_histogram(bins = 20)

qqP <- data.frame('observed' = sort(-log10(obsP$P)),
                  'expected' = sort(-log10(runif(length(obsP$P), 0, 1))))

ggplot(qqP, aes(x = expected, y = observed)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1)
```

Based on these visualizations, do you think there are true positives in this data set?

> Probably. The P-values deviate pretty strongly from the expected line.

Use R's built-in `p.adjust()` function to adjust the P-values using the sequential Bonferroni correction (Holmes procedure). You don't have to sort the P-values first, but it can make it easier to pick out ones that remain significant. Print a `data.frame` with the observed P-value and sequential Bonferroni corrected P-value. Finally, count the number of P-values that are less than 0.05.

```{r}
# FIXME
seq_bonf <- p.adjust(obsP$P, method = "holm")
tibble(P = obsP$P,
           seq_bonf) %>%
  arrange(P) %>%
  filter(seq_bonf <= 0.05)
sum(seq_bonf < 0.05)
```

Repeat the procedure above, but now use the false discovery rate procedure of Benjamini and Hochberg.

```{r}
# FIXME
fdr <- p.adjust(obsP$P, method = "fdr")
tibble(P = obsP$P,
           fdr) %>%
  arrange(P) %>%
  filter(fdr <= 0.05)
sum(fdr < 0.05)
```

Follow the code in lecture to carry out a pFDR analysis of these P-values. You will probably need to install the `qvalue` package, which is available through Bio conductor (rather than CRAN).

```{r}
library(qvalue)
qobj <- qvalue(obsP$P, fdr.level = 0.05, pi0.method = "smoother")
summary(qobj)
```

What is the value of $\pi_0$? What does this value represent?

> 0.39. This means that only 39% of the null hypotheses are true.

How many P-values are associated with q-values less than 0.05?

> 29 (of 32 nominally less than 0.05)

What is an adjusted $\alpha$-level that will control pDFR at 0.05?

```{r}
max(qobj$pvalues[qobj$qvalues <= 0.05])
```

Perform a randomization to estimate the number of expected false positives for this experiment. 1) You will want to keep the correlation structure of the gene expression measures. An easy way to do this is to shuffle the sex labels and leave the expression measures the same. 2) Perform your set of 100 tests in the same way that you did above. 3) Collect the associated p values using the `getP` function. 4) Keep these in a 1000 x 100 matrix with each column as one iteration. This process will take a few minutes to run. You may want to test the steps with a small number of iterations first.

```{r randomization, cache=TRUE}
#FIXME
set.seed(874628)
niter <- 1000

Ys <- as.matrix(M[, 3:ncol(M)])
Sex <- M$Sex

allP <- matrix(NA, ncol(Ys), niter)

for (kk in 1:niter) {
  fms <- lm(Ys ~ sample(Sex))
  allP[, kk] <- getP(fms)
}
```

Visualize the P-values from one or a few of your iterations as you did above with a histogram and a q-q plot.

```{r}
it1 <- data.frame('P' = allP[, 5])
ggplot(it1, aes(P)) +
  geom_histogram(bins = 20)

qqP <- data.frame('observed' = sort(-log10(it1$P)),
                  'expected' = sort(-log10(runif(100, 0, 1))))

ggplot(qqP, aes(x = expected, y = observed)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1)
```

For each iteration, get a count of the number of positives at a threshold of 0.05. Visualize the number of positives with a histogram. Keep in mind, because you have randomized the data, *all positives here are false postives*.

- Calculate the average number of false positives and the average false positive rate at a threshold of 0.05.
- Now calculate the Family-Wise Error Rate (FWER) at a threshold of 0.05 by calculating the proportion of iterations that result in at least one significant result.
- Use your average estimated number of false positives to calculate the false discovery rate at a threshold of 0.05. Note you will need to consider your observed number of positives for this calculation.


```{r}
th <- 0.05

pos.counts <- data.frame(
  'counts' = apply(allP, 2, function(x) sum(x < th)))

ggplot(pos.counts, aes(counts)) +
  geom_histogram(bins = 20)

mean(pos.counts$counts)

mean(pos.counts$counts) / 100

sum(pos.counts >= 1) / 1000

mean(pos.counts$counts) / sum(obsP$P < th)
```

Now use this same procedure to calculate the false positive rate, FWER, and false discovery rate at the range of thresholds given below. Store your output (including the threshold) in a `data.frame`.

```{r}
thresholds <- seq(0.0005, 0.05 , length = 200)

#FIXME
output <- data.frame('threshold' = thresholds,
                      'FP' = numeric(length(thresholds)),
                     'FWER' = numeric(length(thresholds)),
                     'FDR' = numeric(length(thresholds)))

for (tt in 1:length(thresholds)) {
  th <- thresholds[tt]
  pos.counts <- data.frame(
    'counts' = apply(allP, 2, function(x) sum(x < th)))
  output[tt,'FP'] <- mean(pos.counts$counts) / 100
  output[tt,'FWER'] <- sum(pos.counts >= 1) / 1000
  output[tt,'FDR'] <- mean(pos.counts$counts) / sum(obsP$P < th)
}
output
max(output$threshold[output$FDR <= 0.05])
```

Make plots of the FDR, FP, and FWER plotted against the threshold (feel free to make three separate plots). In each, draw a horizontal line at 5%.

```{r}
#FIXME
output %>%
  ggplot(aes(threshold, FDR)) +
  geom_point() +
  geom_hline(yintercept = 0.05)

output %>%
  ggplot(aes(threshold, FP)) +
  geom_point() +
  geom_hline(yintercept = 0.05)

output %>%
  ggplot(aes(threshold, FWER)) +
  geom_point() +
  geom_hline(yintercept = 0.05)
```

Inspecting the `data.frame` you made above, what P-value threshold should you use if you want a false discovery rate of 5%?

>

Calculate the FWER corresponding to an alpha of 5% by calculating the lowest P-value for each iteration. Then, use the `quantile()` function to calculate the 5% quantile. hint: you will want to use `apply()` here.

```{r}
minP <- apply(allP, 2, min)
quantile(minP, 0.05)
```


## Planning experiments

Consider that you are planning some experiments, use the `pwr` package to calculate the unknown quantity for each of the following situations. Assume that $\alpha$ = 0.05 for all tests.

### Activity

Use `cohen.ES()` to look up the effect size for a "small" effect for a *t*-test.

```{r}
# FIXME
cohen.ES(test = "t", size = "small")
```

Calculate the sample size (*n*) needed in each group of a two-sample *t*-test with power = 0.80 to detect a small effect (use the effect size from above).

```{r}
# FIXME
pwr.t.test(d = 0.2, power = 0.80, type = "two.sample")
```

Repeat the test above but for a paired *t*-test.

```{r}
# FIXME
pwr.t.test(d = 0.2, power = 0.80, type = "paired")
```

Calculate the number of observations for a correlation test where you estimate a correlation coefficient of 0.6. Power should be 0.80.

```{r}
# FIXME
pwr.r.test(r = 0.6, power = 0.8)
```

Calculate the power for a correlation test where you estimate the correlation coefficient to be 0.4, for a sample size of 15.

```{r}
# FIXME
pwr.r.test(r = 0.4, n = 15)
```


## Calcium concentration in bird plasma

The file `Bird_Plasma.xlsx` contains factorial data on blood plasma calcium concentration (`Calcium`, mg / 100 mL) in male and female birds (`Sex`) each of which was treated or not with a hormone (`Hormone`).

### Activity

Load the Excel file, and convert treatment and sex to factors.

```{r}
# FIXME
M <- read_excel("../data/Bird_Plasma.xlsx")
M <- M %>% mutate(Treatment = factor(Treatment),
                  Sex = factor(Sex))
```

Use `group_by()` and `tally()` to determine the sample size in each of the groups.

```{r}
# FIXME
M %>% group_by(Treatment, Sex) %>% tally()
```

Assume that the researchers who carried out this study had performed a power analysis prior to collecting data. Calculate the minimum effect size for a main effect they could detect with their experimental design if they use an ANOVA to analyze the data. Assume that there are 4 groups and power is 0.80.

```{r}
# FIXME
pwr.anova.test(k = 4, n = 5, power = 0.80)
```

How do you interpret the minimum effect size that the researchers could detect at power of 0.8?

> The effect has to be really large (> 0.8 standard deviations) in order to detect it.


## Power via simulation

A more appropriate way to analyze the bird plasma data is to use a factorial ANOVA (both main effects and their interaction term). Imagine that you are planning just such an experiment:

- Two treatment groups which each have two levels (a 2 X 2 design)
- n = 5 in each group
- Your alpha will be 0.05
- You are interested in detecting main effects and interactions (full factorial model)

We will simulate data where the main effect of one term varies and the interaction effect varies, figure out the power across a range of different means, and make the assumption that within-group standard deviations are equal at 4 and the main effect of the other variable does not change (for now). We will build up this simulation in steps, first considering one parameter combination and one iteration before setting up the entire simulation.

a. Generate two vectors, which will represent the range of differences in treatment and the range of differences for the interaction. For now, test the sequence from -10 to 10, with a length of 10 (`length.out = 10`).

The `tidyr` package, which is loaded with `tidyverse` has a handy function `crossing()`, which will expand vectors with all pairwise combinations into a `tibble`. Use this function to cross the vectors. You should have a 100 X 2 `tibble`.

```{r}
# FIXME
low <- -10
high <- 10
length.out <- 10

diffs_tx <- seq(low, high, length.out = length.out)
diffs_int <- seq(low, high, length.out = length.out)

pwr_sim <- crossing(diffs_tx, diffs_int)
```

Use `mutate()` at add two additional `numeric()` columns, one each for the power to detect a treatment effect and the sex X treatment interaction.

```{r}
# FIXME
pwr_sim <- pwr_sim %>% 
  mutate(tx = numeric(length = nrow(pwr_sim)),
         sex_tx = numeric(length = nrow(pwr_sim)))
```

b. Let's start by considering a single parameter combination: the first row of your `tibble` from above. First we will calculate the means for the four groups defined by sex and treatment (you might call these `s0t0`, `s1t0`, `s0t1`, `s1t1`). Assign `s0t0` to have a mean of zero. Because we are focusing only on treatment, we will not simulate a main effect of sex so assign `s1t0` to also have a mean of zero. For the effect of treatment, because we are also simulating an interaction, we need to consider both when assigning the means. That is what an interaction is, the effect of one term is dependent on the state of the other. We will add the effect of the main effect and the interaction effect which will be conditional on sex. So, for `s0t1`, add the effect for treatment to 0.5 multiplied by the effect of the interaction. And for `s1t1`, add the effect for treatment to -0.5 multiplied by the effect of the interaction. This will create a set of means in which there is an overall main effect for treatment, no main effect for sex, and an interaction effect.

Confirm this by calculating the values for your means.  

```{r}
# FIXME
s0t0 <- 0
s1t0 <- 0
s0t1 <- as.numeric(pwr_sim[1, 1] + 0.5 * pwr_sim[1, 2])
s1t1 <- as.numeric(pwr_sim[1, 1] + -0.5 * pwr_sim[1, 2])

print(c(s0t0, s1t0, s0t1, s1t1))
```

c. Now that you have code to assign the means for each group, we can move on to simulating one iteration for this parameter combination. We want to generate data for each group from a random normal distribution. All have the same standard deviation (4). Use the sample sizes for the groups defined above. Set up a `data.frame` with these data and labels for the groups (think about how `lm()` will need the data to be formatted). Then we will fit the ANOVA using `anova(lm())` and assign a TRUE/FALSE for the treatment term and interaction term that will tell us whether it is significant or not at our alpha level.  

```{r warning=FALSE}
# FIXME
s0t0 <- 0
s1t0 <- 0
s0t1 <- as.numeric(pwr_sim[10, 1] + 0.5 * pwr_sim[10, 2])
s1t1 <- as.numeric(pwr_sim[10, 1] + -0.5 * pwr_sim[10, 2])
stdev <- 4
nn <- 5
alpha <- 0.05
  
y_sex0_tx0 <- rnorm(nn, mean = s0t0, sd = stdev)
y_sex1_tx0 <- rnorm(nn, mean = s1t0, sd = stdev)
y_sex0_tx1 <- rnorm(nn, mean = s0t1, sd = stdev)
y_sex1_tx1 <- rnorm(nn, mean = s1t1, sd = stdev)
    
D <- tibble(Calcium = c(y_sex0_tx0, y_sex1_tx0,
                        y_sex0_tx1, y_sex1_tx1),
            sex = factor(rep(c("F", "M", "F", "M"), each = nn)),
            tx = factor(rep(c("None", "Hormone"), each = nn * 2)))

fm <- anova(lm(Calcium ~ sex * tx, data = D))
fm

p_tx <- fm$`Pr(>F)`[2] < alpha
p_sex_tx <- fm$`Pr(>F)`[3] < alpha
```

d. You now have all of the code you need to do the simulation. You just need to set it up with two loops. You want to loop through each of your parameter combinations. Then, for each combination, you want to loop through several iterations. First set this up with a small number of iterations. Then scale up to 1000.

Use `Sys.time()` to get a sense for how long your program will run when you scale up. Start by assigning the output of `Sys.time()` to a variable before you loop starts (`t_start <- Sys.time()`). After your loop, subtract that from `Sys.time()` (e.g., `Sys.time() - t_start`).

```{r}
set.seed(2238746)
niter <- 100
alpha <- 0.05
nn <- 5
stdev <- 4

t1 <- Sys.time()

for (ii in 1:nrow(pwr_sim)) {
  s0t0 <- 0
  s1t0 <- 0
  s0t1 <- as.numeric(pwr_sim[ii, 1] + 0.5 * pwr_sim[ii, 2])
  s1t1 <- as.numeric(pwr_sim[ii, 1] + -0.5 * pwr_sim[ii, 2])
  
  for (jj in 1:niter) {
    
    y_sex0_tx0 <- rnorm(nn, mean = s0t0, sd = stdev)
    y_sex1_tx0 <- rnorm(nn, mean = s1t0, sd = stdev)
    y_sex0_tx1 <- rnorm(nn, mean = s0t1, sd = stdev)
    y_sex1_tx1 <- rnorm(nn, mean = s1t1, sd = stdev)
    
    D <- tibble(Calcium = c(y_sex0_tx0, y_sex1_tx0,
                            y_sex0_tx1, y_sex1_tx1),
                sex = factor(rep(c("F", "M", "F", "M"), each = nn)),
                tx = factor(rep(c("None", "Hormone"), each = nn * 2)))
    
    fm <- anova(lm(Calcium ~ sex * tx, data = D))
    
    p_tx[jj] <- fm$`Pr(>F)`[2] < alpha
    p_sex_tx[jj] <- fm$`Pr(>F)`[3] < alpha
  }
  pwr_sim$tx[ii] <- mean(p_tx)
  pwr_sim$sex_tx[ii] <- mean(p_sex_tx)
}
Sys.time() - t1

save(pwr_sim, file = "pwr_sim.Rda")
```

e. Now we want to visualize the results. Make separate plots of with your sets of differences for the treatment and for the interaction on the x and y axes. Color the points by the power associated with the treatment effect in the first plot and by the power associated with the interaction effect in the second. The output should look like a grid of points (one for each combination of values) that are color coded by power. This is a kind of manual heatmap.

```{r}
load("pwr_sim.Rda")

ggplot(pwr_sim, aes(x = diffs_tx, y = diffs_int, color = tx)) +
  geom_point(size = 6)

ggplot(pwr_sim, aes(x = diffs_tx, y = diffs_int, color = sex_tx)) +
  geom_point(size = 6)

ggplot(pwr_sim, aes(x=diffs_tx, y=tx)) +
  geom_point()

ggplot(pwr_sim, aes(x=diffs_int, y=sex_tx)) +
  geom_point()
```

