---
title: 'Problem Set 06'
author: "Your Name Here: Group X"
date: 'Last updated: `r Sys.Date()`'
output:
  html_document:
    smart: no
    theme: flatly
    toc: true
    toc_float: true
---

```{r setup, message=FALSE, warning=FALSE}
```

<!--
Datasets
  Stalkies.csv
  Earwigs.csv
-->

## Comparing Models: Differences between group means

Stalk-eyed flies are a clade of flies whose eyes are at the ends of long projections (stalks) on either side of their head. Shortly after emerging from a pupae, these flies ingest air through their mouth in order to pump it into these stalks to elongate them (watch this craziness here: https://www.youtube.com/watch?v=bGX7zZk0Eo4). Females prefer males with wider eye spans. Photo below:

<div style="width:350px">
![](https://i.imgur.com/7KxZKQF.jpg)
</div>

The `Stalkies.csv` data file contains data for an experiment in which male stalk-eyed flies were reared on different food sources (corn and cotton). The hypothesis is that food type affects eye span. In the next few exercises, you will try to answer this question using different approaches.

### Activity

Load the `Stalkies.csv` file and examine the columns of data it contains.

```{r}

```

Visualize the dataset in two ways:

1. Make a boxplot showing each food type on the x axis.
2. Plot the raw data (points) for the two groups. For the points, include jitter and transparency. Include means and standard errors plotted in a different color (the code in Lecture 06-1 might help you). Try a few different values for each of these elements to find a plot you feel best communicates the structure of the data. You will find that the number of data points you have will affect how much jitter & transparency is most appropriate. Change the axis labels to what you would expect on a publication ready figure.

```{r }

```

Describe the distributions within each group and between groups. What do these plots tell you? Do you have a prediction for what your statistical tests will show?

> 


We will first use the analytical method to determine if the group means are significantly different. In this case, a *t*-test is most appropriate.

By default, the `t.test` function will perform a test that does not assume equal variances (called "Welch's Correction"). You may specify equal variances with `var.equal = TRUE`, *if you decide that is appropriate here*. You maybe want to calculate the per-food source variance.

```{r}

```

Perform a *t*-test using the formula method to specify the groups to test (follow the example in lecture if you need to). Print the results.

```{r }

```

What can you conclude from these results? Does the result of this statistical test agree with your prediction above?

> 

`t.test()` is a function that performs a very specific task: carrying out a *t*-test (by default with Welch's correction for unequal variances). `lm()` is a general function that fits many many different kinds of linear models. We will talk quite a lot about linear models in the coming weeks.

A *t*-test is just a special kind of linear model (1 categorical predictor with 2 levels), so we can also fit the same model using `lm()`.

1. Use the code from above, but use `lm()` in place of `t.test()`. Assign this to a new object. Note that `lm()` always assumes equal variances (there is no `var.equal` argument).
2. Pass the object that you just created to the function `summary()`.

```{r}

```

Compare the results of the linear model with the results of the *t*-test above. Do you recognize any of the estimates? Can you figure out the relationship between the t-statistic and the F-statistic from the `lm()` summary?

> 

### Activity

In lecture, we gave the equations for calculating the Bayes Factor (likelihood ratio) to compare two models. This type of comparison is applicable in situations when you have model likelihoods available and gives you an estimate of the relative support of one model compared to the another. In the future, we will use variations of this test to compare models.

For now, we will give you most of the steps along the way. Study carefully, so that you understand what every block of code is doing.

```{r}
# Function to calculate sum of squares of a vector. Optionally the mean
# of x cn be provided. If not, the mean is calculated within the function.
SS <- function(x, x_bar = NULL) {
  # If x_bar is not provided, use the mean of x
  if (is.null(x_bar)) x_bar <- mean(x, na.rm = TRUE)

  # Return sum of squares
  return(sum((x - x_bar) ^ 2, na.rm = TRUE))
}
```


#### Setup data

Set `eval=FALSE` to `eval=TRUE` in the chunks below.

```{r, eval=FALSE}
# Change stalk below to whatever you called the Stalkies.csv data
n1 <- nrow(stalk[stalk$food_source == "Corn", ])
n2 <- nrow(stalk[stalk$food_source == "Cotton", ])

x <- stalk$eye_span
x1 <- stalk$eye_span[stalk$food_source == "Corn"]
x2 <- stalk$eye_span[stalk$food_source == "Cotton"]
```


#### Data drawn from two separate distributions

Assume equal variances but different means.

$$\hat{\sigma}^2_2 = \frac{1}{n_1 + n_2}\left(\sum^{n_1}_{i=1}\left(y_{1i} - \bar{y}_1\right)^2 +  \sum^{n_2}_{i=1} \left(y_{2i} - \bar{y}_2\right)^2 \right)$$

$$\mathcal{L}\left(\hat{\mu}_1, \hat{\mu}_2, \hat{\sigma}^2_2|x\right) = \frac{1}{\left(2\pi \hat{\sigma}^2_2\right)^\left(\frac{n_1 + n_2}{2}\right)}e^{\left(-\frac{n_1 + n_2}{2}\right)}$$

Translated into R:

```{r, eval=FALSE}
s_hat_2 <- (1 / (n1 + n2)) * (SS(x1) + SS(x2))

L_2 <- 1 / (2 * pi * s_hat_2) ^ ((n1 + n2) / 2) * exp(-(n1 + n2) / 2)
log(L_2)
```


#### All observations come from the same distribution

$$\hat{\sigma}^2_1 = \frac{1}{n_1 + n_2}\left(\sum^{n_1}_{i=1}\left(y_{1i} - \bar{\bar{y}}\right)^2 +  \sum^{n_2}_{i=1} \left(y_{2i} - \bar{\bar{y}}\right)^2 \right)$$

where $\bar{\bar{y}}$ is the grand mean.

$$\mathcal{L}\left(\hat{\mu}, \hat{\sigma}^2_1|x\right) = \frac{1}{\left(2\pi \hat{\sigma}^2_1\right)^\left(\frac{n_1 + n_2}{2}\right)}e^{\left(-\frac{n_1 + n_2}{2}\right)}$$

Translated into R:

```{r, eval=FALSE}
s_hat_1 <- (1 / (n1 + n2)) * (SS(x1, mean(x)) + SS(x2, mean(x)))
L_1 <- 1 / ((2 * pi * s_hat_1) ^ ((n1 + n2) / 2)) * exp(-((n1 + n2) / 2))
log(L_1)
```


#### Likelihood ratio (Bayes Factor)

The equation for Bayes Factor can be calculated in two ways:

$$BF = \frac{\mathcal{L}(Model~2)}{\mathcal{L}(Model~1)} = \left(\frac{\hat{\sigma}^2_2}{\hat{\sigma}^2_1}\right)^{-\frac{n_1 + n_2}{2}}$$

Use the calculations we have given you above to calculate the Bayes Factor for model 2 (the data come from distributions with different means) vs. model 1 (the data come from a single distribution with one mean).

You can either use the ratio of the likelihoods or the ratio of variances from the equation above.

```{r}

```

What do you conclude based on the Bayes Factor comparison?

> 

We have actually violated one of the assumptions of the likelihood calculations that we did. What is it?

> 


## Confidence intervals

We will use the same data to explore confidence intervals.


### Activity

Calculate a ~95% confidence interval for each mean (cotton & corn groups). You can use the approximate formula discussed in lecture $\mu$ +/- 2 * SEM.

```{r}

```

We can simulate what we would expect for a confidence interval if we set the true mean and standard deviation for our population. Knowing these true values allows us to draw random data from a true underlying distribution. 

For this simulation, we could assume our estimated means and standard deviations are representative of the population parameters. For any empirical dataset, will you ever know the true mean and standard deviation?

> 

Let's set up our simulation. Do the following:

1. Generate simulated data from a normal distribution matching the number of observations, mean, and standard deviation (note sd not SEM) for each group.
2. Calculate the mean for each group and the upper and lower bounds of the confidence interval.
3. Put these values into a `tibble`
4. Repeat this 1000 times
5. Calculate the proportion of times the mean for each group falls within the confidence interval


```{r}

```

What do you conclude based on this simulation?

> 

Is it correct to say that there is a 95% probability that the true mean is within a 95% confidence interval? Why or why not? Think about what you just showed with your simulation.

> 


## OLS Regression

The file `Earwigs.csv` contains data for the proportion of earwigs in a sample that have forceps and associated data on the population density of earwigs. An earwig with forceps is shown below.

<div style="width:350px">
![](https://i.imgur.com/YJ0NnSb.jpg)
</div>

The hypothesis is that earwigs living at higher densities will be more likely to have forceps, so that samples living at higher densities will have a higher proportion with forceps.


### Activity

Load the earwigs data:

```{r}

```

Make a scatterplot of the proportion of earwigs with forceps vs. density.

```{r}

```

Use the following code to add an OLS regression line through the points to your plot: `+ geom_smooth(method = "lm")`. Just add that code after `geom_point()` in your call to ggplot.

```{r}

```

Use `lm()` to fit a linear model in which the proportion of earwigs with forceps is modeled by the density of earwigs. Assign the linear model to an object, and then pass that object to `summary()` as you did above.

```{r}

```

See if you can figure out what the coefficients represent. You will find estimates for the slope and intercept of the line, their respective standard errors,^[remember that standard errors tell us about the uncertainty in parameter estimates] and a *t*-test, which tests each parameter estimate vs. 0. Try to figure out how the *t*-value is calculated (divide one thing by another).

> 

The data we gave you here is a proportion. What is another format that the data could have been in? Think about the source of the portion values.

> 
