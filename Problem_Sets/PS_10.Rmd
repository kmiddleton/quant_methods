---
title: 'Problem Set 10'
author: "Your Name Here: Group X"
date: 'Last updated: `r Sys.Date()`'
output:
  html_document:
    smart: no
    theme: flatly
    toc: true
    toc_float: true
---

```{r setup, message=FALSE, warning=FALSE}

```


Some useful color palettes for ggplot are in the [ggsci package](https://cran.r-project.org/web/packages/ggsci/vignettes/ggsci.html)

Try adding `scale_color_d3()`, `scale_color_aaas()`, and others to your plots.


## Nested Sites in Streams

Snyder et al. (2015)^[Snyder CD, Hitt NP, Young JA (2015) Accounting for groundwater in stream fish thermal habitat responses to climate change. *Ecological Applications* 25(5): 1397-1419. http://datadryad.org/resource/doi:10.5061/dryad.th6g8] collected data on the response of water temperature to air temperature to better understand the time scale of how changing air temperature affects water temperature (e.g., as a result of global climate change). The data they collected are in the file `Streams.xlsx`, the raw data file provided by the authors on Dryad.

One goal is to be able to predict water temperature only using air temperature (since air temperature is more efficiently measured at large scale).


### Activity

Load the `TemperatureData` sheet from the `Streams.xlsx` file. Look at the structure of the file. You should have 5 columns: `Site`, `Date`, `AirTemp_observed`, `AirTempPredicted`, and `WaterTemp`. If you don't, then you probably did not load the correct sheet. Check the `read_excel()` help to see how to do this.

```{r}

```

Now load the `SiteData` sheet from the Excel file. It has four columns, including the mapping from `Stream_Name` to `Site`. We need to merge the two tibbles together. Because they share a column `Site`, we can match rows, yielding a single tibble.

We covered joining tibbles in Unit 03-3, but here is a brief review. Base R has a `merge()` function, but it can be somewhat finicky. `tidyverse` has a complex set of functions for joining tibbles and data.frames. The most useful are:

1. `inner_join()`: "return all rows from x where there are matching values in y, and all columns from x and y. If there are multiple matches between x and y, all combination of the matches are returned."
2. `left_join()`: "return all rows from x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns. If there are multiple matches between x and y, all combinations of the matches are returned."
3. `full_join()`: "return all rows and all columns from both x and y. Where there are not matching values, returns NA for the one missing."

`left_join()` the temperature data and the site data and assign to a new object. This will add the columns of one to the other, matching on `Site`. You should end up with a 3,744 x 8 tibble.

```{r}

```

Data were collected for 78 different sites across 48 days. The 78 sites are nested within 9 different watersheds (`Stream_Name`). So this is a large, multilevel dataset.

Ultimately, we want to assess the ability to predict water temperature from predicted air temperature, but we have a few steps before we get there. Start by converting `Site`, `Stream_Name`, and `Date` into factors. If you look at the structure before you convert to factors, you will see that `Date` is a `POSIXct` variable, which means R recognizes it as a date.

We then need to do one more thing: make a new variable that converts `Date` into a day number. We can do this with the function `as.numeric()`. Make a new variable `Day`, which is the numeric representation of `Date`. It will look something like: `mutate(Day = as.numeric(Date))`.

```{r}

```

Because not all observations have observed air temperature, first we should check for correlation between observed and predicted air temperatures. Calculate the correlation between the two. We could filter out missing data, but instead just provide the argument `use = "pairwise.complete.obs"` to `cor()`, which will tell the function to use only complete pairs for the calculation. If you find that the correlation is `NA`, that means R is using incomplete cases. Any correlation between data with `NA`s is `NA`.

```{r}

```

The correlation should be pretty high (*r* = 0.95). So this means that we can safely use predicted air temperature (data which is complete) as a proxy for actual air temperature (data which is incomplete).

Let's plot: make a line plot with day on the x-axis and predicted air temp on the-y axis. Then facet by `Stream_Name`.

One more thing we need to add to `aes()` is `group = Site`. This code will tell ggplot to treat each `Site` separately when plotting lines. If your plot has a bunch of jagged vertical lines, you haven't set the group correctly. The plot should look something like this: https://imgur.com/J0lD2cp.png

Once you get your plot working, make the same plot for water temperature (copy the code and change the y variable).

```{r}

```

What general patterns do you see, comparing the two plots?

> 

We want to fit two models to these data. In both models we want to model water temperature by predicted air temperature. The difference between the models will be in the random effects.

We have an added wrinkle with these data that we have not yet encountered: nesting. Each site is only associated with one stream, so we need to tell R to only expect sites in some stream names. In an R formula, you can designate nesting using `/`. If B is nested in A, this would be coded as `A/B`.^[http://conjugateprior.org/2013/01/formulae-in-r-anova/ has a good overview of R's model formula, especially for more complicated relationships than just additive or multiplicative models.]

1. Fit a multilevel model where the intercept for site nested in stream is random (`~ 1 | ...`). This model allows each `Site` to have it's own intercept, which is nested within `Stream_Name`. But this value does not change over time.
2. Fit a multilevel model where the intercept for site nested in stream is random and day is also included as a continuous random predictor (`~ Day | ...`).

Use `lme()` and be sure to include the option `method = "ML"` to use maximum likelihood instead of the default of restricted maximum likelihood^[We need to use ML instead of REML so that the resulting likelihoods are comparable]. Save both models to objects and take a look at the summaries to make sure that you are getting output that makes sense. 

```{r}

```

Looking at the `summary()` of each, you should see sections in the random effects for `Stream_Name` and `Site %in% Stream_Name`.

Explain in non-mathematical terms what the addition of `Day` in the random effects part of the formula accomplishes.

> 

Load the `AICcmodavg` package and use the `aictab()` function to compare the two models you fit above.

```{r}

```

Interpret the difference in AICc values and Akaike weights (`AICcWt`). Does the model comparison make sense in light of your understanding of the difference between the two models?

> 

Fit a third model that is the same as model 2, but with the addition of `Day` as an additive, main effect. This model fits an overall slope to `Day` that is shared across all streams, with the random effects slope now an offset from the overall slope. Then use `aictab()` to compare all three models.

```{r}

```

What do you learn? Remember that AIC can only compare models that you fit.

> 


## Neandertal Brains

There are well-known differences in both body size between modern humans and Neanderthals. But are there differences in brain size as well, adjusted for body size. Ruff and colleagues (1997) tried to answer just that question.^[Ruff, C.B., E. Trinkaus, and T.W. Holliday. 1997. Body mass and encephalization in Pleistocene _Homo_. _Nature_ 387: 173-176.]

### Activity

The file `NeandertalBrainSize.csv` contains data on estimated log body mass, log brain size, and `Species`. Load the file, and convert `Species` to a factor.

```{r}

```

Make a scatterplot of brain size vs. body mass, with points colored by species. See if you can find information on the internet about how to move the legend from the side of the plot into the lower right hand corner. Your plot should look something like this https://imgur.com/CVgQ9RU.png.

```{r}

```

Fit three linear models and assign each to an object:

1. Brain size modeled by body mass
2. Brain size modeled by body mass and species (additive)
3. Brain size modeled by body mass and species with the mass X species interaction

```{r}

```

Use the `lrtest()` function from the `lmtest` package to perform a likelihood ratio test on the three models.

```{r}

```

What is your interpretation of the likelihood ratio test?

> 

Now use the `aictab()` function from the `AICcmodavg` package to compare AICc for the three models.

```{r}

```

What is your interpretation of the results?

> 

In lecture, we wrote out own code to do cross-validation. In practice, you would use code from others to automate this process, which is more general than any specific code you would write. A good package for cross-validation is `cvTools`. Install this package.

The function `cvFit()` performs cross-validation on a variety of R objects, including `lm()`. Look at the help for `cvFit()`. Running cross-validation is simple:

`cvFit(fm1, data = M, y = M$ln_Brain, K = 10, R = 100)`

The basic parts are a fitted model (`fm1`), a source for the data, the ID of the `y` variable (using `$` notation). Optionally, you can supply the number of `K` folds (default is 5 fold) and the number of replicates `R`. Increasing the number of replicates is important to get a more accurate estimate for the error.

Run 10-fold cross-validation on the three models with 100 replicates of each.

```{r}

```

What do the results indicate? Which model has the lowest error?

> 

Try comparing the models with leave-one-out cross-validating, by setting `K` to the number of rows of data. 

```{r}

```

How do these results compare to 10-fold CV?

> 

Compare the results of the likelihood ratio test, the AICc comparison, and cross-validation. What information does each provide you?

> 

Finally, let's add predicted lines for the additive model. To do this, you need to predict brain size across a range of body masses for each species. The code below does this. Change `eval=FALSE` to `eval=TRUE` to have R run the code when you knit. 

Study the code and see if you can figure out what it is doing.

```{r, eval=FALSE, echo=FALSE}

pred <- crossing(ln_Mass = seq(3.9, 4.5, length.out = 100),
                 Species = levels(M$Species))
pred <- pred |> 
  mutate(ln_Brain_pred = predict(fm2, newdata = pred))

ggplot() +
  geom_point(data = M, aes(ln_Mass, ln_Brain, color = Species), size = 3) +
  geom_line(data = pred, aes(ln_Mass, ln_Brain_pred, color = Species),
            size = 1.5) +
  scale_color_d3() +
  theme(legend.position = c(0.75, 0.12),
        legend.background = element_rect(size = 0.5,
                                         color = "black",
                                         linetype = "solid"),
        legend.margin=margin(c(2, 2, 2, 2))) +
  labs(x = "ln Body Mass", y = "ln Brain Size")
```

