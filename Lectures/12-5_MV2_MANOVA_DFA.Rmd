---
title: "Composite Variables 2: Canonical Correlation, MANOVA, and DFA"
subtitle: "Quantitative Methods in Life Sciences"
author: "Elizabeth King, Kevin Middleton, and Lauren Sullivan"
output:
  ioslides_presentation:
    fig_width: 8
    css: styles.css
csl: evolution.csl
bibliography: Multivariate.bib
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(cowplot)
library(GGally)
library(car)
library(MASS)

theme_set(theme_cowplot())
```


## Hierarchy of GLMs

<center>
<img src="https://i.imgur.com/sdx4fMz.png" width="100%" />
</center>


## Approaches to continuous data

*Correlation*:

- 2 interchangeable variables

*Canonical correlation*

- *p* Continuous ~ *q* Continuous

*PCA*:

- 0 outcome variables (`~ .`)
- *q* possibly correlated variables

*Multiple regression*:

- 1 continuous outcome variable
- *q* predictors


## Multivariate (Gaussian) linear models

Directional:

$$\mbox{Outcome variables} \sim \mbox{Predictor variables}$$

1. "MLM": *p* Continuous ~ *q* Continuous
2. MANOVA and Hotelling's *T*^2^: *p* Continuous ~ *q* Categorical
3. MANCOVA: *p* Continuous ~ *q* Categorical + *r* Continuous
4. (Multiple) correspondence analysis: *p* Categorical ~ *q* Categorical

All of these create composite variables.

## Canonical correlation

Correlations of *linear combinations* of a set of multiple continuous variables with a separate set of multiple continuous variables.

$$\mbox{Multiple variables} \sim \mbox{Multiple variables}$$

- Weather variables vs. Growth variables
- Ecology variables vs. Urbanization variables
- Diversity variables vs. Landscape variables


## Wheat

> "Two identical 13.6-ton (500 bu.) parcels of Manitoba Northern wheat, variety Selkirk, were stored 183 cm deep in 2 similar and adjoining 305 cm X 333 cm wooden bins in a granary in Winnipeg during 1959-67. Two hundred-gram samples were collected monthly from fixed sampling sites in the bins." Sinha et al. [-@Sinha1969-mp]

Measure biotic (insects, fungi, mites) and abiotic (location, depth, temperature) factors.

- How do these correlate?


## Wheat

<center>
<img src="https://i.imgur.com/jJd74yv.png" width="50%" />
</center>


## MANOVA

Morant [-@Morant1923-co] published data on 32 skulls from Tibet:

```{r}
M <- read_excel("../data/Tibetan_Skulls.xlsx") %>%
  mutate(Origin = factor(Origin))
str(M)
```


## Tibetan skulls

<center>
<img src="https://i.imgur.com/a4DpQbL.png" width="80%" />
</center>


## Tibetan skulls

```{r echo=FALSE}
ggscatmat(M, 1:5, color = "Origin") +
  theme(text = element_text(size = 9),
        axis.text = element_text(size = 6),
        axis.text.x = element_text(angle = -90, vjust = 0.5))
```


## MANOVA: Tibetan skulls

```{r}
fm <- lm(cbind(Skull_Length, Skull_Width,
               Skull_Height, Face_Height,
               Face_Breadth) ~ Origin, data = M)
Anova(fm, type = "III")
```


## MANOVA vs DFA & Logistic Regression

MANOVA and Hotelling's *T*^2^: *p* Continuous ~ *q* Categorical

MANOVA creates a composite variable from the set of outcome variables and asks whether the groups differ.

  - Predict the composite from the group membership

DFA & Logistic Regression: *q* Categorical ~ *p* Categorical

DFA and Logistic Regression create a composite variable from the set of predictor variables and asks which variables best predict group membership.

  - Predict group membership from the composite scores


## Logistic regression vs. DFA

Both:

- Which variable(s) are good predictors of group membership
- Groups are known *a priori*
- Uses linear combinations of variables
- **Predict new observations**

Differences:

- Some assumptions
- Predictive ability (depends on specifics)
- See: [this stackexchange](https://stats.stackexchange.com/questions/95247/logistic-regression-vs-lda-as-two-class-classifiers)
- Ease of interpretation for different questions


## Penguin data

```{r}
library(palmerpenguins)
glimpse(penguins)
penguins <- penguins %>% 
  dplyr::select(species, bill_length_mm, bill_depth_mm,
         flipper_length_mm, body_mass_g) %>% 
  drop_na()
```


## Penguin data

```{r echo=FALSE}
ggscatmat(penguins, 2:5, color = "species") +
  theme(text = element_text(size = 9),
        axis.text = element_text(size = 6),
        axis.text.x = element_text(angle = -90, vjust = 0.5))
```


## DFA in R

Linear discriminant analysis:

- `lda()` with `Group ~ Predictors`

```{r}
penguin_lda <- lda(species ~ bill_length_mm + bill_depth_mm +
                     flipper_length_mm + body_mass_g,
                   data = penguins)
```


## Posterior prediction

```{r}
penguin_predict <- predict(penguin_lda, penguins[, 2:5])
penguin_predict$posterior %>% head(10)
```


## Classification

```{r}
(penguin_classify <- penguin_predict$class)
```


## Percent correct

Mean value of the correct classifications

```{r}
mean(penguin_classify == penguins$species)
```


## Confusion matrix

```{r}
table(Original = penguins$species, Predicted = penguin_classify)
```


## Predicting a new observation

```{r}
new_penguin <- tibble(bill_length_mm = 45,
                      bill_depth_mm = 15,
                      flipper_length_mm = 220,
                      body_mass_g = 5000)
```


## Predicting a new observation

```{r echo=FALSE, fig.height = 3.5}
p1 <- ggplot(penguins, aes(y = bill_length_mm, x = body_mass_g,
                     color = species)) +
  geom_point() +
  geom_point(data = new_penguin, color = "red", size = 3) +
  theme(legend.position = "none")

p2 <- ggplot(penguins, aes(y = bill_depth_mm, x = body_mass_g,
                     color = species)) +
  geom_point() +
  geom_point(data = new_penguin, color = "red", size = 3) +
  theme(legend.position = "none")

p3 <- ggplot(penguins, aes(y = flipper_length_mm, x = body_mass_g,
                     color = species)) +
  geom_point() +
  geom_point(data = new_penguin, color = "red", size = 3) +
  theme(legend.position = c(0.65, 0.2),
        legend.box.background = element_rect(colour = "black"))

plot_grid(p1, p2, p3, ncol = 3)
```

```{r}
predict(penguin_lda, new_penguin)$posterior
```


## Predicting a new observation

```{r}
new_penguin <- tibble(bill_length_mm = 40,
                      bill_depth_mm = 12,
                      flipper_length_mm = 150,
                      body_mass_g = 2500)
```


## Predicting a new observation

```{r echo=FALSE, fig.height = 3.5}
p1 <- ggplot(penguins, aes(y = bill_length_mm, x = body_mass_g,
                     color = species)) +
  geom_point() +
  geom_point(data = new_penguin, color = "red", size = 3) +
  theme(legend.position = "none")

p2 <- ggplot(penguins, aes(y = bill_depth_mm, x = body_mass_g,
                     color = species)) +
  geom_point() +
  geom_point(data = new_penguin, color = "red", size = 3) +
  theme(legend.position = "none")

p3 <- ggplot(penguins, aes(y = flipper_length_mm, x = body_mass_g,
                     color = species)) +
  geom_point() +
  geom_point(data = new_penguin, color = "red", size = 3) +
  theme(legend.position = c(0.65, 0.2),
        legend.box.background = element_rect(colour = "black"))

plot_grid(p1, p2, p3, ncol = 3)
```

```{r}
predict(penguin_lda, new_penguin)$posterior
```


## References
