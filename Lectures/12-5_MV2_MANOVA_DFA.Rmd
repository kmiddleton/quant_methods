---
title: "Multivariate stats 2: Canonical Correlation, MANOVA, and DFA"
subtitle: "Quantitative Methods in Life Sciences"
author: "Elizabeth King, Kevin Middleton, and Lauren Sullivan"
output:
  ioslides_presentation:
    fig_width: 8
    css: styles.css
csl: evolution.csl
bibliography: Multivariate.bib
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(cowplot)
library(CCA)
library(candisc)
library(GGally)
library(car)
library(epiDisplay)

theme_set(theme_cowplot())
```


## Approaches to continuous data

*Correlation*:

- 2 interchangeable variables

*PCA*:

- 0 outcome variables (`~ .`)
- *q* possibly correlated variables

*Multiple regression*:

- 1 continuous outcome variable
- *q* predictors


## Canonical correlation

Correlations of *linear combinations* of a set of multiple continuous variables with a separate set of multiple continuous variables.

- Multiple variables ~ Multiple variables
- Weather variables vs. Growth variables
- Ecology variables vs. Urbanization variables
- Diversity variables vs. Landscape variables


## Canonical correlation

*Pairs* of canonical variates:

- Like a PCA on both sets of variables, but constrained to maximize the correlation between the *i*th pair of composite variables (canonical correlates)
- However, *not just the correlation of PCs*.


## Wheat

> "Two identical 13.6-ton (500 bu.) parcels of Manitoba Northern wheat, variety Selkirk, were stored 183 cm deep in 2 similar and adjoining 305 cm X 333 cm wooden bins in a granary in Winnipeg during 1959-67. Two hundred-gram samples were collected monthly from fixed sampling sites in the bins." Sinha et al. [-@Sinha1969-mp]

Measure biotic (insects, fungi, mites) and abiotic (location, depth, temperature) factors.

- How do these correlate?

## Wheat

<center>
<img src="https://i.imgur.com/jJd74yv.png" width="50%" />
</center>

## Generating multivariate data

Generate multivariate normal data with a specified correlation coefficient:

```{r}
MVnorm <- function(n, rho, mu, lab, sigma = 1) {
  Sigma <- matrix(c(sigma ^ 2, rho * sigma ^ 2,
                    rho * sigma ^ 2, sigma ^ 2), 2)
  x <- MASS::mvrnorm(n, mu, Sigma, empirical = TRUE)
  colnames(x) <- paste0(lab, 1:2)
  return(x)
}
```

```{r}
set.seed(10)
X <- MVnorm(n = 30, rho = 0.5, mu = c(0, 0), lab = "X")
Y <- MVnorm(n = 30, rho = 0.7, mu = c(0, 0), lab = "Y")
```

## Generating multivariate data

```{r}
head(X)
head(Y)
```

## Canonical correlation in R

`cancor()`

- Built-in R function
- Rudimentary but functional

`candisc`

- Overrides `cancor()` with its own version
- Useful plotting functions

`CCA::cc()`

- `CCA` package extension of `cancor()`
- Plotting functions
- Handles missing data

## Canonical correlation {.smaller}

```{r}
library(candisc)
z <- cancor(X, Y)
z
```

## Canonical correlates

```{r}
z$cancor
```

Canonical correlates:

1. Xcan1 vs. Ycan1: 0.35
2. Xcan2 vs. Ycan2: 0.04

## Canonical coefficients

```{r}
z$coef
```

Interpret as you would multiple regression:

- 1 unit increase in `X1` leads to a -0.79 unit increase in CCx1 when all other variables are constant

## Canonical loadings {.smaller}

```{r}
z$structure
```

## CCA is adirectional

Are the coefficients equal up to some small tolerance value?

```{r}
all.equal(cancor(X, Y)$cancor, cancor(Y, X)$cancor)
```

- the multivariate equivalent of `cor(x, y) == cor(y, x)`.


## Visualizing

```{r}
plot(z, which = 1, smooth = TRUE)
```


## Visualizing

```{r}
plot(z, which = 2, smooth = TRUE)
```


## Head measurements

Head length and breadth from 25 pairs of sons:

```{r}
M <- read_excel("../data/headsize.xlsx")
str(M)
```


## Head measurements

```{r echo=FALSE}
p1 <- ggplot(M, aes(head1, breadth1)) + geom_point()
p2 <- ggplot(M, aes(head2, breadth2)) + geom_point()
plot_grid(p1, p2, ncol = 2)
```


## Head measurements

```{r}
ggscatmat(M)
```


## Head measurements

Split into two matrices and convert to *Z*-scores:

```{r}
M1 <- M %>% dplyr::select(head1, breadth1) %>% as.matrix() %>% scale()
M2 <- M %>% dplyr::select(head2, breadth2) %>% as.matrix() %>% scale()
```

Canonical correlation

```{r}
z <- cancor(M1, M2, set.names = c("First Born", "Second Born"))
z$cancor
```


## Head measurements

```{r echo=FALSE}
p1 <- ggplot(as.data.frame(M1), aes(head1, breadth1)) + geom_point()
p2 <- ggplot(as.data.frame(M2), aes(head2, breadth2)) + geom_point()
plot_grid(p1, p2, ncol = 2)
```

## Head measurements

```{r}
plot(z, which = 1, smooth = TRUE)
```

## Head measurements

```{r}
plot(z, which = 2, smooth = TRUE)
```

## Head measurements

Canonical coefficients

```{r}
z$coef
```

## Head measurements {.smaller}

Standardized canonical loadings

```{r}
z$structure
```

## Multivariate (Gaussian) linear models

Directional:

- `outcome variables ~ predictor variables`

1. "MLM": *p* Continuous ~ *q* Continuous
2. MANOVA and Hotelling's *T*^2^: *p* Continuous ~ *q* Categorical
3. MANCOVA: *p* Continuous ~ *q* Categorical + *r* Continuous
4. (Multiple) correspondence analysis: *p* Categorical ~ *q* Categorical

MANOVA creates a composite variable from the set of outcome variables and asks whether the groups differ.

## General approach

1. Think about the question you want to answer
2. Group the outcome variables into a single matrix (`cbind()`)
3. Fit a linear model with `lm()`, predicting the outcome matrix by some predictors using formula notation
4. Analyze with `Anova()` and visualize with `plot()` and `heplot()` [@Friendly2006-zz]

## Head measurements

```{r}
(fm <- lm(cbind(head1, breadth1) ~ head2 + breadth2, data = M))
```

## Head measurements

```{r}
coef(lm(head1 ~ head2 + breadth2, data = M))
coef(lm(breadth1 ~ head2 + breadth2, data = M))
```

## Head measurements

```{r}
Anova(fm, type = "III")
```

Pillai–Bartlett statistic is the most robust of the various statistics (Wilk's $\Lambda$, Hotelling's trace) that are most often reported.

## Head measurements

```{r}
heplot(fm)
```

## Drug testing

```{r}
M <- read_excel("../data/Drug_test.xlsx")
M$Treatment <- factor(M$Treatment)
str(M)
```

## Drug testing

```{r warning=FALSE}
ggscatmat(M, 1:3, color = "Treatment")
```

## Drug testing

```{r}
fm <- lm(cbind(Fever, BP, Pain) ~ Treatment, data = M)
fm
```

## Drug testing

```{r}
Anova(fm, type = "III")
```

## Drug testing

```{r}
heplot(fm, type = "III")
```

## Drug testing

```{r}
pairs(fm, type = "III")
```

## Tibetan skulls

Morant [-@Morant1923-co] published data on 32 skulls from Tibet:

```{r}
M <- read_excel("../data/Tibetan_Skulls.xlsx")
str(M)
M <- M %>% mutate(Origin = factor(Origin))
```

## Tibetan skulls

<center>
<img src="https://i.imgur.com/a4DpQbL.png" width="80%" />
</center>

## Tibetan skulls

```{r echo=FALSE}
ggscatmat(M, 1:5, color = "Origin") +
  theme(text = element_text(size = 9),
        axis.text = element_text(size = 6),
        axis.text.x = element_text(angle = -90, vjust = 0.5))
```

## Tibetan skulls

```{r}
fm <- lm(cbind(Skull_Length, Skull_Width,
               Skull_Height, Face_Height,
               Face_Breadth) ~ Origin, data = M)
Anova(fm, type = "III")
```

## Tibetan skulls

```{r}
pairs(fm, type = "III")
```

## MANOVA vs DFA & Logistic Regression

MANOVA and Hotelling's *T*^2^: *p* Continuous ~ *q* Categorical

MANOVA creates a composite variable from the set of outcome variables and asks whether the groups differ.

  - Predict the composite from the group membership

DFA & Logistic Regression: *q* Categorical ~ *p* Categorical

DFA and Logistic Regression create a composite variable from the set of predictor variables and asks which variables best predict group membership.

  - Predict group membership from the composite scores


## Tibetan skulls

```{r}
M <- read_excel("../data/Tibetan_Skulls.xlsx")
str(M)
M <- M %>% mutate(Origin = factor(Origin))
```

## Tibetan skulls

```{r echo=FALSE}
ggscatmat(M, 1:5, color = "Origin") +
  theme(text = element_text(size = 9),
        axis.text = element_text(size = 6),
        axis.text.x = element_text(angle = -90, vjust = 0.5))
```

## Tibetan skulls

```{r}
fm <- glm(Origin ~ ., data = M, family = "binomial")
```

- Use `~ .` to include all other variables

## Tibetan skulls {.smaller}

```{r echo=FALSE}
summary(fm)
```

## Tibetan skulls

```{r}
Anova(fm, type = "III")
```

## Tibetan skulls {.smaller}

```{r}
logistic.display(fm)
```

## Logistic regression vs. DFA

Both:

- Which variable(s) are good predictors of group membership
- Groups are known *a priori*
- Uses linear combinations of variables
- **Predict new observations**

Differences:

- Some assumptions
- Predictive ability (depends on specifics)
- See: [this stackexchange](https://stats.stackexchange.com/questions/95247/logistic-regression-vs-lda-as-two-class-classifiers)
- Ease of interpretation for different questions

## Iris data

```{r}
glimpse(iris)
```

## Iris data

Anderson, E. 1935. The irises of the Gaspe Peninsula. *Bulletin of the American Iris Society* 59: 2–5.

```{r echo=FALSE}
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +
  geom_point()
```

## Iris data

```{r echo=FALSE}
ggscatmat(iris, 1:4, color = "Species") +
  theme(text = element_text(size = 9),
        axis.text = element_text(size = 6),
        axis.text.x = element_text(angle = -90, vjust = 0.5))
```

## DFA in R

Linear discriminant analysis:

- `lda()` with `Group ~ Predictors`

```{r}
iris.lda <- lda(Species ~ . ,  data = iris)
```

## Posterior prediction

```{r}
iris.predict <- predict(iris.lda, iris[, 1:4])
iris.predict$posterior
```

## Classification

```{r}
(iris.classify <- iris.predict$class)
```

## Percent correct

Mean value of the correct classifications

```{r}
mean(iris.classify == iris$Species)
```

## Confusion matrix

```{r}
table(Original = iris$Species, Predicted = iris.classify)
```

## Predicting a new observation

```{r}
new_iris <- data.frame(Sepal.Length = c(6.11),
                       Sepal.Width = c(3.55),
                       Petal.Length = c(4.44),
                       Petal.Width = c(2.5))
```

## Predicting a new observation

```{r echo=FALSE, fig.height = 2.5}
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +
  geom_point() +
  geom_point(data = new_iris, color = "red", size = 3)
```

```{r}
predict(iris.lda, new_iris)
```

## Predicting a new observation

```{r echo=FALSE, fig.height = 2.5}
new_iris <- data.frame(Sepal.Length = c(6.11),
                       Sepal.Width = c(3.55),
                       Petal.Length = c(5.05),
                       Petal.Width = c(1.66))
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +
  geom_point() +
  geom_point(data = new_iris, color = "red", size = 3)
```

```{r}
predict(iris.lda, new_iris)
```

## Tibetan skulls

```{r}
M <- read_excel("../data/Tibetan_Skulls.xlsx") %>%
  mutate(Origin = factor(Origin)) %>%
  as.data.frame()
str(M)

skulls_lda <- lda(Origin ~ ., data = M)
```

## Tibetan skulls

```{r}
skulls_predict <- predict(skulls_lda, M[, 1:5])
(skulls_classify <- skulls_predict$class)
```

## Tibetan skulls

```{r}
mean(skulls_classify == M$Origin)
table(Original = M$Origin, Predicted = skulls_classify)
```

## Oreodonts

Skull measurements of oreodonts

<center>
<img src="https://i.imgur.com/OOqkgmx.jpg" width="80%" />
</center>

## Oreodonts

```{r}
M <- read_excel("../data/Oreodont.xlsx") %>%
  mutate(Sex = factor(Sex)) %>%
  as.data.frame()
```

## Oreodonts

```{r}
M %>% group_by(Sex) %>% summarise_each("mean")
```

## Oreodonts

```{r echo=FALSE}
ggplot(M, aes(Length, Width, color = Sex)) +
  geom_point(size = 3)
```

## Oreodonts

```{r}
oreo_lda <- lda(Sex ~ ., data = M)
oreo_predict <- predict(oreo_lda, M[, 1:2])
(oreo_classify <- oreo_predict$class)
```

## Oreodonts

```{r}
mean(oreo_classify == M$Sex)
table(Original = M$Sex, Predicted = oreo_classify)
```

## Predict sex of new fossils

```{r}
new_oreo <- data.frame(Length = c(30, 25, 25),
                       Width = c(15, 20, 15))
predict(oreo_lda, new_oreo)
```


## References
