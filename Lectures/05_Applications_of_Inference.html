<!DOCTYPE html>
<html>
  <head>
    <title>Applications of Inference Frameworks</title>
    <meta charset="utf-8">
    <meta name="author" content="Elizabeth King and Kevin Middleton" />
    <link href="05_Applications_of_Inference_files/remark-css-0.0.1/example.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse

# Applications of Inference Frameworks
## Quantitative Methods in Life Sciences
### Elizabeth King and Kevin Middleton
### Last updated: 2017-01-04

---




# Notes

---

# Readings

---

# Three frameworks

1. Frequentist
2. Maximum likelihood
3. Bayesian

---

# Inferring a mean

Mean undulation rate for `\(n = 8\)` gliding snakes (http://www.flyingsnake.org/):


```r
undulation_rate &lt;- c(0.9, 1.2, 1.2, 1.3, 1.4, 1.4, 1.6, 2.0)
```

![](http://www.lazerhorse.org/wp-content/uploads/2015/01/Flying-Snake-Chrysopelea.jpg)

What is the mean undulation rate for this sample of flying snakes?

---


```r
ggplot(data_frame(undulation_rate), aes(undulation_rate)) +
  geom_histogram() +
  labs(x = "Undulation Rate (Hz)", y = "Count")
```

&lt;img src="05_Applications_of_Inference_files/figure-html/undulation_plot-1.png" width="672" /&gt;

---

# Frequentist inference of mean

Arthimetic mean:

`$$\hat{Y} = \frac{\sum_{i=1}^{n}Y_i}{n}$$`

---

# Frequentist inference of mean


```r
sum(undulation_rate) / length(undulation_rate)
```

```
## [1] 1.375
```

---

# Maximum likelihood inference of mean

Define a function to calculate the probability of an observed value `\(Y_i\)` given the mean (`\(\mu\)`) and standard deviation (`\(\sigma\)`). Default to the standard normal distribution `\(\mathcal{N}(0,1)\)`.

`$$f\left(Y_i; \mu, \sigma\right) = \frac{1}{\sqrt{2\pi\sigma^{2}}} e^{\frac{-\left(Y_i-\mu\right)^{2}}{2\sigma^{2}}}$$`


```r
normal &lt;- function(Y, mu = 0, sigma = 1) {
  1 / sqrt(2 * pi * sigma ^ 2) * 
    exp((-(Y - mu) ^ 2 / (2 * sigma ^ 2)))
}
```

_Note_: this function is built into R as `dnorm()`.

---

# Test our function


```r
normal(0, mu = 0, sigma = 1)
```

```
## [1] 0.3989423
```

```r
dnorm(0, mean = 0, sd = 1)
```

```
## [1] 0.3989423
```

&lt;img src="05_Applications_of_Inference_files/figure-html/normal_plot-1.png" width="672" /&gt;

---

# Calculating a likelihood

If the true mean is 0 and the standard deviation is 1, what is the likelihood of the observed values?

1. This is a model.
2. Calculate the probability of each observation.
3. Overall model likelihood is the product of the individual probabilities.
4. log-likelihood is more tractable, so calculate that.

---

# Model Likelihood (`\(\mathcal{L}\)`)

For a set of `\(Y_i\)` and parameters (`\(\Theta\)`; i.e., mean and standard deviation) the likelihood of the model is the product of their individual probabilities:

`$$\mathcal{L}\left(\left\{ Y_{i}\right\} _{i=1}^{n};\Theta\right) = \prod_{i=1}^{n}\phi\left(Y_{i}; \Theta\right)$$`

Evaluate the likelihood function for different values of `\(\Theta\)` to estimate `\(\mathcal{L}\)` for different sets of `\(\Theta\)`.

- Maximize `\(\mathcal{L}\)` and you will have the maximum likelihood set of parameter estimates.

---

# Model Likelihood (`\(\mathcal{L}\)`)

Probabilities are in the range 0 to 1, so taking the product of a large number of probabilities can result in some very small numbers.

- Computers don't handle really small numbers very well
    - There is a lower limit to the smallest number a computer can keep track of.
    - This is `\(2.2250739\times 10^{-308}\)` in R on the computer that compiled these slides.

Think about computing the likelihood for thousands or millions of observations.

---

# Model Likelihood (`\(\mathcal{L}\)`)

It's usually easier to minimize the (natural) log of the likelihood function. The log-likelihood is easier to deal with mathematically.

Log both sides of the equation:

`$$\log\left(\mathcal{L}\left(\left\{ Y_{i}\right\} _{i=1}^{n};\Theta\right)\right) = \log\left(\prod_{i=1}^{n}\phi\left(Y_{i};\Theta\right)\right)$$`

---

# Model Likelihood (`\(\mathcal{L}\)`)

Taking advantage of the algebraic rules associated with logs (the log of the products equals the sum of the logs):

`$$\log\left(\mathcal{L}\left(\left\{ Y_{i}\right\} _{i=1}^{n};\Theta\right)\right) = \sum_{i=1}^{n} \log\left(\phi\left(Y_{i};\Theta\right)\right)$$`

So we just need to sum the log-likelihoods to get the overall model likelihood. 

_Note_: `log()` is _natural_ log.

---

# Calculating the log-likelihood for a single combination of `\(\mu\)` and `\(\sigma\)`

If the mean is 0 and the standard deviation is 1, what is the likelihood of the observed values?

Probability for the first observation (`undulation_rate[1]`):


```r
undulation_rate[1]
```

```
## [1] 0.9
```

```r
normal(undulation_rate[1], mu = 0, sigma = 1)
```

```
## [1] 0.2660852
```

---

&lt;img src="05_Applications_of_Inference_files/figure-html/unnamed-chunk-6-1.png" width="672" /&gt;

This is only the probability for first observation. We need the likelihoods for all 8 undulation rates to get a model likelihood.

---

# Calculating the log-likelihood for a single combination of `\(\mu\)` and `\(\sigma\)`

Vector of probabilities for all values in `undulation_rate` given `mu = 0` and `sigma = 1`:


```r
(probs &lt;- normal(undulation_rate, mu = 0, sigma = 1))
```

```
## [1] 0.26608525 0.19418605 0.19418605 0.17136859 0.14972747 0.14972747
## [7] 0.11092083 0.05399097
```

Likelihood is the product of those probabilities:


```r
(lik &lt;- prod(probs))
```

```
## [1] 2.308476e-07
```

---

# Likelihood to log-likelihood


```r
log(lik)
```

```
## [1] -15.28151
```

```r
sum(log(probs))
```

```
## [1] -15.28151
```

If the true mean is 0 and true standard deviation is 1, then the model log-likelihood is -15.282.

Is there another combination of `\(\mu\)` and `\(\sigma\)` that gives a higher likelihood (= larger log-likelihood)?


```r
sum(log(normal(undulation_rate, mu = 1, sigma = 1)))
```

```
## [1] -8.281508
```

---

# Calculating the log-likelihood for a _range_ of `\(\mu\)` and `\(\sigma\)`

Find the combination of `\(\mu\)` and `\(\sigma\)` that maximizes the log-likelihood of the model for the mean and standard deviation of undulation rates.

Ranges of possible values:

1. Mean (`\(\mu\)`): `\(-\infty &lt; \mu &lt; \infty\)`
2. Standard deviation (`\(\sigma\)`): `\(0 &lt; \sigma &lt; \infty\)`

---

# Grid approximation

For combinations of `\(\mu\)` and `\(\sigma\)`, calculate the model likelihood. Pick the largest log-likelihood as the maximum likelihood estimates.

Set up the grid:


```r
n &lt;- 100
mus &lt;- seq(0.1, 5, length = n)
sigmas &lt;- seq(0.1, 2, length = n)

grid_mu_sigma &lt;- mesh(mus, sigmas)

grid_approx &lt;- data_frame(
  mu = as.numeric(grid_mu_sigma$x),
  sigma = as.numeric(grid_mu_sigma$y),
  log_lik = rep(NA, length = n ^ 2)
)
```

---


```r
grid_approx
```

```
## # A tibble: 10,000 × 3
##           mu sigma log_lik
##        &lt;dbl&gt; &lt;dbl&gt;   &lt;lgl&gt;
## 1  0.1000000   0.1      NA
## 2  0.1494949   0.1      NA
## 3  0.1989899   0.1      NA
## 4  0.2484848   0.1      NA
## 5  0.2979798   0.1      NA
## 6  0.3474747   0.1      NA
## 7  0.3969697   0.1      NA
## 8  0.4464646   0.1      NA
## 9  0.4959596   0.1      NA
## 10 0.5454545   0.1      NA
## # ... with 9,990 more rows
```

---

# Grid approximation


```r
for (i in 1:nrow(grid_approx)) {
  grid_approx[i, 3] &lt;- 
    sum(log(normal(undulation_rate,
                   mu = grid_approx$mu[i],
                   sigma = grid_approx$sigma[i])))
}
head(grid_approx)
```

```
## # A tibble: 6 × 3
##          mu sigma   log_lik
##       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;
## 1 0.1000000   0.1 -675.9308
## 2 0.1494949   0.1 -626.4259
## 3 0.1989899   0.1 -578.8807
## 4 0.2484848   0.1 -533.2954
## 5 0.2979798   0.1 -489.6698
## 6 0.3474747   0.1 -448.0041
```

This approach is coarse and time consuming. For `\(n = 100\)`, there are 10000 comparisons.

---

# Grid approximation

&lt;img src="05_Applications_of_Inference_files/figure-html/unnamed-chunk-13-1.png" width="672" /&gt;

---

# Grid approximation


```r
plot3d(grid_approx`\(mu, grid_approx\)`sigma, grid_approx$log_lik,
       type = "s", col = "red", size = 1,
       xlab = "mu",
       ylab = "sigma",
       zlab = "log-likelihood")
```

---

# Grid approximation

On this grid, the maximum likelihood estimates of `\(\mu\)` and `\(\sigma\)` are:


```r
grid_approx[which.max(grid_approx$log_lik), ]
```

```
##            mu     sigma   log_lik
## 1127 1.386869 0.3111111 -1.813363
```

---

# Maximum likelihood as an optimization problem

&lt;img src="05_Applications_of_Inference_files/figure-html/unnamed-chunk-16-1.png" width="672" /&gt;

---

# Maximum likelihood as an optimization problem

Define a function that takes a vector of values to optimize `x` (`\(\mu\)` and `\(\sigma\)`) as well as a set of data `Y` and returns the log-likelihood:


```r
log_lik &lt;- function(x, Y){
  probs &lt;- normal(Y, mu = x[1], sigma = x[2])
  return(sum(log(probs)))
}
```

We can now jointly optimize `\(\mu\)` and `\(\sigma\)`, maximizing the log-likelihood.

---

# Maximum likelihood as an optimization problem


```r
optim(c(0.1, 0.1),
      log_lik,
      Y = undulation_rate,
      control = list(fnscale = -1,
                     reltol = 10^-100))
```

```
## $par
## [1] 1.3750000 0.3031089
## 
## $value
## [1] -1.802203
## 
## $counts
## function gradient 
##      287       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL
```

---

# Maximum likelihood as an optimization problem


```r
fm &lt;- glm(undulation_rate ~ 1)
coef(fm)
```

```
## (Intercept) 
##       1.375
```

```r
logLik(fm)
```

```
## 'log Lik.' -1.802203 (df=2)
```

---

# Bayesian inference of mean

Ranges of possible maximum likelihood values:

1. `\(\mu\)`: `\(-\infty &lt; \mu &lt; \infty\)`
2. `\(\sigma\)`: `\(0 &lt; \sigma &lt; \infty\)`

Drawbacks:

1. `\(\mu\)` can't be negative (no negative undulation rates) and probably isn't a large number
2. `\(\sigma\)` is also probably not huge either

Can we do better? Yes, Bayesian priors.

---

# Prior for the mean

&lt;img src="05_Applications_of_Inference_files/figure-html/unnamed-chunk-18-1.png" width="672" /&gt;

---

# Prior for the mean

Cauchy distribution (location = 0, scale = 2)

&lt;img src="05_Applications_of_Inference_files/figure-html/unnamed-chunk-19-1.png" width="672" /&gt;

---

# Bayesian model


```r
fm_priors &lt;- map2stan(
  alist(
    undulation_rate ~ dnorm(mu, sigma),
    mu ~ dcauchy(0, 2),
    sigma ~ dcauchy(0, 5)),
  data = data.frame(undulation_rate),
  WAIC = FALSE,
  iter = 10^5, warmup = 10^3,
  constraints = list(mu = "lower=0")
)
```

```
## 
## SAMPLING FOR MODEL 'undulation_rate ~ dnorm(mu, sigma)' NOW (CHAIN 1).
## 
## Chain 1, Iteration:     1 / 100000 [  0%]  (Warmup)
## Chain 1, Iteration:  1001 / 100000 [  1%]  (Sampling)
## Chain 1, Iteration: 11000 / 100000 [ 11%]  (Sampling)
## Chain 1, Iteration: 21000 / 100000 [ 21%]  (Sampling)
## Chain 1, Iteration: 31000 / 100000 [ 31%]  (Sampling)
## Chain 1, Iteration: 41000 / 100000 [ 41%]  (Sampling)
## Chain 1, Iteration: 51000 / 100000 [ 51%]  (Sampling)
## Chain 1, Iteration: 61000 / 100000 [ 61%]  (Sampling)
## Chain 1, Iteration: 71000 / 100000 [ 71%]  (Sampling)
## Chain 1, Iteration: 81000 / 100000 [ 81%]  (Sampling)
## Chain 1, Iteration: 91000 / 100000 [ 91%]  (Sampling)
## Chain 1, Iteration: 100000 / 100000 [100%]  (Sampling)
##  Elapsed Time: 0.072522 seconds (Warm-up)
##                6.50908 seconds (Sampling)
##                6.5816 seconds (Total)
## 
## 
## SAMPLING FOR MODEL 'undulation_rate ~ dnorm(mu, sigma)' NOW (CHAIN 1).
## WARNING: No variance estimation is
##          performed for num_warmup &lt; 20
## 
## 
## Chain 1, Iteration: 1 / 1 [100%]  (Sampling)
##  Elapsed Time: 8e-06 seconds (Warm-up)
##                7.6e-05 seconds (Sampling)
##                8.4e-05 seconds (Total)
```

```
## Warning in map2stan(alist(undulation_rate ~ dnorm(mu, sigma), mu ~ dcauchy(0, : There were 12 divergent iterations during sampling.
## Check the chains (trace plots, n_eff, Rhat) carefully to ensure they are valid.
```

Using the `map2stan()` from the rethinking package (https://github.com/rmcelreath/rethinking).

---

# Inspecting the samples

&lt;img src="05_Applications_of_Inference_files/figure-html/unnamed-chunk-20-1.png" width="672" /&gt;

---

# Summarizing the results

&lt;img src="05_Applications_of_Inference_files/figure-html/unnamed-chunk-21-1.png" width="672" /&gt;

---

# Summarizing the results


```r
precis(fm_priors, digits = 5)
```

```
## Warning in precis(fm_priors, digits = 5): There were 12 divergent iterations during sampling.
## Check the chains (trace plots, n_eff, Rhat) carefully to ensure they are valid.
```

```
##          Mean  StdDev lower 0.89 upper 0.89 n_eff    Rhat
## mu    1.36376 0.15028    1.13589    1.59640 44038 0.99999
## sigma 0.40131 0.14313    0.21600    0.58201 35576 1.00004
```

---

# Linear regression

`$$Y = \theta_1 + \theta_2 X$$`

Where

- `\(Y\)` is the response variable
- `\(X\)` is the explanatory variable
- `\(\theta_1\)` is the intercept (`\(a\)`)
- `\(\theta_2\)` is the slope of the line (`\(b\)`)

---

# Linear regression

What values of `\(\theta_1\)` and `\(\theta_2\)` provide the best fit line through `\(Y\)` as a function of `\(X\)`?

`$$Y = \theta_1 + \theta_2 X$$`

How do we estimate `\(\theta_1\)` and `\(\theta_2\)`?

---

# Generate data

Generate `\(n=30\)` random data points: `\(X \sim \mathcal{N}(10, 1)\)` and `\(Y = 2.3 X + \epsilon\)`, where `\(\epsilon \sim \mathcal{N}(1, 1)\)`:


```r
set.seed(4)
n &lt;- 30
X &lt;- rnorm(n, mean = 10, sd = 1)
Y &lt;- 2.3 * X + rnorm(n, mean = 1, sd = 1)
M &lt;- data.frame(X, Y)
M %&gt;% head()
```

```
##           X        Y
## 1 10.216755 24.65200
## 2  9.457507 23.80420
## 3 10.891145 25.29542
## 4 10.595981 23.88857
## 5 11.635618 28.62305
## 6 10.689275 25.18081
```

---

# Generate data

&lt;img src="05_Applications_of_Inference_files/figure-html/unnamed-chunk-23-1.png" width="672" /&gt;

---

# Solve analytically

`\(Y\)` is a (n x 1) vector of observed values
`\(X\)` is an (n X 2) matrix of ones followed by observations

`$$\theta = (X'X)^{-1} X'Y$$`

Where `\(X'\)` is the transpose of `\(X\)` and `\(X{-1}\)` is the inverse [See: https://www.mathsisfun.com/algebra/matrix-inverse.html].


```r
(X_mat &lt;- matrix(c(rep(1, n), X), ncol = 2))
```

```
##       [,1]      [,2]
##  [1,]    1 10.216755
##  [2,]    1  9.457507
##  [3,]    1 10.891145
##  [4,]    1 10.595981
##  [5,]    1 11.635618
##  [6,]    1 10.689275
##  [7,]    1  8.718753
##  [8,]    1  9.786855
##  [9,]    1 11.896540
## [10,]    1 11.776863
## [11,]    1 10.566604
## [12,]    1 10.015719
## [13,]    1 10.383057
## [14,]    1  9.954863
## [15,]    1 10.034352
## [16,]    1 10.169027
## [17,]    1 11.165027
## [18,]    1  9.955796
## [19,]    1  9.899632
## [20,]    1  9.716555
## [21,]    1 11.540815
## [22,]    1 10.165169
## [23,]    1 11.307622
## [24,]    1 11.288257
## [25,]    1 10.592897
## [26,]    1  9.717056
## [27,]    1 11.255884
## [28,]    1 10.909839
## [29,]    1  9.071972
## [30,]    1 11.240181
```

---

# Solve analytically


```r
(theta &lt;- (solve(t(X_mat) %*% X_mat)) %*% (t(X_mat) %*% Y))
```

```
##             [,1]
## [1,] -0.03540546
## [2,]  2.39524849
```

`theta` is a 2 x 1 matrix of coefficients:

$$
\theta=\left[\begin{array}{c}
-0.035\\
2.395
\end{array}\right]
$$

---


```r
lm_fast &lt;- function(X, Y) {
  X_mat &lt;- matrix(cbind(rep(1, length(Y)), X), nrow = length(Y))
  theta &lt;- (solve(t(X_mat) %*% X_mat, tol = 1e-25)) %*% (t(X_mat) %*% Y)
  return(theta)
}

predict_Y &lt;- function(theta, X) {
  X &lt;- as.matrix(X, nrow = length(theta))
  Y_hat &lt;- theta[1, 1] + rowSums(theta[2:nrow(theta), 1] * X)
  return(Y_hat)
}

log_lik &lt;- function(Y, Y_hat) {
  var_hat &lt;- sum((Y - Y_hat)^2) / (length(Y))
  sd_hat &lt;- sqrt(var_hat)
  probs_Y &lt;- dnorm(Y, mean = Y_hat, sd = sd_hat)
  LL &lt;- sum(log(probs_Y))
  return(LL)
}
```

---


```r
theta &lt;- lm_fast(X, Y)
Y_hat &lt;- predict_Y(theta, X)
(LL &lt;- log_lik(Y, Y_hat))
```

```
## [1] -40.25821
```

```r
fm &lt;- lm(Y~X)
logLik(fm)
```

```
## 'log Lik.' -40.25821 (df=3)
```

```r
X_mat &lt;- matrix(c(rep(1, length(X)), X), ncol = 2)
fm_fast &lt;- fastLmPure(X_mat, as.matrix(Y))
log_lik(Y, fm_fast$fitted.values)
```

```
## [1] -40.25821
```

---


```r
t1 &lt;- Sys.time()

reps &lt;- 10^4
liks &lt;- numeric(length = reps)

load("~/Dropbox/House/Aprob.rda")
n &lt;- nrow(Aprob)

# Add column of 1's for intercept term
X &lt;- as.matrix(cbind(rep(1, n), Aprob[, 2:9]), nrow = n)

set.seed(5)

for (i in 1:reps) {
  Y &lt;- rexp(n, rate = 10)
  
  # Normalize so each row sums to 1.
  fm_fast &lt;- fastLmPure(X, Y)
  liks[i] &lt;- log_lik(Y, fm_fast$fitted.values)
}
Sys.time() - t1
```

```
## Time difference of 36.44112 secs
```

```r
ggplot(as.data.frame(liks), aes(x = 1:length(liks), y = liks)) + geom_path()
```

&lt;img src="05_Applications_of_Inference_files/figure-html/unnamed-chunk-28-1.png" width="672" /&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github"
});
if (window.HTMLWidgets) slideshow.on('showSlide', function (slide) {setTimeout(function() {window.dispatchEvent(new Event('resize'));}, 100)});</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
