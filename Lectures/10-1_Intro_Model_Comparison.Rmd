---
title: "Introduction to Model Comparison"
subtitle: "Quantitative Methods in Life Sciences"
author: 'Elizabeth King, Kevin Middleton, and Lauren Sullivan'
output:
  ioslides_presentation:
    fig_width: 8
    css: styles.css
csl: evolution.csl
bibliography: Multivariate.bib
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(cowplot)
theme_set(theme_cowplot())
library(ggrepel)
```

<!--
Datasets
  
-->


## Underfitting and overfitting {.columns-2}

**Occam's Razor**: Models with fewer assumptions are preferred

- How should we designate "preferred"?
- How should we compare among them?
- How good is good enough?


<center>
<img src="https://i.imgur.com/ilkU4eN.jpg" width="80%" />
</center>


## Tradeoff in model specification

**Underfitting**: Model is too simple. Doesn't describe the sample *well enough*.

- Leads to poor prediction

**Overfitting**: Model is too accurate. Describes the specific sample *too well*.

- Leads to poor prediction

We need to find a balance between *regular* features (what we want to model) and *irregular* features (noise we want to ignore).


## Why do we make models?

- Estimates of the magnitudes of parameters and/or differences between parameter estimates, *and* their relative precision
- Hypothesis testings, etc.

No model represents the true biological process

- This is actually a good thing.
- Which model comes *closest* to the true biological process from a *(small) set of working hypotheses*?


## Brain volume and (estimated) body mass in hominins (*Homo*)

```{r echo=FALSE}
Species <- c("afarensis", "africanus", "habilis", "boisei",
             "rudolfensis", "ergaster", "sapiens")
Brain_Vol <- c(438, 452, 612, 521, 752, 871, 1350)
Mass <- c(37, 35.5, 34.5, 41.5, 55.5, 61, 53.3)
M <- data.frame(Species, Brain_Vol, Mass)
```

```{r echo=FALSE}
M
```


## Brain volume and body mass in hominins

```{r echo=FALSE}
set.seed(29)
ggplot(M, aes(Mass, Brain_Vol, label = Species)) +
  geom_point(size = 2) +
  geom_text_repel(size = 4, force = 15,
                  fontface = "italic",
                  box.padding = unit(0.5, "lines"),
                  point.padding = unit(0.5, "lines")) +
  labs(x = "Body Mass (kg)", y = "Brain Volume (cc)")
```


## 6 increasingly complex polynomial models {.smaller}

- $\mbox{Brain Vol} = \beta_0 + \beta1 \mbox{M}$
- $\mbox{Brain Vol} = \beta_0 + \beta1 \mbox{M} + \beta2 \mbox{M}^2$
- $\mbox{Brain Vol} = \beta_0 + \beta1 \mbox{M} + \beta2 \mbox{M}^2 + \beta3 \mbox{M}^3$
- $\mbox{Brain Vol} = \beta_0 + \beta1 \mbox{M} + \beta2 \mbox{M}^2 + \beta3 \mbox{M}^3 + \beta4 \mbox{M}^4$
- $\mbox{Brain Vol} = \beta_0 + \beta1 \mbox{M} + \beta2 \mbox{M}^2 + \beta3 \mbox{M}^3 + \beta4 \mbox{M}^4 + \beta5 \mbox{M}^5$
- $\mbox{Brain Vol} = \beta_0 + \beta1 \mbox{M} + \beta2 \mbox{M}^2 + \beta3 \mbox{M}^3 + \beta4 \mbox{M}^4 + \beta5 \mbox{M}^5 + \beta6 \mbox{M}^6$

Compare *R*^2^ among them.


## 1: $\mbox{Brain Vol} = \beta_0 + \beta_1 \mbox{M}$

```{r}
fm1 <- lm(Brain_Vol ~ Mass, data = M)
coef(fm1)
summary(fm1)$r.squared
```


## 1: $\mbox{Brain Vol} = \beta_0 + \beta_1 \mbox{M}$

```{r echo=FALSE}
ggplot(M, aes(Mass, Brain_Vol, label = Species)) +
  geom_point(size = 3) +
  geom_smooth(formula = y ~ x, method = "lm", color = "red", lwd = 1.5) +
  labs(x = "Body Mass (kg)", y = "Brain Volume (cc)")
```


## 2. $\mbox{Brain Vol} = \beta_0 + \beta_1 \mbox{M} + \beta_2 \mbox{M}^2$

```{r}
fm2 <- lm(Brain_Vol ~ Mass + I(Mass ^ 2), data = M)
coef(fm2)
summary(fm2)$r.squared
```


## 2. $\mbox{Brain Vol} = \beta_0 + \beta_1 \mbox{M} + \beta_2 \mbox{M}^2$

```{r echo=FALSE}
ggplot(M, aes(Mass, Brain_Vol, label = Species)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", formula = 'y ~ poly(x, 2)',
              color = "red", lwd = 1.5) +
  labs(x = "Body Mass (kg)", y = "Brain Volume (cc)")
```


## 3. $\mbox{Brain Vol} = \beta_0 + ... + \beta_3 \mbox{M}^3$

```{r}
fm3 <- lm(Brain_Vol ~ Mass + I(Mass ^ 2) + I(Mass ^ 3), data = M)
coef(fm3)
summary(fm3)$r.squared
```


## 3. $\mbox{Brain Vol} = \beta_0 + ... + \beta_3 \mbox{M}^3$

```{r echo=FALSE}
ggplot(M, aes(Mass, Brain_Vol, label = Species)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", formula = 'y ~ poly(x, 3)',
              color = "red", lwd = 1.5) +
  labs(x = "Body Mass (kg)", y = "Brain Volume (cc)")
```


## 4. $\mbox{Brain Vol} = \beta_0 + ... + \beta_4 \mbox{M}^4$

```{r}
fm4 <- lm(Brain_Vol ~ Mass + I(Mass ^ 2) + I(Mass ^ 3) +
            I(Mass ^ 4), data = M)
coef(fm4)
summary(fm4)$r.squared
```


## 4. $\mbox{Brain Vol} = \beta_0 + ... + \beta_4 \mbox{M}^4$

```{r echo=FALSE}
ggplot(M, aes(Mass, Brain_Vol, label = Species)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", formula = 'y ~ poly(x, 4)',
              color = "red", lwd = 1.5) +
  labs(x = "Body Mass (kg)", y = "Brain Volume (cc)")
```


## 5. $\mbox{Brain Vol} = \beta_0 + ... + \beta_5 \mbox{M}^5$

```{r}
fm5 <- lm(Brain_Vol ~ Mass + I(Mass ^ 2) + I(Mass ^ 3) +
            I(Mass ^ 4) + I(Mass ^ 5), data = M)
coef(fm5)
summary(fm5)$r.squared
```


## 5. $\mbox{Brain Vol} = \beta_0 + ... + \beta_5 \mbox{M}^5$

```{r echo=FALSE}
ggplot(M, aes(Mass, Brain_Vol, label = Species)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", formula = 'y ~ poly(x, 5)',
              color = "red", lwd = 1.5) +
  labs(x = "Body Mass (kg)", y = "Brain Volume (cc)")
```


## 6. $\mbox{Brain Vol} = \beta_0 + ... + \beta_6 \mbox{M}^6$

```{r}
fm6 <- lm(Brain_Vol ~ Mass + I(Mass ^ 2) + I(Mass ^ 3) +
            I(Mass ^ 4) + I(Mass ^ 5) + I(Mass ^ 6), data = M)
coef(fm6)
summary(fm6)$r.squared
```


## 6. $\mbox{Brain Vol} = \beta_0 + ... + \beta_6 \mbox{M}^6$

```{r echo=FALSE, warning=FALSE}
ggplot(M, aes(Mass, Brain_Vol, label = Species)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", formula = 'y ~ poly(x, 6)',
              color = "red", lwd = 1.5,
              se = FALSE) +
  labs(x = "Body Mass (kg)", y = "Brain Volume (cc)")
```


## Summary R^2^

Model  | R^2^  |
------:|------:|
1      | `r round(summary(fm1)$r.squared, 2)`
2      | `r round(summary(fm2)$r.squared, 2)`
3      | `r round(summary(fm3)$r.squared, 2)`
4      | `r round(summary(fm4)$r.squared, 2)`
5      | `r round(summary(fm5)$r.squared, 2)`
6      | `r round(summary(fm6)$r.squared, 2)`


## Which model(s) balance under- vs. overfitting?

Model 0: Mean only

- Poor *in-sample* prediction
- Poor *out-of-sample* prediction

Model 1: Maybe enough information?

Model 6: Clearly overfit

- Perfect *in-sample* prediction
- Poor *out-of-sample* prediction


## Tradeoff in model specification

> "Overfitting: **fitting is easy; prediction is hard**. Future data will not be exactly like past data, and so any model that is unaware of this fact tends to make worse predictions than it could. So if we wish to make good predictions, we cannot judge our models simply on how well they fit our data. Information criteria provide estimates of predictive accuracy, rather than merely fit. So they compare models where it matters." [@McElreath2015-no]


## Methods for model comparison

1. Likelihood ratio tests
2. Information criteria
3. Cross validation


## References

