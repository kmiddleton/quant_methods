---
title: "Applications of Inference Frameworks: Linear Regression"
subtitle: "Quantitative Methods in Life Sciences"
author: 'Elizabeth King, Kevin Middleton, and Lauren Sullivan'
output:
  ioslides_presentation:
    fig_width: 8
    css: styles.css
csl: evolution.csl
bibliography: Multivariate.bib
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(cowplot)
theme_set(theme_cowplot())
library(RcppEigen)
library(rethinking)
library(latex2exp)

source("https://gist.githubusercontent.com/kmiddleton/869e0bbd7c4321e918a2270d5fecc27a/raw/fd77c17aed279728db38a4b7008fac89f0c5ed5f/ssPlot.R")
```


## Linear regression {.smaller}

Generate $n=30$ random data points: $X \sim \mathcal{N}(10, 1)$ and $Y = 2.3 X + \epsilon$, where $\epsilon \sim \mathcal{N}(1, 1)$:

```{r Generate_data, echo = FALSE, fig.width = 5, fig.height = 3.5, fig.align = 'center'}
set.seed(4)
n <- 30
X <- rnorm(n, mean = 10, sd = 1)
Y <- 2.3 * X + rnorm(n, mean = 1, sd = 1)
M <- data.frame(X, Y)

ggplot(M, aes(X, Y)) + geom_point()
```

What values of $\theta_0$ and $\theta_1$ provide the best fit line through $Y$ as a function of $X$?

$$Y = \theta_0 + \theta_1 X$$


## Frameworks for inference

1. ~~Analytical~~
2. Maximum likelihood
    - Minimizing the residual sum of squares is numerically equal to *maximizing* the model likelihood.
3. Resampling
4. Bayesian


## Errors are normally distributed around the regression line

<center>
<img src="https://i.imgur.com/F0Y5Fva.png" width="60%" />
</center>

How would probabilities change for a different slope estimate?


## What is the likelihood of an observed value given a regression model? 

Define a function to calculate the likelihood of an observed value $Y_i$ given the mean ($\mu$) and standard deviation ($\sigma$). Default to the standard normal distribution $\mathcal{N}(0,1)$.

$$Pr\left[Y_i\right] = \frac{1}{\sqrt{2\pi\sigma^{2}}} e^{\frac{-\left(Y_i-\mu\right)^{2}}{2\sigma^{2}}}$$


## Define a model

$$y = \bar{Y} + \theta_1 X$$

$$\theta_1 = 0$$

This is a flat line ($\theta_1 = 0$) through the mean of $Y$.


## Define a model

```{r echo=FALSE, results='hide'}
set.seed(4)
n <- 30
X <- rnorm(n, mean = 10, sd = 1)
Y <- 2.3 * X + rnorm(n, mean = 1, sd = 1)
M <- data.frame(X, Y)

ssPlot(X, Y, b = 0, do.labels = FALSE)
```


## Calculate the predicted values

Just the mean of $Y$ repeated 30 times.

```{r}
Y_bar <- mean(Y)
Y_hat <- rep(Y_bar, length(Y))
Y_hat
```


## Likelihood of a predicted value of $Y$

$$Pr\left[Y_i\right]=\frac{1}{\sqrt{2\pi\hat{\sigma}^2}} e^{\frac{-\left(Y_i - \mu\right)^{2}}{2\hat{\sigma}^2}}$$

- $\mu =$ the predicted value $\hat{Y}_i$
- Need the estimate of the residual variance $\left(\hat{\sigma}^2\right)$.


## Residual variance

$$\hat{\sigma}^2 = \frac{\Sigma_i\left(Y_i - \hat{Y}_i\right)^2}{n}$$

This is a biased estimate, but that's ok. It is how model likelihoods are calculated in this case. 

$$s^2 = MSE = \frac{n}{n-2}\left(\hat{\sigma}^2\right)$$

For non-small $n$, $s^2 \approx \left(\hat{\sigma}^2\right)$. Here ~7% difference.


## Calculate the estimated residual variance and standard deviation

```{r}
# Estimated variance
var_hat <- sum((Y - Y_hat)^2) / (length(Y))
var_hat
sd_hat <- sqrt(var_hat)
sd_hat
```


## Likelihoods for observed $Y$s

```{r}
# Check the likelihood for the first Y
dnorm(Y[1], mean = Y_hat[1], sd = sd_hat)

# Calculate for all Ys
(liks_Y <- dnorm(Y, mean = Y_hat, sd = sd_hat))
```


## Model Likelihood ($\mathcal{L}$)

For a set of $Y_i$ and parameters ($\Theta$; i.e., slope and intercept) the likelihood of the model is the product of their individual probabilities:

$$\mathcal{L}\left(\left\{ Y_{i}\right\} _{i=1}^{n};\Theta\right) = \prod_{i=1}^{n}Pr\left[Y_{i}; \Theta\right]$$

$$\log\left(\mathcal{L}\left(\left\{ Y_{i}\right\} _{i=1}^{n};\Theta\right)\right) = \sum_{i=1}^{n} \log\left(Pr\left[Y_{i};\Theta\right]\right)$$


## Model Likelihood ($\mathcal{L}$)

```{r}
# Sum of the log-likelihoods
sum(log(liks_Y))
```


## Likelihood from linear regression with $\theta_1 = 0$

Fit a linear model (`lm()`) with only an intercept (`~ 1`) and use the built-in function `logLik()` to extract the log-likelihood.

```{r}
fm <- lm(Y ~ 1)
logLik(fm)
```


## Maximizing the log-Likelihood

Function to calculate the log-likelihood. Input the slope ($\theta_1$) and observed values of $X$ and $Y$. Return the log-likelihood.

```{r log_lik}
log_lik <- function(theta_1, X, Y){
  Y_bar <- mean(Y)
  X_bar <- mean(X)
  theta_0 <- Y_bar - theta_1 * X_bar
  Y_hat <- theta_0 + theta_1 * X
  var_hat <- sum((Y - Y_hat) ^ 2) / (length(Y))
  sd_hat <- sqrt(var_hat)
  liks_Y <- dnorm(Y, mean = Y_hat, sd = sd_hat)
  return(sum(log(liks_Y)))
}
```


## Maximizing the log-Likelihood

For a range of $\theta_1$ from -10 to 10 in increments of 0.01, calculate the log-likelihood of the model. Save to a variable called `lls`. Then find the maximum value of `lls`, the associated $\theta_1$ and log-likelihood.

```{r}
theta_1 <- seq(-10, 10, by = 0.01)
lls <- numeric(length(theta_1))
for (ii in 1:length(theta_1)) {
  lls[ii] <- log_lik(theta_1[ii], X, Y)
}

# Location of maximum log-likelihood
max.ll <- lls[which.max(lls)]

# Slope at maximum log-likelihood
theta_1_hat <- theta_1[which.max(lls)]
```


## Maximizing the log-Likelihood

```{r echo=FALSE}
ggplot() +
  geom_line(data = tibble(lls, theta_1), aes(y = lls, x = theta_1)) +
  geom_point(data = tibble(x = theta_1_hat, y = max.ll), aes(x, y),
             color = "red", size = 3) +
  labs(y = "log Likelihood") +
  annotate(geom = "text", x = -7.5, y = -60,
           size = 7,
           label = paste("theta_1 =", theta_1_hat, "\nlogLik =",
                         round(max.ll, 2)))
```


## Maximizing the log-Likelihood

```{r}
max.ll
fm <- lm(Y ~ 1 + X) # Fit model with intercept and slope
logLik(fm)      # Use built-in function to extract log-likelihood
```

The values are not identical, because we only calculated $\theta_1$ in increments of 0.01.


## Bayesian priors

What priors for intercept ($\theta_0$) and slope ($\theta_1$)?

```{r echo=FALSE, fig.height=4}
p1 <- M %>% 
  ggplot(aes(X, Y)) +
  geom_point()
x <- seq(-50, 50, length = 100)
p2 <- tibble(x = x, y = dnorm(x, 0, 10)) %>% 
  ggplot(aes(x, y)) +
  geom_line() +
  labs(x = "value", y = "Probability",
       title = TeX("$N(0, 10)$"))
plot_grid(p1, p2, ncol = 2)
```


## Bayesian {.smaller}

```{r Bayes_regression, cache=TRUE, message=FALSE}
fm <- ulam(
  alist(
    Y ~ normal(mu, sigma),
    mu <- theta_0 + theta_1 * X,
    theta_0 ~ normal(0, 10),
    theta_1 ~ normal(0, 5),
    sigma ~ exponential(0.5)
  ),
  data = M,
  iter = 1e4,
  chains = 4)
```


## Bayesian sampling

```{r, echo=FALSE}
trankplot(fm)
```


## Bayesian posteriors

```{r, echo=FALSE}
post <- extract.samples(fm) %>% as_tibble() %>% 
  select(-sigma)
post %>%
  gather(variable, value) %>% 
  ggplot(aes(value)) +
  geom_line(stat = "density") +
  facet_grid(. ~ variable, scales = "free_x")
```


## Bayesian summary

```{r}
precis(fm, digits = 4)
coef(lm(Y ~ X, data = M))
```


## A sample of Bayesian samples

```{r echo = FALSE}
post_samp <- post %>%
  sample_n(200)
ggplot() +
  geom_abline(data = post_samp, aes(intercept = theta_0, slope = theta_1),
              alpha = 0.2) +
  geom_point(data = M, aes(X, Y), color = "red", size = 2.5)
```


## Key features

- Analytical solutions are fast, hand-calculable
    - Unavailable for complex models (e.g., hierarchical models)
- Analytical and maximum likelihood estimates will converge given enough precision
- Bayesian estimates can include prior knowledge
- Bayesian estimates will converge on ML estimates for sufficiently flat priors and/or sufficient data


## Key points

Comparison of means between groups:

- Categorical predictor variables (factors)

Linear relationships

- Continuous predictor variables

