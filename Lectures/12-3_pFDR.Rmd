---
title: "False Discovery"
subtitle: "Quantitative Methods in Life Sciences"
author: 'Elizabeth King and Kevin Middleton'
date: 'Last updated: `r Sys.Date()`'
output:
  ioslides_presentation:
    fig_width: 8
    css: styles.css
csl: evolution.csl
bibliography: Multivariate.bib
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(cowplot)
library(readxl)
library(qvalue)
```

```{r echo=FALSE, message=FALSE}
Ps <- read_excel("../data/P_values.xlsx")
```

## A menu of MCPs

1. <s>Do nothing</s>
2. <s>Common sense</s>
3. <s>Bonferroni correction</s> (excessively conservative, and we can do better)
4. Sequential Bonferroni procedure
    - Generally more permissive than Bonferroni. Provides control of FWER at $\alpha$.
5. Permutation Test
6. False Discovery Rate
7. _Positive_ False Discovery Rate

## FWER vs. False discovery rate

Controlling FWER is appropriate when you want to guard against **any** false positives (Bonferroni and sequential Bonferroni).

- When might this be appropriate?

In many cases we can live with a certain number of false positives.

If so, the more relevant quantity to control is the false discovery rate (FDR).

## False discovery rate

Proposed by Benjamini and Hochberg [-@Benjamini1995-cw].

- Also see Curran-Everett (2000).

Controls FDR (i.e., rate of Type I errors), rather than FWER

$$\mbox{FDR} = \frac{\mbox{n False Positives}}{\mbox{n All Positives}}$$

e.g., I'm OK with 5% false positives *among the tests I judge as significant*.

Note: False Positive Rate = $\frac{\mbox{n False Positives}}{\mbox{n All Tests}}$

## False discovery rate

- Sort the *P* values from largest ($p_k$) to smallest ($p_1$)
- Starting at the *largest* *P* value ($p_k$), calculate the critical significance level ($d_k^*$) for that *P* value:

$$d_k^* = q^* \frac{k}{k}$$

Where $q^*$ is the desired false discovery rate (e.g., 0.05).

- If $p_k \leq d_k^*$, then reject the null hypothesis for this and all remaining tests.
    - 1st comparison is vs. $q^*$
    - Rarely will the largest *P* value be less than $q^*$.

## False discovery rate

- Move to the 2nd largest *P* value and calculate the critical significance level

$$d_k^* = q^* \frac{k - 1}{k}$$

- 2nd test vs. $q^* \times 202/203 = 0.04975$
- Continue until $p_k \leq d_k^*$ then call that and all remaining tests (smaller *P* values) significant.
    - Last test vs. $q^* \times 1/203 = 0.00025$ (Bonferroni)

"Step-down" method

## False discovery rate

```{r}
Ps$FDR <- p.adjust(Ps$P, "fdr") < 0.05
sum(Ps$FDR)
```

44 of the original 62 significant tests remain.

## False discovery rate {.smaller}

```{r}
Ps %>% slice(32:47)
```

## A menu of MCPs

1. <s>Do nothing</s>
2. <s>Common sense</s>
3. <s>Bonferroni correction</s> (excessively conservative, and we can do better)
4. Sequential Bonferroni procedure
    - Generally more permissive than Bonferroni. Provides control of FWER at $\alpha$.
5. Permutation Test
6. False Discovery Rate
7. _Positive_ False Discovery Rate

## Positive false discovery rate

- First method to deal with actual distribution of *P* values from the study in question.
    - This is a major advance.
- "Positive" refers to the fact that positive findings have occurred (rejection of null hypotheses).
- But which of these apparent positive findings are actually false (i.e., which are most likely Type I errors)?

## Positive false discovery rate (pFDR)

- Is an extension of Benjamini and Hochberg's [-@Benjamini1995-cw] FDR procedure
- Described in:
    - Storey [-@Storey2002-mz]
    - Storey and Tibshirani [-@Storey2003-mz]
    - Storey et al. [-@Storey2004-fr]

## Goals of pFDR

Seek a balance between false positives and true positives

- Assume that we are doing an experiment because we expect some non-zero number of true positives.
    - Otherwise, why would we bother doing the experiment?
- We will accept a pFDR of 0.05 or 0.10, meaning that 5% or 10% of the tests we call true (nominally statistically significant) are actually false.

## Positive false discovery rate

Estimate $\pi_0$ (ratio of true null tests to total tests) *from the distribution of P values*

- Other multiple comparison procedures assume that $\pi_0 = 1$.


$$\pi_0 = \frac{\mbox{n True Null Tests}}{\mbox{n Total Tests}}$$

- FDR = pFDR when $\pi_0 = 1$
- More powerful than Bonferroni, sequential Bonferroni or FDR approaches
    - But the underlying assumptions of FDR and pFDR differ from Bonferroni and sequential Bonferroni

## pFDR and the `qvalue` package

pFDR is implemented in `qvalue`. Installation is a little different, because it is not in CRAN.

```{r eval = FALSE}
source("https://bioconductor.org/biocLite.R")
biocLite("qvalue")
library(qvalue)
```

## $\pi_0 = 1$

```{r echo=FALSE, message = FALSE}
ggplot(Ps, aes(P)) +
  geom_histogram(aes(y = ..density..), fill = "gray", bins = 20) +
  geom_hline(yintercept = 1, color = "blue")
```

If all H~0~ are true, you expect all bars to fall on the blue line.

## Estimation of $\pi_0$

- True non-null *P* values will fall nearest 0 (in the first bin: 0 - 0.05).
- Remaining *P* values will be uniformly distributed 0.05 - 1.00.
- Height of the uniform portion of the distribution estimates $\pi_0$
    - $\pi_0$ falls in the range 0 to 1, where 1 equal FDR.
    - Follow the math in Storey & Tibshirani [-@Storey2003-mz] if you wish

## The $q$ value

$q$ is the the minimum FDR that can be attained when calling *that test significant*

- Each *P* value is associated with a $q$ value
- A *P* value associated with a $q$ value of 0.05 means that 5% of *P* values lower are expected to be false positives
    - In other words, consider a *P* value with an associated $q$ value of 0.05. That *P* value and those lower than it should have a false positive rate of 0.05.

## pFDR in practice

- Look at the distribution of your *P* values.
    - The first bin should be largest (you have positive results)
    - If the distribution is completely flat, you can't use pFDR
    - If the distribution looks like a normal curve, is skewed to the right, or has peaks near 0 and 1, then $\pi_0$ estimation will probably fail. Fall back to FDR or sequential Bonferroni.

## pFDR in practice

- For large numbers of *P* values (hundreds?, thousands?), the default settings in the qvalue package work fine:
    - `lambda = seq(0, 0.90, 0.05)`
    - `pi0.method = "smoother"`
    - This estimates $\pi_0$ across a sequence of tuning parameters (lambda) using a smoother to determine the optimal $\pi_0$
    - If the smoother method fails, try `pi0.method = "bootstrap"` and/or `pdfr = TRUE`

## pFDR in practice

Just pass a vector of *P* values to `qvalue()`:

```{r}
qobj <- qvalue(Ps$P)
```

Explicitly:

```{r eval = FALSE}
qobj <- qvalue(Ps$P, fdr.level = 0.05, pi0.method = "smoother")
```


## pFDR in practice

```{r}
summary(qobj)
```

## Visualizing $\pi_0 = `r round(qobj$pi0, 3)`$

```{r echo=FALSE}
ggplot(Ps, aes(P)) +
  geom_histogram(aes(y = ..density..), fill = "gray", bins = 20) +
  geom_hline(yintercept = 1, color = "blue") +
  geom_hline(yintercept = qobj$pi0, color = "red")
```

Blue: All H~0~ are true. Red: Only 32% of H~0~ are true.

## Calculating an adjusted $\alpha$ level

```{r}
max(qobj$pvalues[qobj$qvalues <= 0.05])
```

- *Translation*: Find the largest *P* value in `qobj` which has an associated $q$ value less than or equal to 0.05.
    - So, if you use 0.03957791 as the $\alpha$ level across all of your *P* values in this example, then you will be controlling the FDR at 0.05.

Include unadjusted *P* values and adjust $\alpha$. Discuss results with respect to adjusted $\alpha$.

## Writing about pFDR

> After completing our statistical analyses, a positive false discovery rate (pFDR) analysis was used to control the false discovery rate at 0.05. Our pFDR analysis used the "smoother" option in the R package `qvalue`. We determined that an adjusted $\alpha$ level of 0.040 controlled experiment-wide false discovery at 5% for our 203 statistical tests, and subsequent inferences (Tables 1-3 in the electronic supplementary materials) use this adjusted $\alpha$ level. All tables present raw (unadjusted) *P* values.

## Empirical estimate of FDR

How could you estimate the expected number of false positives for a given dataset?

## Quiz 12-3

No more lectures.

## References
