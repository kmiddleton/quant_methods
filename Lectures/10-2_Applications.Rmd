---
title: "Randomization: Applications"
subtitle: "Quantitative Methods in Life Sciences"
author: 'Elizabeth King and Kevin Middleton'
date: 'Last updated: `r Sys.Date()`'
output:
  ioslides_presentation:
    fig_width: 8
    css: styles.css
csl: evolution.csl
bibliography: Multivariate.bib
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(cowplot)
library(readxl)
library(wesanderson)
library(knitr)
library(scales)
knitr::opts_chunk$set(cache = TRUE)
```

## Hypothesis testing by randomization

<center>
<img src="https://s-media-cache-ak0.pinimg.com/originals/e2/6f/cf/e26fcf6e3c0b082534cb6009f76567d9.jpg" width="55%" />
</center>

## Hypothesis testing by randomization

Mandible lengths of female and male jackals from the Natural History Museum (London).

```{r}
M <- read_excel("../data/Jackals.xlsx")
glimpse(M)
M <- M %>% mutate(Sex = factor(Sex)) %>% as.data.frame()
```

## Randomization tests

```{r echo=FALSE, message=FALSE}
ggplot(M, aes(Mandible)) +
  geom_histogram(bins = 30) +
  facet_grid(Sex ~ .) +
  labs(x = "Mandible Length (mm)", y = "Count")
```

## Randomization tests

We'd like to do a *t*-test to compare group means.

But:

1. Assumes random sampling
2. Assumes equal group variances
3. Assumes normal distribution within groups

## Basic randomization procedure

1. Decide on a test statistic
1. Calculate the test statistic for the *observed* data
1. Randomly shuffle the observations
1. Calculate the test statistic for that group
1. Repeat thousands of times
1. Determine the proportion of random combinations resulting in a test statistic more extreme than the observed value ("empirical *P*")

When test assumptions are met, the results will match asymptotic procedures. When the assumptions are not met, the results will be valid.

## Decide on a test statistic

1. Mean difference
2. *t*: *t*-test, linear model parameter estimate (slope, intercept)
3. *F*: ANOVA-like
4. $\chi^2$
5. Any metric of your choice (P-value, Fst, heterozygosity, LOD score, etc.)

## Calculate the test statistic for the observed data

Observed difference in mean value for females vs. males:

```{r}
(obs <- mean(M$Mandible[M$Sex == "F"]) -
   mean(M$Mandible[M$Sex == "M"]))
```

## Randomly reassign, recalculate, repeat

```{r mc_mean}
# Set random number seed
set.seed(10)

nreps <- 1e4
diffs <- numeric(length = nreps)
diffs[1] <- obs
for (ii in 2:nreps) {
  Rand_Sex <- sample(M$Sex)
  diffs[ii] <- mean(M$Mandible[Rand_Sex == "F"]) -
    mean(M$Mandible[Rand_Sex == "M"])
}
```

## Visualize the results

```{r ex_1, eval=FALSE, message=FALSE}
ggplot(data.frame(diffs), aes(diffs)) +
  geom_histogram(bins = 25) +
  geom_vline(xintercept = diffs[1], color = "red") +
  labs(x = "Difference", y = "Count")
```

Histogram of the randomized differences, vertical red line to mark the observed value.

## Visualize the results

```{r echo=FALSE, ref.label="ex_1", message=FALSE}
```

## Proportion of randomized differences more extreme than the observed

```{r}
mean(diffs <= diffs[1])
```

Mean of `diffs` where the value is *less than or equal to* the observed mean difference.

Empirically determined *P*-value is `r round(mean(diffs <= diffs[1]), 4)`.

```{r}
mean(c(TRUE, TRUE, FALSE))
```

## Randomization using R functions

- Run the test as usual, save the result to an object
- Extract the value of the test statistic
    - `str()`
    - `summary()`

## Finding the test statistic

```{r}
obs <- t.test(Mandible ~ Sex, data = M)
obs
```

## Finding the test statistic

```{r}
str(obs)
```

## Randomization using a *t*-test

```{r t_rand}
set.seed(8987324)
nreps <- 1e4
t_stats <- numeric(nreps)
t_stats[1] <- t.test(Mandible ~ Sex, data = M)$statistic

for (ii in 2:nreps) {
  t_stats[ii] <- t.test(sample(Mandible) ~ Sex, data = M)$statistic
}
```

`t.test(Mandible ~ Sex, data = M)$statistic` returns only the *t*-statistic.

## Visualize the results

```{r echo=FALSE, message=FALSE}
ggplot(data.frame(t_stats), aes(t_stats)) +
  geom_histogram() +
  geom_vline(xintercept = t_stats[1], color = "red") +
  labs(x = "t statistics", y = "Count")
```

## Proportion of randomized *t* more extreme than the observed

```{r prop_t}
2 * mean(t_stats <= t_stats[1])
```

Empirical *P* value is `r round(2 * mean(t_stats <= t_stats[1]), 4)` (*two-tailed test*).

## Use an *F*-test {.smaller}

```{r rand_F, cache=TRUE}
tic <- Sys.time()

set.seed(432431)
nreps <- 1e4
F_stats <- numeric(nreps)
obs_lm <- lm(Mandible ~ Sex, data = M)
obs_F <- anova(obs_lm)$"F value"[1]
F_stats[1] <- obs_F

for (ii in 2:nreps) {
  rand_lm <- lm(sample(Mandible) ~ Sex, data = M)
  rand_F <- anova(rand_lm)$"F value"[1]
  F_stats[ii] <- rand_F
}

toc <- Sys.time()
```

## Finding *F*

```{r}
str(anova(obs_lm))
```

## Visualize the results

```{r echo=FALSE, message=FALSE}
ggplot(data.frame(F_stats), aes(F_stats)) +
  geom_histogram() +
  geom_vline(xintercept = F_stats[1], color = "red") +
  labs(x = "F statistics", y = "Count")
```

## Proportion of randomized *F* more extreme than the observed

```{r}
mean(F_stats >= F_stats[1])
```

Empirical *P* value is `r round(mean(F_stats >= F_stats[1]), 4)`. No need to double here, because of the shape of the *F* distribution

## Randomization or permutation?

We have randomly sampled 10,000 possible combinations (some of which may have repeated).

We could permute all 184,756 combinations of 20 items taken 10 at a time (`choose(20, 10)`) with a little time. 10,000 combinations took:

```{r}
toc - tic
```

With larger sample sizes, permutation is not feasible.

## Example: Age predicted by coloration in lion noses

<center>
<img src="http://photorator.com/photos/images/african-lion-nose--9087.jpg" width="40%" />
<img src="https://s-media-cache-ak0.pinimg.com/236x/36/0e/0e/360e0e6f2172303e53a6661ce47920ad.jpg" width="42%" />
</center>


## Age predicted by coloration in lion noses

```{r echo=FALSE}
data("LionNoses", package = "abd")
ggplot(LionNoses, aes(proportion.black, age)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Proportion of Nose that is Black", y = "Age (y)")
```

## Age predicted by nose coloration {.smaller}

```{r}
fm <- lm(age ~ proportion.black, data = LionNoses)
summary(fm)
```

## Age predicted by nose coloration {.smaller}

```{r}
str(summary(fm))
```

## Age predicted by nose coloration

```{r}
summary(fm)$coefficients
summary(fm)$coefficients[2, 3]
```

## Age predicted by nose coloration

```{r mc_ols}
set.seed(481769)
nreps <- 1e4
t_stats <- numeric(nreps)
t_stats[1] <- summary(fm)$coefficients[2, 3]

for (ii in 2:nreps) {
  Rand_Age <- sample(LionNoses$age)
  fm_rand <- lm(Rand_Age ~ proportion.black, data = LionNoses)
  t_stats[ii] <- summary(fm_rand)$coefficients[2, 3]
}
```

## Age predicted by nose coloration

```{r echo=FALSE, message=FALSE}
ggplot(data.frame(t_stats), aes(t_stats)) +
  geom_histogram(bins = 30) +
  geom_vline(xintercept = t_stats[1], color = "red") +
  labs(x = "t statistics", y = "Count")
```

## Age predicted by nose coloration

```{r}
2 * mean(t_stats >= t_stats[1])
```

Note sign of the inequality: how often does the randomized age data give a *larger* *t* statistic than the observed?

Why 2 x 10^-4^?

## Horned lizard predation

```{r echo=FALSE, warning=FALSE, message=FALSE}
HornedLizards <- read_csv("../data/HornedLizards.csv")
HornedLizards <- HornedLizards %>%
  drop_na() %>%
  as.data.frame()
HornedLizards %>% 
  ggplot(aes(horn_length)) +
  geom_histogram(bins = 30) +
  facet_grid(group ~ .) +
  labs(x = "Horn Length (mm)", y = "Count")
```

## Horned lizard predation

```{r}
t_obs <- t.test(horn_length ~ group, data = HornedLizards)
t_obs
```

## Horned lizard predation

```{r mc_lizard, cache=TRUE}
set.seed(928374)
nreps <- 1e4
t_stats <- numeric(nreps)
t_stats[1] <- t_obs$statistic

for (i in 2:nreps) {
  Rand_group <- sample(HornedLizards$group)
  t_stats[i] <- t.test(horn_length ~ Rand_group,
                       data = HornedLizards)$statistic
}
```

## Horned lizard predation

```{r echo=FALSE, message=FALSE}
ggplot(data.frame(t_stats), aes(t_stats)) +
  geom_histogram(bins = 30) +
  geom_vline(xintercept = t_stats[1], color = "red") +
  labs(x = "F statistics", y = "Count")
```

## Horned lizard predation

```{r}
2 * mean(t_stats >= t_stats[1])
```

## Iterations vs. *P*

```{r iter_sim, echo=FALSE, eval=FALSE}
# Repeatedly generate empirical p values for increasing
# numbers of repetitions. Show that as reps increases,
# the empirical p will be the lower limit for as long 
# as the actual p is less than 1/reps (or 2 * 1/reps).

HornedLizards <- read_csv("../data/HornedLizards.csv")
HornedLizards <- HornedLizards %>%
  drop_na() %>%
  as.data.frame()

set.seed(4345)

library(parallel)
library(foreach)
library(doParallel)

n_cores <- 10
cl <- makeCluster(n_cores)
registerDoParallel(cl, cores = n_cores)

logspace <- function(d1, d2, n) exp(log(10) * seq(d1, d2,
                                                  length.out = n))
steps <- 20
reps_list <- floor(logspace(1, 6.1, n = steps))
reps_ex <- data.frame(nreps = reps_list,
                      P = numeric(length = steps))

t_obs <- t.test(horn_length ~ group, data = HornedLizards)

for (j in 1:nrow(reps_ex)) {
  tic <- Sys.time()
  nreps <- reps_ex$nreps[j]
  message("nreps = ", nreps)
  t_stats <- numeric(nreps)
  t_stats[1] <- t_obs$statistic
  
  do_rand <- function() {
    Rand_group <- sample(HornedLizards$group)
    as.numeric(t.test(horn_length ~ Rand_group,
                      data = HornedLizards)$statistic)
  }
  
  t_stats[2:nreps] <- foreach(i = 2:nreps,
                              .combine = c) %dopar% do_rand()
  
  reps_ex$P[j] <- 2 * mean(t_stats >= t_stats[1])
  message(Sys.time() - tic)
}
save(reps_ex, file = "../data/MC_reps_ex.Rda")
reps_ex
stopImplicitCluster()

ggplot(reps_ex, aes(nreps, P)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "n Iterations", y = "Empirical P-value")
```

```{r echo=FALSE}
load("../data/MC_reps_ex.Rda")
ggplot(reps_ex, aes(nreps, P)) +
  geom_point() +
  scale_y_log10() +
  scale_x_log10("x",
                breaks = trans_breaks("log10", function(x) 10^x),
                labels = trans_format("log10", math_format(10^.x)))+
  labs(x = "n Iterations", y = "Empirical P-value")
```

What is the minimal detectable *P* for *n* iterations?

## Empirical *P* vs. Iterations {.smaller}

```{r echo=FALSE}
reps_ex$min_P <- 2 * (1 / reps_ex$nreps)
kable(reps_ex[seq(2, nrow(reps_ex), by = 2),])
```

## Non-parametric tests

Non-parametric tests often used when data do not meet the assumptions of a traditional (parametric) test:

- One-sample *t*-test $\rightarrow$ Sign test, Wilcoxon test
- Two-sample *t*-test $\rightarrow$ Mann-Whitney test
- ANOVA $\rightarrow$ Kruskal-Wallis

Small sample size, non-normality, unequal variances

**Dramatically lower power compared to a parametric test**

## Randomization as an alternative

For all practical cases, randomization is a better alternative

- Increased power
- No reliance on asymptotic properties of tests
- More relaxed assumptions

## Quiz 10-2

Lecture 10-3
