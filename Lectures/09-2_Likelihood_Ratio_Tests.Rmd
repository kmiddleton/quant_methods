---
title: "Likelihood Ratio Tests"
subtitle: "Quantitative Methods in Life Sciences"
author: 'Elizabeth King and Kevin Middleton'
output:
  ioslides_presentation:
    fig_width: 8
    css: styles.css
csl: evolution.csl
bibliography: Multivariate.bib
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(cowplot)
library(readxl)
library(wesanderson)
```

## Likelihoods and model comparison

We have maximized likelihoods for single models

Now maximize likelihoods for competing models

- How to explicitly compare likelihoods?

## Likelihood ratio tests

- Nested models
    - Mean only
    - Mean for each group
- Asymptotic assumptions
- $\chi^2$ distribution

```{r echo=FALSE, fig.height = 2.5, fig.width = 4}
source("Shade_Distributions.R")
shade_chisq(0.05, 1)
```

## Energy expenditure in naked mole rats

<center>
<img src="https://i.imgur.com/FMMAjpa.jpg" width="80%" />
</center>

## Energy expenditure in naked mole rats

```{r echo=FALSE}
data(MoleRats, package = "abd")
M <- MoleRats
M$caste <- factor(as.numeric(M$caste),
                  labels = c("Non-worker", "Worker"))
names(M) <- c("Caste", "Mass", "Energy")

ggplot(M, aes(x = Mass, y = Energy, color = Caste)) +
  geom_point(size = 4) +
  scale_color_manual(values = wes_palette("Cavalcanti1")) +
  theme(legend.justification = c(0, 1),
        legend.position = c(0, 1)) +
  labs(x = "ln Body Mass (g)",
       y = "ln Daily Energy Expenditure (kJ)")
```

## Fit different models to these data

1. Mean (overall mean, no body mass)
1. ANOVA (group mean, no body mass)
1. OLS regression (body mass only, no grouping)
1. ANCOVA, intercepts varying
1. ANCOVA, slopes varying & intercepts varying

## 1: Mean

```{r}
fm1 <- lm(Energy ~ 1, data = M)
```

```{r echo=FALSE}
M <- M %>% mutate(pred1 = predict(fm1))
ggplot(M, aes(x = Mass, y = Energy, color = Caste)) +
  geom_point(size = 4) +
  geom_line(aes(x = Mass, y = pred1), lwd = 2,
            color = wes_palette("Cavalcanti1")[5]) +
  scale_color_manual(values = wes_palette("Cavalcanti1")) +
  theme(legend.justification = c(0, 1),
        legend.position = c(0, 1)) +
  labs(x = "ln Body Mass (g)",
       y = "ln Daily Energy Expenditure (kJ)")
```

## 2: ANOVA

```{r}
fm2 <- lm(Energy ~ Caste, data = M)
```

```{r echo=FALSE}
M <- M %>% mutate(pred2 = predict(fm2))
ggplot(M, aes(x = Mass, y = Energy, color = Caste)) +
  geom_point(size = 4) +
  geom_line(aes(x = Mass, y = pred2, color = Caste), lwd = 2) +
  scale_color_manual(values = wes_palette("Cavalcanti1")) +
  theme(legend.justification = c(0, 1),
        legend.position = c(0, 1)) +
  labs(x = "ln Body Mass (g)",
       y = "ln Daily Energy Expenditure (kJ)")
```

## 3: OLS regression

```{r}
fm3 <- lm(Energy ~ Mass, data = M)
```

```{r echo=FALSE}
M <- M %>% mutate(pred3 = predict(fm3))
ggplot(M, aes(x = Mass, y = Energy, color = Caste)) +
  geom_point(size = 4) +
  geom_line(aes(x = Mass, y = pred3, group = 1), lwd = 2,
            color = wes_palette("Cavalcanti1")[5]) +
  scale_color_manual(values = wes_palette("Cavalcanti1")) +
  theme(legend.justification = c(0, 1),
        legend.position = c(0, 1)) +
  labs(x = "ln Body Mass (g)",
       y = "ln Daily Energy Expenditure (kJ)")
```

## 4: ANCOVA, intercepts varying

```{r}
fm4 <- lm(Energy ~ Mass + Caste, data = M)
```

```{r echo=FALSE}
M <- M %>% mutate(pred4 = predict(fm4))
ggplot(M, aes(x = Mass, y = Energy, color = Caste)) +
  geom_point(size = 4) +
  geom_line(aes(x = Mass, y = pred4, color = Caste), lwd = 2) +
  scale_color_manual(values = wes_palette("Cavalcanti1")) +
  theme(legend.justification = c(0, 1),
        legend.position = c(0, 1)) +
  labs(x = "ln Body Mass (g)",
       y = "ln Daily Energy Expenditure (kJ)")
```

## 5: ANCOVA, intercepts and slopes vary

```{r}
fm5 <- lm(Energy ~ Mass * Caste, data = M)
```

```{r echo=FALSE}
M <- M %>% mutate(pred5 = predict(fm5))
ggplot(M, aes(x = Mass, y = Energy, color = Caste)) +
  geom_point(size = 4) +
  geom_line(aes(x = Mass, y = pred5, color = Caste), lwd = 2) +
  scale_color_manual(values = wes_palette("Cavalcanti1")) +
  theme(legend.justification = c(0, 1),
        legend.position = c(0, 1)) +
  labs(x = "ln Body Mass (g)",
       y = "ln Daily Energy Expenditure (kJ)")
```

## 5: ANCOVA, intercepts and slopes varying {.smaller}

```{r}
summary(fm5)
```

## Comparing likelihoods

Model  | *n* Estimated Parameters | log-Likelihood
-------|------:|------:
1 - Mean only                  | 1 | `r round(logLik(fm1), 3)`
2 - ANOVA                      | 2 | `r round(logLik(fm2), 3)`
3 - OLS regression             | 2 | `r round(logLik(fm3), 3)`
4 - ANCOVA, intercept only     | 3 | `r round(logLik(fm4), 3)`
5 - ANCOVA, slope & intercept  | 4 | `r round(logLik(fm5), 3)`

## Adding predictors *can't decrease* model likelihood

At worst, likelihood will be equal. It will probably go up.

Add a column of [0, 1] random numbers:

```{r}
set.seed(341)
M <- M %>% mutate(Noise = runif(nrow(M), 0, 1))
```

```{r echo=FALSE}
dplyr::select(M, Mass, Caste, Energy, Noise) %>% head()
```

## 6: ANCOVA 5 + Noise {.smaller}

```{r}
fm6 <- lm(Energy ~ Mass * Caste + Noise, data = M)
summary(fm6)
```

## Comparing likelihoods

Model  | *n* Parameters | log-Likelihood
-------|------:|------:
1 - Mean only                 | 1 | `r round(logLik(fm1), 3)`
2 - ANOVA                     | 2 | `r round(logLik(fm2), 3)`
3 - OLS regression            | 2 | `r round(logLik(fm3), 3)`
4 - ANCOVA, intercept         | 3 | `r round(logLik(fm4), 3)`
5 - ANCOVA, intercept, slope  | 4 | `r round(logLik(fm5), 3)`
6 - ANCOVA, intercept, slope, Noise  | 5 | `r round(logLik(fm6), 3)`

## Comparing models

Six nested models:

1. Mean
1. ANOVA
1. OLS regression
1. ANCOVA, intercepts varying
1. ANCOVA, intercepts and slopes varying
1. ANCOVA, intercepts and slopes varying, Noise

What one(s) has/have the best support?

## Likelihood ratio tests

- Compare *nested* models
    - Add or remove predictors
    - Add or remove interactions
    - **Outcome variable does not change**
- 2 * $\Delta$log-Likelihood of the two models
- Distributed as $\chi^2$ with *df* equal to difference in number of estimated parameters
    - 1 more predictor, 1 less *df*

## ANOVA vs. Mean only {.smaller}

```{r message = FALSE}
(chisq <- as.numeric(2 * (logLik(fm2) - logLik(fm1))))
pchisq(chisq, df = 1, lower.tail = FALSE)

library(lmtest)
lrtest(fm1, fm2)
```

## Likelihood ratio tests {.smaller}

Compare sets of *nested* models. Each compared to the one above with a $\chi^2$ test. 

```{r}
lrtest(fm1, fm2, fm3, fm4, fm5, fm6)
```

`Energy ~ Mass + Caste` is not improved by adding an interaction term.

## "Preferred" model {.smaller}

The "preferred" model is the one that is not improved by increased complexity. Interpretations based on this model:

```{r echo=FALSE}
summary(fm4)
```

```{r echo=FALSE}
# Save models
save(fm1, fm2, fm3, fm4, fm5, fm6, file = "lrtmodels.Rda")
```

## "Preferred" model {.smaller}

```{r echo=FALSE}
ggplot(M, aes(x = Mass, y = Energy, color = Caste)) +
  geom_point(size = 4) +
  geom_line(aes(x = Mass, y = pred4, color = Caste), lwd = 2) +
  scale_color_manual(values = wes_palette("Cavalcanti1")) +
  theme(legend.justification = c(0, 1),
        legend.position = c(0, 1)) +
  labs(x = "ln Body Mass (g)",
       y = "ln Daily Energy Expenditure (kJ)")
```

## Quiz 09-2

Lecture 09-3
