---
title: "Model Comparison: Cross Validation"
subtitle: "Quantitative Methods in Life Sciences"
author: 'Elizabeth King and Kevin Middleton'
date: 'Last updated: `r Sys.Date()`'
output:
  ioslides_presentation:
    fig_width: 8
    css: styles.css
csl: evolution.csl
bibliography: Multivariate.bib
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(cowplot)
library(ggplot2)
library(latex2exp)
library(wesanderson)

```

## The Problem

- Models maximize the amount of variation explained in the outcome variable by the predictor variables
- Can produce results specific to the data set (i.e., overfitting)
- More complex models = more specific, less general

## Simulation

- Set up a data set where we know which variables are true predictors and which are not
- Pull outcome variable from a normal distribution
    - Two groups, differ in mean value
- Predictor 1 is the grouping variable 
- All other predictors are randomly generated and unrelated to the outcome variable

## Simulation

```{r}
set.seed(29545)

nobs <- 10
yy <- c(rnorm(nobs / 2, 0, 1), rnorm(nobs / 2, 3, 1))
xx1 <- rep(c(0, 1), each = nobs / 2)
xx2 <- rnorm(nobs)
xx3 <- rnorm(nobs)
xx4 <- rnorm(nobs)
xx5 <- rnorm(nobs)
xx6 <- rnorm(nobs)
xx7 <- rnorm(nobs)
xx8 <- rnorm(nobs)
xx9 <- rnorm(nobs)
xx10 <- rnorm(nobs)
```

## Simulation

Fit 3 models:

```{r}
mod1 <- lm(yy ~ xx1)
mod2 <- lm(yy ~ xx1 + xx2 + xx3 + xx4 + xx5)
mod3 <- lm(yy ~ xx1 + xx2 + xx3 + xx4 + xx5 +
             xx6 + xx7 + xx8 + xx9 + xx10)
```

- Compare fitted values to assess how well the model fits the data  
- Remember $R^2$ = the squared correlation between the fitted and observed y values

## Model 1

```{r, echo=FALSE}
yy_hat1 <- mod1$fitted.values
data.frame('Yhat' = yy_hat1, 'Yobs' = yy) %>%
  ggplot(aes(x = Yhat, y = Yobs)) +
  geom_point(size = 3, alpha = 1 / 3) +
  labs(x = TeX("$\\hat{Y}$"),
       title = TeX(paste0("R^2 = ",
                          round(summary(mod1)$r.squared, 2),
                          "; Correlation = ",
                          round(sqrt(summary(mod1)$r.squared), 2))))
```

## Model 2

```{r, echo=FALSE}
yy_hat2 <- mod2$fitted.values
data.frame('Yhat' = yy_hat2, 'Yobs' = yy) %>%
  ggplot(aes(x = Yhat, y = Yobs)) +
  geom_point(size = 3, alpha = 1 / 3) +
  labs(x = TeX("$\\hat{Y}$"),
       title = TeX(paste0("R^2 = ",
                          round(summary(mod2)$r.squared, 2),
                          "; Correlation = ",
                          round(sqrt(summary(mod2)$r.squared), 2))))
```

## Model 3

```{r, echo=FALSE}
yy_hat3 <- mod3$fitted.values
data.frame('Yhat' = yy_hat3, 'Yobs' = yy) %>%
  ggplot(aes(x = Yhat, y = Yobs)) +
  geom_point(size = 3, alpha = 1 / 3) +
  labs(x = TeX("$\\hat{Y}$"),
       title = TeX(paste0("R^2 = ",
                          round(summary(mod3)$r.squared, 2),
                          "; Correlation = ",
                          round(sqrt(summary(mod3)$r.squared), 2))))
```

## Simulation

- How will our models perform with new data?

```{r}
newN <- 10
newy <- c(rnorm(newN / 2, 0, 1), rnorm(newN / 2, 3, 1))
newx <- data.frame('xx1' = rep(c(0, 1), each = newN / 2),
                   'xx2' = rnorm(newN),
                   'xx3' = rnorm(newN),
                   'xx4' = rnorm(newN),
                   'xx5' = rnorm(newN),
                   'xx6' = rnorm(newN),
                   'xx7' = rnorm(newN),
                   'xx8' = rnorm(newN),
                   'xx9' = rnorm(newN),
                   'xx10' = rnorm(newN))

y_hat_new1 <- predict.lm(object = mod1, newdata = newx)
y_hat_new2 <- predict.lm(object = mod2, newdata = newx)
y_hat_new3 <- predict.lm(object = mod3, newdata = newx)
```

## Model 1

```{r, echo=FALSE}
miny <- min(c(newy, yy))
maxy <- max(c(newy, yy))

minx <- min(c(y_hat_new1, y_hat_new2, y_hat_new3,
              yy_hat1, yy_hat2, yy_hat3))
maxx <- max(c(y_hat_new1, y_hat_new2, y_hat_new3,
              yy_hat1, yy_hat2, yy_hat3))

new1 <- data.frame('Yhat' = y_hat_new1, 'Yobs' = newy)
new2 <- data.frame('Yhat' = y_hat_new2, 'Yobs' = newy)
new3 <- data.frame('Yhat' = y_hat_new3, 'Yobs' = newy)

data.frame('Yhat' = yy_hat1, 'Yobs' = yy) %>%
  ggplot(aes(x = Yhat, y = Yobs)) +
  geom_point(size = 3, alpha = 1 / 3) +
  geom_point(data = new1, aes(x = Yhat, y = Yobs),
             size = 3, alpha = 1 / 3, color = 'red') +
  ylim(miny, maxy) +
  xlim(minx, maxx) +
  labs(x = TeX("$\\hat{Y}$"),
       title = TeX(paste0("Correlation (new data vs. predicted values) = ",
                          round(cor(y_hat_new1,newy), 2))))
```

## Model 2

```{r, echo=FALSE}
data.frame('Yhat' = yy_hat2, 'Yobs' = yy) %>%
  ggplot(aes(x = Yhat, y = Yobs)) +
  geom_point(size = 3, alpha = 1 / 3) +
  geom_point(data = new2, aes(x = Yhat, y = Yobs),
             size = 3, alpha = 1 / 3, color = 'red') +
  ylim(miny, maxy) +
  xlim(minx, maxx) +
  labs(x = TeX("$\\hat{Y}$"),
       title = TeX(paste0("Correlation (new data vs. predicted values) = ",
                          round(cor(y_hat_new2,newy), 2))))
```

## Model 3

```{r, echo=FALSE}
data.frame('Yhat' = yy_hat3, 'Yobs' = yy) %>%
  ggplot(aes(x = Yhat , y = Yobs)) +
  geom_point(size = 3, alpha = 1 / 3) +
  geom_point(data = new3, aes(x = Yhat, y = Yobs),
             size = 3, alpha = 1 / 3, color = 'red') +
  ylim(miny, maxy) +
  xlim(minx, maxx) +
  labs(x = TeX("$\\hat{Y}$"),
       title = TeX(paste0("Correlation (new data vs. predicted values) = ",
                          round(cor(y_hat_new3,newy), 2))))
```

## Cross Validation

> When we don't know the true model (all cases but simulation), how do we know if a model is too specific to a given data set?

- Cross validation is one technique to address this question

## Approach

- Split data into training and test sets
- Use training set to build model
- Use test set to evaluate model
- Repeat to get average error
- **Which Predictors?**
- **What Parameter Values?**

## Approaches

- Random sub samples as test set
    - Issue: reuse of data
- K-fold cross validation
    - Issue: need large data set
- Leave one out
    - Issue: small test set

## Energy expenditure in naked mole rats

<center>
<img src="images/NMR.jpg" width="80%" />
</center>

## Energy expenditure in naked mole rats

```{r echo=FALSE}
data(MoleRats, package = "abd")
M <- MoleRats
M$caste <- factor(as.numeric(M$caste),
                  labels = c("Non-worker", "Worker"))
names(M) <- c("Caste", "Mass", "Energy")

ggplot(M, aes(x = Mass, y = Energy, color = Caste)) +
  geom_point(size = 4) +
  scale_color_manual(values = wes_palette("Cavalcanti")) +
  theme(legend.justification = c(0, 1),
        legend.position = c(0, 1)) +
  labs(x = "ln Body Mass (g)",
       y = "ln Daily Energy Expenditure (kJ)")
```

## Fit different models to these data

1. ANOVA (group mean, no body mass)
1. ANCOVA, intercepts varying
1. ANCOVA, slopes varying & intercepts varying

## Assess models with 10-fold cross validation

1. Split data into 10 ~equal sets
1. Use each set as the test set with the remaining data as the training set
1. Quantify error 
    - **Mean squared error**
    - Correlation between predicted & observed values
    - Categorical outcome: % misclassified

## 10-fold cross validation
    
```{r}
set.seed(666)
folds <- 10
M$id.group <- sample(c(rep(seq(1, folds), each = 3),
                       sample(seq(1, folds), 5)))
gg <- 1:folds
err.out <- data.frame('mod1' = numeric(length = folds),
                      'mod2' = numeric(length = folds),
                      'mod3' = numeric(length = folds))
```

## 10-fold cross validation
    
```{r}
for(ii in gg) {
  M_train <- M[M$id.group != ii, ]
  mod1 <- lm(Energy ~ Caste, data = M_train)
  mod2 <- lm(Energy ~ Mass + Caste, data = M_train)
  mod3 <- lm(Energy ~ Mass * Caste, data = M_train)
  M_test <- M[M$id.group == ii, ]
  pp1 <- predict(mod1, M_test)
  pp2 <- predict(mod2, M_test)
  pp3 <- predict(mod3, M_test)
  err.out[ii, 'mod1'] <- mean((M_test$Energy - pp1) ^ 2)
  err.out[ii, 'mod2'] <- mean((M_test$Energy - pp2) ^ 2)
  err.out[ii, 'mod3'] <- mean((M_test$Energy - pp3) ^ 2)
}
```

## Visualizing MSE of one "fold"

```{r echo=FALSE}
preds <- M_test %>% 
  select(-id.group) %>% 
  mutate(`Overall Mean` = pp1,
         `Mass + Caste` = pp2,
         `Mass * Caste` = pp3) %>% 
  gather(Model, value, -Caste, -Mass, -Energy)

ggplot() +
  geom_vline(xintercept = M_test$Mass, color = "gray60",
             linetype = "dotted") +
  geom_point(data = M_train, aes(x = Mass, y = Energy, color = Caste),
             size = 1, alpha = 0.5) +
  scale_color_manual(values = wes_palette("Cavalcanti")) +
  theme(legend.justification = c(0, 1),
        legend.position = c(0, 1)) +
  labs(x = "ln Body Mass (g)",
       y = "ln Daily Energy Expenditure (kJ)") +
  geom_point(data = M_test, aes(x = Mass, y = Energy, color = Caste),
             size = 4, pch = 16) +
  geom_point(data = preds, aes(x = Mass, y = value, color = Caste,
                               shape = Model), size = 4) +
  scale_shape_manual(values = c(0, 1, 2))
```

## 10-fold cross validation {.smaller}
    
```{r}
err.out

colMeans(err.out)
```

## 10-fold cross validation

```{r echo=FALSE}
err.out %>%
  mutate(Fold = factor(1:folds)) %>% 
  gather(Model, MSE, -Fold) %>% 
  mutate(Model = factor(
    Model,
    labels = c("Overall Mean", "Mass + Caste", "Mass * Caste"))) %>% 
  ggplot(aes(Model, MSE, color = Fold)) +
  geom_point() +
  stat_summary(fun.y = "mean", geom = "point", color = "black",
               size = 3)

```

## Notes & Resources

- Training and Test sets must come from the same population
- There is uncertainty in cross validation estimates
- We've written our own code here but there are several R packages:

    - `cvTools` package
    - `boot` package

## No Quiz

No more Lectures
