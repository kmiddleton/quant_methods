---
title: "Unit 10: In Class Discussion"
subtitle: "Quantitative Methods in Life Sciences"
author: 'Elizabeth King, Kevin Middleton, and Lauren Sullivan'
output:
  ioslides_presentation:
    fig_width: 8
    css: styles.css
csl: evolution.csl
bibliography: Multivariate.bib
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(cowplot)
library(ggsci)
library(lme4)
library(AICcmodavg)

theme_set(theme_cowplot())
```

## PC 2


## Quiz 10-4


## Summary of model comparison procedures

1. Likelihood ratio tests
    - Significance tests of model differences
2. AIC and AICc
    - Model comparison that penalizes complexity (and small sample size)
3. Akaike Weights
    - Relative weighting of a set of models (0 - 100%)
4. Cross validation
    - Out-of-sample prediction error for a model


## AIC and AICc

$$\mbox{AIC} = -2 \log\left(\mathcal{L}[\theta | \mbox{data}]\right) + 2 k$$

$$\mbox{AICc} = -2 \log\left(\mathcal{L}[\theta | \mbox{data}]\right) + 2 k + \frac{2k(k+1)}{n - k - 1}$$

> My main question is about AICc. The way that I think about it, is it similar to transforming the data into log10? Or should I not be thinking about AICc this way?


## Distance from the "true" model

> I suppose I would like a bit more explanation of how AIC can weight certain models against the "true" model even though we don't know it.

- No model represents the true process that generated the outcomes.
- Information is the "distance" between a prospective models and the true model


## Interpreting information criteria

> For the delta AIC in R I was wondering if each value in that column is the delta AIC compared to the model above?

```
Model selection based on AICc:

    K   AICc Delta_AICc AICcWt Cum.Wt     LL
fm2 4 100.21       0.00   0.91   0.91 -43.88
fm3 5 105.21       5.01   0.07   0.99 -43.86
fm0 2 110.25      10.04   0.01   0.99 -52.58
fm4 6 110.82      10.61   0.00   1.00 -43.41
fm1 3 113.30      13.09   0.00   1.00 -52.45
fm5 7 119.48      19.27   0.00   1.00 -43.40
```

## Akaike weights

> I struggled to understand what the weight is in AICc evaluations. I understand larger is better, but I don't know why.


## Cross-validation

> The cross validation was most challenging for me this week. I believe I understand how to interpret the results of a cross validation but I do not fully understand the code used to set it up.


## Cross-validation

- Split data into training and test sets
- Use training set to build model
- Use test set to evaluate model
- Repeat to get average error
- **Which Predictors?**
- **What Parameter Values?**


## Interpreting cross-validation

> I think I could benefit from reviewing how to interpret output/results from different cross-validation techniques again.

> Predicting which model is preferred when using the leave-one-out approach.


## Model comparison

> How should I choose which method to use?

> Is it possible to simulate all these methods, not just Cross Validation?

