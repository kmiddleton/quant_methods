---
title: "Applications of Inference Frameworks: Linear Regression"
subtitle: "Quantitative Methods in Life Sciences"
author: 'Elizabeth King, Kevin Middleton, and Lauren Sullivan'
output:
  ioslides_presentation:
    fig_width: 8
    css: styles.css
csl: evolution.csl
bibliography: Multivariate.bib
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(cowplot)
theme_set(theme_cowplot())
library(RcppEigen)
library(rethinking)
library(latex2exp)

ssPlot <- function(X, Y, b, do.plot = TRUE, do.labels = TRUE){
  n <- length(X)
  SSy <- sum((Y - (X * b + (mean(Y) - b * mean(X)))) ^ 2)
  if (do.labels) {
    main <- paste(bquote(theta_1), " =", sprintf("%.2f", b), "\nSS =", 
                  sprintf("%.2f", SSy))
  } else {
    main <- ""
  }
  if (do.plot) {
    par(cex.lab = 2, cex.main = 2)
    plot(X, Y, type = 'n', main = main)
    points(mean(X), mean(Y), pch = 1, cex = 4, col = 'blue')
    abline(a = mean(Y) - b * mean(X), b = b, col = 'blue')
    for (i in 1:n) {
      segments(X[i], Y[i], X[i], X[i] * b + (mean(Y) - b * mean(X)), 
           col = 'red')
    }
    points(X, Y, pch = 16)
  }
  return(SSy)
}
```

## Frameworks for inference

1. Analytical
2. Maximum likelihood
3. Resampling
4. Bayesian

## For example...

1. Calibration curves
2. Metabolic rate vs. Body mass
3. Leaf area vs. Total rainfall

Continuous variable vs. continuous variable

## Linear regression

What values of $\theta_0$ and $\theta_1$ provide the best fit line through $Y$ as a function of $X$?

$$Y = \theta_0 + \theta_1 X$$

<center>
<img src="https://i.imgur.com/hbkIVLg.gif" width="30%" />
</center>

- How do we estimate $\theta_0$ and $\theta_1$?
- What is "best fit"?

## Generate data

Generate $n=30$ random data points: $X \sim \mathcal{N}(10, 1)$ and $Y = 2.3 X + \epsilon$, where $\epsilon \sim \mathcal{N}(1, 1)$:

```{r Generate_data}
set.seed(4)
n <- 30
X <- rnorm(n, mean = 10, sd = 1)
Y <- 2.3 * X + rnorm(n, mean = 1, sd = 1)
M <- data.frame(X, Y)
M %>% head()
```

## Generate data

```{r, echo=FALSE}
ggplot(M, aes(X, Y)) + geom_point()
```

## Analytical solution

An infinite range of possible slopes ($\theta_1$)

1. All pass through $\left(\bar{X}, \bar{Y}\right)$.
1. Sum of the squared deviations vary continuously.
1. Only one value of $\theta_1$ will minimize the SS.
    - The *Ordinary Least Squares* estimate

## Analytical calculation of $\theta_1$

$$\theta_1 = \frac{\sum\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{\sum\left(X_{i}-\bar{X}\right)^{2}}$$

Numerator:  Sum of the products of *X* and *Y*

Denominator: Sum of squares of *X*

## Analytical calculation of $\theta_0$

Because the OLS line must pass through $\left(\bar{X},\bar{Y}\right)$:

$$\theta_0 = \bar{Y} - \theta_1 \bar{X}$$

## Assumptions of OLS

At each *X*, there is a normally distributed population of *Y* observations with a mean at the regression line

- The variance of all *Y* observations is equal. 

Few assumptions are made about *X*

- *Is* measured without error
- Not that it is normal
- Not that it is randomly sampled
- Think about calibration curves. You set the *X* observations explicitly.

## Normally distributed population of *Y* observations

<center>
<img src="https://i.imgur.com/F0Y5Fva.png" width="70%" />
</center>

## Minimizing Sums of Squares

```{r echo=FALSE}
plot(X, Y, pch = 16, cex.lab = 2)
```

## Minimizing Sums of Squares

```{r echo=FALSE}
plot(X, Y, pch = 16, cex.lab = 2)
points(mean(X), mean(Y), pch = 1, cex = 4, col = 'blue')
text(9.25, 28, bquote(bar(Y) == .(round(mean(Y), 2))), cex = 2)
text(9.25, 27, bquote(bar(X) == .(round(mean(X), 2))), cex = 2)
```

## Minimizing Sums of Squares

```{r echo=FALSE, results='hide'}
# b = 0
ssPlot(X, Y, 0)
text(9.25, 28, bquote(bar(Y) == .(round(mean(Y), 2))), cex = 2)
```

## Minimizing Sums of Squares

```{r echo=FALSE, results='hide'}
# b = 0.5
ssPlot(X, Y, 0.5)
text(9.25, 28, bquote(bar(Y) == .(round(mean(Y), 2))), cex = 2)
```

## Minimizing Sums of Squares

```{r echo=FALSE, results='hide'}
# b = 1
ssPlot(X, Y, 1)
text(9.25, 28, bquote(bar(Y) == .(round(mean(Y), 2))), cex = 2)
```

## Minimizing Sums of Squares

```{r echo=FALSE, results='hide'}
# b = 1.5
ssPlot(X, Y, 1.5)
text(9.25, 28, bquote(bar(Y) == .(round(mean(Y), 2))), cex = 2)
```

## Minimizing Sums of Squares

```{r echo=FALSE, results='hide'}
# b = 2
ssPlot(X, Y, 2)
text(9.25, 28, bquote(bar(Y) == .(round(mean(Y), 2))), cex = 2)
```

## Minimizing Sums of Squares

```{r echo=FALSE, results='hide'}
# b = 2.1
ssPlot(X, Y, 2.1)
text(9.25, 28, bquote(bar(Y) == .(round(mean(Y), 2))), cex = 2)
```

## Minimizing Sums of Squares

```{r}
# Iteratively find the minimum SS
theta_1 <- seq(-10, 10, by = 0.01)

# data.frame to hold output
SumSq <- data.frame(theta_1 = theta_1,
                    SS = numeric(length(theta_1)))
head(SumSq)
```

## Minimizing Sums of Squares

```{r}
# Iterate through slopes
for (ii in 1:nrow(SumSq)) {
  theta_1 <- SumSq$theta_1[ii]
  SumSq$SS[ii] <- ssPlot(X, Y, theta_1, do.plot = FALSE)
}

# Location of minimum SS
minSS.theta_1 <- SumSq$theta_1[which.min(SumSq$SS)]
minSS.SS <- SumSq$SS[which.min(SumSq$SS)]
```

## Minimizing Sums of Squares

```{r echo=FALSE}
plot(SumSq,
     cex.lab = 1.5,
     type = "l", lwd = 2,
     xlab = TeX("$\\theta_1$"),
     ylab = "Sum of Squares")
points(minSS.theta_1, minSS.SS, col = "red", pch = 16, cex = 1.5)
text(-10, 250, paste("theta_1 =", minSS.theta_1, "\nSS =", round(minSS.SS, 2)),
     pos = 4, cex = 2)
```

## Frameworks for inference

1. ~~Analytical~~
2. Maximum likelihood
    - Minimizing the residual sum of squares is numerically equal to *maximizing* the model likelihood.
3. Resampling
4. Bayesian
