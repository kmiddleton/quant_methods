---
title: "Unit 9: In Class Discussion"
subtitle: "Quantitative Methods in Life Sciences"
author: 'Elizabeth King, Kevin Middleton and Lauren Sullivan'
output:
  ioslides_presentation:
    fig_width: 8
    css: styles.css
csl: evolution.csl
bibliography: Multivariate.bib
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(cowplot)
library(wesanderson)
library(lme4)

theme_set(theme_cowplot())
```


## $R^2$ and $P$

> What is the relationship between the p-value and R squared? Say the p-value is really small, but so is the R-squared value, what does that mean? Also, is there a typical way that sample size effect the R-squared value (i.e., does larger sample size tend to make smaller R-squared values)?


## Example Questions

*Arabidopsis* growth

We measured above ground biomass of plants after six weeks of growth in each of two treatment groups (n = 1000 per group). The means were 10.31 g (SEM = 0.001) for the control group and 10.42 (SEM = 0.002) for the fertilizer treatment group. A paired t-test showed highly significant difference of treatment ($t_{1998}$ = 52.4, P << 0.0001).

*Drosophila* larval density

You perform a multiple regression predicting wing area in Drosophila melanogaster using larval density and temperature. You have a very large data set (n = >1,000 observations). The R2 value for the regression is 0.02, and the overall P value for the linear model is 0.0003.


## Review Quiz 09-2


## Means and Intercepts

> I was getting confused in the lecture videos where when we allow for a random intercept effect it was stated that "we're allowing for each group to have a different mean, or intercept".  Maybe this is just a wording issue for me, but does that mean that we allow the groups to have different means and different intercepts, and in the graphs we see the effect on different intercepts more quickly?  The mean and the intercept are not the *same* values for each group though, correct?  But in building a model that allows for random intercept, it inherently is accompanied by the allowance of different means for each group?


## Intercepts as group means

```{r}
M <- read_csv("../data/Molerats.csv", col_types = c("cdd")) %>% 
  rename(Caste = caste,
         Mass = ln.mass,
         Energy = ln.energy) %>% 
  mutate(Caste = if_else(Caste == "worker", "Worker", "Non-worker") %>% 
           factor())
```


## ANOVA

```{r}
fm <- lm(Energy ~ Caste - 1, data = M)
```

```{r echo=FALSE}
M <- M %>% mutate(pred2 = predict(fm))
ggplot(M, aes(x = Mass, y = Energy, color = Caste)) +
  geom_point(size = 4) +
  geom_line(aes(x = Mass, y = pred2, color = Caste), lwd = 2) +
  scale_color_manual(values = wes_palette("Cavalcanti1")) +
  theme(legend.justification = c(0, 1), legend.position = c(0.05, 0.95)) +
  labs(x = "ln Body Mass (g)", y = "ln Daily Energy Expenditure (kJ)")
```


## Intercepts as group means

```{r}
coef(fm)

M %>% 
  group_by(Caste) %>% 
  summarize(`Mean Energy Expenditure` = mean(Energy),
            .groups = "drop")
```


## Thinking about coefficients

1. Categorical predictors
    - Group means or offsets from the `(Intercept)`
2. Continuous predictors
    - slopes measuring the change in the outcome resulting from a  unit increase in the predictor.


## ANCOVA, intercepts varying

```{r}
fm4 <- lm(Energy ~ Mass + Caste, data = M)
```

```{r echo=FALSE}
M <- M %>% mutate(pred4 = predict(fm4))
ggplot(M, aes(x = Mass, y = Energy, color = Caste)) +
  geom_point(size = 4) +
  geom_line(aes(x = Mass, y = pred4, color = Caste), lwd = 2) +
  scale_color_manual(values = wes_palette("Cavalcanti1")) +
  theme(legend.justification = c(0, 1), legend.position = c(0.05, 0.95)) +
  labs(x = "ln Body Mass (g)", y = "ln Daily Energy Expenditure (kJ)")
```


## ANCOVA, intercepts varying

```
Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept) -0.09687    0.94230  -0.103   0.9188
Mass         0.89282    0.19303   4.625 5.89e-05
CasteWorker  0.39334    0.14611   2.692   0.0112
```

- `(Intercept)`: Intercept for non-workers
- `Mass`: Slope for both groups
- `CasteWorker`: *Offset* for the Worker group (difference in intercepts)


## Visualization of Hierarchical Modeling

http://mfviz.com/hierarchical-models/


## Generate data

```{r}
set.seed(543675)
departments <- c('sociology', 'biology', 'english', 'informatics', 'statistics')
base.salaries <- c(40000, 50000, 60000, 70000, 80000)
annual.raises <- c(2000, 500, 500, 1700, 500)
faculty.per.dept <- 20
total.faculty <- faculty.per.dept * length(departments)

# Generate dataframe of faculty and (random) years of experience
ids <- 1:total.faculty
department <- rep(departments, faculty.per.dept)
experience <- floor(runif(total.faculty, 0, 10))
bases <- rep(base.salaries, faculty.per.dept) * runif(total.faculty, .9, 1.1) # noise
raises <- rep(annual.raises, faculty.per.dept) * runif(total.faculty, .9, 1.1) # noise
df <- data.frame(ids, department, bases, experience, raises)

# Generate salaries (base + experience * raise)
df <- df %>% mutate(
    salary = bases + experience * raises)
```


## Plotting

```{r, echo=FALSE}
p <- ggplot(df, aes(x = experience, y = salary, color = department)) +
  geom_point()
p
```


## No grouping

`salary ~ 1 + experience`

```{r, echo=FALSE}
fm1 <- lm(salary ~ 1 + experience, data = df)

preds <- crossing(department = unique(df$department),
                    experience = c(0, 10))
preds <- preds |> 
  mutate(fm1 = predict(fm1, newdata = preds))

p +
  geom_line(data = preds, aes(x = experience, y = fm1), color = "firebrick4",
            linewidth = 1)
```


## Group by department

`salary ~ 1 + experience + department`

```{r, echo=FALSE}
fm2 <- lm(salary ~ 1 + experience + department, data = df)

preds <- preds |> 
  mutate(fm2 = predict(fm2, newdata = preds))

p +
  geom_line(data = preds, aes(x = experience, y = fm2, color = department),
            linewidth = 1) +
  geom_line(data = preds, aes(x = experience, y = fm1), color = "firebrick4",
            linewidth = 1)
```


## Group by department, partial pooling

`salary ~ 1 + experience + (1 | department)`

```{r, echo=FALSE}
fm3 <- lmer(salary ~ 1 + experience + (1 | department), data = df)

preds <- preds |> 
  mutate(fm3 = predict(fm3, newdata = preds))

p +
  geom_line(data = preds, aes(x = experience, y = fm3, color = department),
            linewidth = 1) +
  geom_line(data = preds, aes(x = experience, y = fm1), color = "firebrick4",
            linewidth = 1)
```


## Predictions

```{r, echo=FALSE}
preds |> knitr::kable()
```


## Slope by department

`salary ~ 1 + experience * department`

```{r, echo=FALSE}
fm4 <- lm(salary ~ 1 + experience * department, data = df)

preds <- preds |> 
  mutate(fm4 = predict(fm4, newdata = preds))

p +
  geom_line(data = preds, aes(x = experience, y = fm4, color = department),
            linewidth = 1)
```


## Slope by department, partial pooling

`salary ~ 1 + experience + (1 + experience | department)`

```{r, echo=FALSE}
fm5 <- lmer(salary ~ 1 + experience + (1 + experience | department), data = df)

preds <- preds |> 
  mutate(fm5 = predict(fm5, newdata = preds))

p +
  geom_line(data = preds, aes(x = experience, y = fm5, color = department),
            linewidth = 1) +
  geom_line(data = preds, aes(x = experience, y = fm4, color = department),
            linewidth = 1, linetype = "dashed") +
  geom_line(data = preds, aes(x = experience, y = fm1), color = "firebrick4",
            linewidth = 1)
```


