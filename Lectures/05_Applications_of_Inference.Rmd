---
title: "Applications of Inference Frameworks"
subtitle: "Quantitative Methods in Life Sciences"
author: 'Elizabeth King and Kevin Middleton'
date: 'Last updated: `r Sys.Date()`'
output:
  xaringan::moon_reader:
    seal: yes
    nature:
      highlightStyle: github
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rethinking)
library(cowplot)
library(RcppEigen)
library(plot3D)
```

# Notes

---

# Readings

---

# Three frameworks

1. Frequentist
2. Maximum likelihood
3. Bayesian

---

# Inferring a mean

Mean undulation rate for $n = 8$ gliding snakes (http://www.flyingsnake.org/):

```{r}
undulation_rate <- c(0.9, 1.2, 1.2, 1.3, 1.4, 1.4, 1.6, 2.0)
```

![](http://www.lazerhorse.org/wp-content/uploads/2015/01/Flying-Snake-Chrysopelea.jpg)

What is the mean undulation rate for this sample of flying snakes?

---

```{r undulation_plot, message=FALSE}
ggplot(data_frame(undulation_rate), aes(undulation_rate)) +
  geom_histogram() +
  labs(x = "Undulation Rate (Hz)", y = "Count")
```

---

# Frequentist inference of mean

Arthimetic mean:

$$\hat{Y} = \frac{\sum_{i=1}^{n}Y_i}{n}$$

---

# Frequentist inference of mean

```{r}
sum(undulation_rate) / length(undulation_rate)
```

---

# Maximum likelihood inference of mean

Define a function to calculate the probability of an observed value $Y_i$ given the mean ($\mu$) and standard deviation ($\sigma$). Default to the standard normal distribution $\mathcal{N}(0,1)$.

$$f\left(Y_i; \mu, \sigma\right) = \frac{1}{\sqrt{2\pi\sigma^{2}}} e^{\frac{-\left(Y_i-\mu\right)^{2}}{2\sigma^{2}}}$$

```{r}
normal <- function(Y, mu = 0, sigma = 1) {
  1 / sqrt(2 * pi * sigma ^ 2) * 
    exp((-(Y - mu) ^ 2 / (2 * sigma ^ 2)))
}
```

_Note_: this function is built into R as `dnorm()`.

---

# Test our function

```{r}
normal(0, mu = 0, sigma = 1)
dnorm(0, mean = 0, sd = 1)
```

```{r normal_plot, echo=FALSE, fig.height=2.5}
M <- data_frame(x = seq(-3, 3, length = 100),
                y = dnorm(x))
ggplot(M, aes(x, y)) + geom_line() +
  labs(x = "Y", y = "Probability")
```

---

# Calculating a likelihood

If the true mean is 0 and the standard deviation is 1, what is the likelihood of the observed values?

1. This is a model.
2. Calculate the probability of each observation.
3. Overall model likelihood is the product of the individual probabilities.
4. log-likelihood is more tractable, so calculate that.

---

# Model Likelihood ($\mathcal{L}$)

For a set of $Y_i$ and parameters ($\Theta$; i.e., mean and standard deviation) the likelihood of the model is the product of their individual probabilities:

$$\mathcal{L}\left(\left\{ Y_{i}\right\} _{i=1}^{n};\Theta\right) = \prod_{i=1}^{n}\phi\left(Y_{i}; \Theta\right)$$

Evaluate the likelihood function for different values of $\Theta$ to estimate $\mathcal{L}$ for different sets of $\Theta$.

- Maximize $\mathcal{L}$ and you will have the maximum likelihood set of parameter estimates.

---

# Model Likelihood ($\mathcal{L}$)

Probabilities are in the range 0 to 1, so taking the product of a large number of probabilities can result in some very small numbers.

- Computers don't handle really small numbers very well
    - There is a lower limit to the smallest number a computer can keep track of.
    - This is $`r .Machine$double.xmin`$ in R on the computer that compiled these slides.

Think about computing the likelihood for thousands or millions of observations.

---

# Model Likelihood ($\mathcal{L}$)

It's usually easier to minimize the (natural) log of the likelihood function. The log-likelihood is easier to deal with mathematically.

Log both sides of the equation:

$$\log\left(\mathcal{L}\left(\left\{ Y_{i}\right\} _{i=1}^{n};\Theta\right)\right) = \log\left(\prod_{i=1}^{n}\phi\left(Y_{i};\Theta\right)\right)$$

---

# Model Likelihood ($\mathcal{L}$)

Taking advantage of the algebraic rules associated with logs (the log of the products equals the sum of the logs):

$$\log\left(\mathcal{L}\left(\left\{ Y_{i}\right\} _{i=1}^{n};\Theta\right)\right) = \sum_{i=1}^{n} \log\left(\phi\left(Y_{i};\Theta\right)\right)$$

So we just need to sum the log-likelihoods to get the overall model likelihood. 

_Note_: `log()` is _natural_ log.

---

# Calculating the log-likelihood for a single combination of $\mu$ and $\sigma$

If the mean is 0 and the standard deviation is 1, what is the likelihood of the observed values?

Probability for the first observation (`undulation_rate[1]`):

```{r}
undulation_rate[1]
normal(undulation_rate[1], mu = 0, sigma = 1)
```

---

```{r, echo=FALSE, fig.height=4.5}
ggplot(M, aes(x, y)) + geom_line() +
  labs(x = "Y", y = "Probability") +
  geom_point(aes(x = undulation_rate[1],
                 y = normal(undulation_rate[1], mu = 0, sigma = 1)),
             color = "red",
             size = 3)
```

This is only the probability for first observation. We need the likelihoods for all `r length(undulation_rate)` undulation rates to get a model likelihood.

---

# Calculating the log-likelihood for a single combination of $\mu$ and $\sigma$

Vector of probabilities for all values in `undulation_rate` given `mu = 0` and `sigma = 1`:

```{r}
(probs <- normal(undulation_rate, mu = 0, sigma = 1))
```

Likelihood is the product of those probabilities:

```{r}
(lik <- prod(probs))
```

---

# Likelihood to log-likelihood

```{r}
log(lik)
sum(log(probs))
```

If the true mean is 0 and true standard deviation is 1, then the model log-likelihood is `r round(log(lik), 3)`.

Is there another combination of $\mu$ and $\sigma$ that gives a higher likelihood (= larger log-likelihood)?

```{r}
sum(log(normal(undulation_rate, mu = 1, sigma = 1)))
```

---

# Calculating the log-likelihood for a _range_ of $\mu$ and $\sigma$

Find the combination of $\mu$ and $\sigma$ that maximizes the log-likelihood of the model for the mean and standard deviation of undulation rates.

Ranges of possible values:

1. Mean ($\mu$): $-\infty < \mu < \infty$
2. Standard deviation ($\sigma$): $0 < \sigma < \infty$

---

# Grid approximation

For combinations of $\mu$ and $\sigma$, calculate the model likelihood. Pick the largest log-likelihood as the maximum likelihood estimates.

Set up the grid:

```{r}
n <- 100
mus <- seq(0.1, 5, length = n)
sigmas <- seq(0.1, 2, length = n)

grid_mu_sigma <- mesh(mus, sigmas)

grid_approx <- data_frame(
  mus = as.numeric(grid_mu_sigma$x),
  sigmas = as.numeric(grid_mu_sigma$y),
  log_lik = rep(NA, length = n ^ 2)
)
```

---

```{r}
grid_approx
```

---

# Grid approximation

```{r grid_approx, cache=TRUE}
for (i in 1:nrow(grid_approx)) {
  grid_approx[i, 3] <- 
    sum(log(normal(undulation_rate,
                   mu = grid_approx$mus[i],
                   sigma = grid_approx$sigmas[i])))
}
head(grid_approx)
```

This approach is coarse and time consuming. For $n = `r n`$, there are `r nrow(grid_approx)` comparisons.

---

# Grid approximation

```{r webgl=TRUE, echo=FALSE}
grid_approx <- do.call(data.frame,
                       lapply(grid_approx,
                              function(x) replace(x, is.infinite(x), NA)))
scatter3D(grid_approx$mus, grid_approx$sigmas, grid_approx$log_lik, pch = 16)
```

---

# Grid approximation

On this grid, the maximum likelihood estimates of $\mu$ and $\sigma$ are:

```{r}
grid_approx[which.max(grid_approx$log_lik), ]
```

---

# Maximum likelihood as an optimization problem

```{r}
log_lik <- function(x, Y){
  probs <- normal(Y, mu = x[1], sigma = x[2])
  return(sum(log(probs)))
}
```

---

# Maximum likelihood as an optimization problem

```{r ML_optim, cache = TRUE}
optim(c(0.1, 0.1),
      log_lik,
      Y = undulation_rate,
      control = list(fnscale = -1,
                     reltol = 10^-100))
```

---

# Maximum likelihood as an optimization problem

```{r ML_glm}
fm <- glm(undulation_rate ~ 1)
coef(fm)
logLik(fm)
```

---

# Bayesian inference of mean

Ranges of possible maximum likelihood values:

1. $\mu$: $-\infty < \mu < \infty$
2. $\sigma$: $0 < \sigma < \infty$

Drawbacks:

1. $\mu$ can't be negative (no negative undulation rates) and probably isn't a large number
2. $\sigma$ is also probably not huge either

Can we do better? Yes, Bayesian priors.

---

# Prior for the mean

```{r, ref.label="undulation_plot", echo=FALSE, message=FALSE}

```

---

# Prior for the mean

Cauchy distribution (location = 0, scale = 2)

```{r echo=FALSE, fig.height=3.5}
data_frame(
  x = seq(0, 10, length = 100),
  y = dcauchy(x, scale = 2)) %>% 
  ggplot(aes(x, y)) + geom_line()
```

---

# Bayesian model

```{r undulation_bayes, cache = TRUE}
fm_priors <- map2stan(
  alist(
    undulation_rate ~ dnorm(mu, sigma),
    mu ~ dcauchy(0, 2),
    sigma ~ dcauchy(0, 5)),
  data = data.frame(undulation_rate),
  WAIC = FALSE,
  iter = 10^4,
  constraints = list(mu = "lower=0")
)
```

Using the `map2stan()` from the rethinking package (https://github.com/rmcelreath/rethinking).

---

# Inspecting the samples

```{r echo=FALSE}
plot(fm_priors)
```

---

# Summarizing the results

```{r, echo=FALSE}
post <- extract.samples(fm_priors) %>% as_data_frame()
post %>% gather(key, value) %>% 
  ggplot(aes(value)) +
  geom_density() +
  facet_grid(. ~ key)
```

---

# Summarizing the results

```{r}
precis(fm_priors, digits = 5)
```

---

# Linear regression

$$Y = \theta_1 + \theta_2 X$$

Where

- $Y$ is the response variable
- $X$ is the explanatory variable
- $\theta_1$ is the intercept ($a$)
- $\theta_2$ is the slope of the line ($b$)

---

# Linear regression

What values of $\theta_1$ and $\theta_2$ provide the best fit line through $Y$ as a function of $X$?

$$Y = \theta_1 + \theta_2 X$$

How do we estimate $\theta_1$ and $\theta_2$?

---

# Generate data

Generate $n=30$ random data points: $X \sim \mathcal{N}(10, 1)$ and $Y = 2.3 X + \epsilon$, where $\epsilon \sim \mathcal{N}(1, 1)$:

```{r Generate_data}
set.seed(4)
n <- 30
X <- rnorm(n, mean = 10, sd = 1)
Y <- 2.3 * X + rnorm(n, mean = 1, sd = 1)
M <- data.frame(X, Y)
M %>% head()
```

---

# Generate data

```{r, echo=FALSE}
ggplot(M, aes(X, Y)) + geom_point()
```

---

# Solve analytically

$Y$ is a (n x 1) vector of observed values
$X$ is an (n X 2) matrix of ones followed by observations

$$\theta = (X'X)^{-1} X'Y$$

Where $X'$ is the transpose of $X$ and $X{-1}$ is the inverse [See: https://www.mathsisfun.com/algebra/matrix-inverse.html].

```{r}
(X_mat <- matrix(c(rep(1, n), X), ncol = 2))
```

---

# Solve analytically

```{r}
(theta <- (solve(t(X_mat) %*% X_mat)) %*% (t(X_mat) %*% Y))
```

`theta` is a 2 x 1 matrix of coefficients:

$$
\theta=\left[\begin{array}{c}
`r round(theta[1, 1], 3)`\\
`r round(theta[2, 1], 3)`
\end{array}\right]
$$

---

```{r}
lm_fast <- function(X, Y) {
  X_mat <- matrix(cbind(rep(1, length(Y)), X), nrow = length(Y))
  theta <- (solve(t(X_mat) %*% X_mat, tol = 1e-25)) %*% (t(X_mat) %*% Y)
  return(theta)
}

predict_Y <- function(theta, X) {
  X <- as.matrix(X, nrow = length(theta))
  Y_hat <- theta[1, 1] + rowSums(theta[2:nrow(theta), 1] * X)
  return(Y_hat)
}

log_lik <- function(Y, Y_hat) {
  var_hat <- sum((Y - Y_hat)^2) / (length(Y))
  sd_hat <- sqrt(var_hat)
  probs_Y <- dnorm(Y, mean = Y_hat, sd = sd_hat)
  LL <- sum(log(probs_Y))
  return(LL)
}
```

---

```{r}
theta <- lm_fast(X, Y)
Y_hat <- predict_Y(theta, X)
(LL <- log_lik(Y, Y_hat))

fm <- lm(Y~X)
logLik(fm)

X_mat <- matrix(c(rep(1, length(X)), X), ncol = 2)
fm_fast <- fastLmPure(X_mat, as.matrix(Y))
log_lik(Y, fm_fast$fitted.values)
```

---

```{r}
t1 <- Sys.time()

reps <- 10^4
liks <- numeric(length = reps)

load("~/Dropbox/House/Aprob.rda")
n <- nrow(Aprob)

# Add column of 1's for intercept term
X <- as.matrix(cbind(rep(1, n), Aprob[, 2:9]), nrow = n)

set.seed(5)

for (i in 1:reps) {
  Y <- rexp(n, rate = 10)
  
  # Normalize so each row sums to 1.
  fm_fast <- fastLmPure(X, Y)
  liks[i] <- log_lik(Y, fm_fast$fitted.values)
}
Sys.time() - t1

ggplot(as.data.frame(liks), aes(x = 1:length(liks), y = liks)) + geom_path()
```

```{r}
```

