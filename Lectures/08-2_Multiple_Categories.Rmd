---
title: "Linear Models: Multiple Categories"
subtitle: "Quantitative Methods in Life Sciences"
author: 'Elizabeth King, Kevin Middleton and Lauren Sullivan'
output:
  ioslides_presentation:
    fig_width: 8
    css: styles.css
csl: evolution.csl
bibliography: Multivariate.bib
---


```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(cowplot)
theme_set(theme_cowplot())
library(multcomp)
source("Shade_Distributions.R")
```


## Hierarchy of GLMs

<center>
<img src="https://i.imgur.com/sdx4fMz.png" width="100%" />
</center>


## Problem

<center>
<img src="https://i.imgur.com/GsL12KL.jpg" width="90%" />
</center>


## Prediction

> Outcome: Juvenile Survival

<div class="columns-2">

Possible predictors:

- Birth date  
- Birth mass
- Year
- Sex
- Maternal fecundity
- Maternal reproductive status

<br />

- Maternal age
- Population size
- Temperature (min, max, ave...)
- Rainfall
- Wind speed
- ...

</div>


## Prediction with categorical variables

We have done prediction with only two groups (levels)

- Are the means different?

- Horned lizards example

Now predict with three or more groups

- Is *at least* one mean different?

- Then, *which* mean(s) is/are different


## Does bright light treatment alleviate jet lag symptoms?

<div class="columns-2">

<center>
<img src="https://i.imgur.com/X8kUEJq.jpg" width="100%" />
<br />
<br />
<br />

</center>

- 3 groups
- No light (control)
- Bright light in eyes
- Bright light in knees
- Outcome
- Shift in circadian pattern (hours)

</div>


## ANOVA

A more general version of a two-sample *t*-test for *k* groups:

- H~0~: $\mu_{1}=\mu_{2}=\mu_{3}\dots=\mu_{k}$
- H~A~: At least one mean is different (but we won't know which yet)

One-way ANOVA: a single categorical variable

- We will generalize to more variables later


## "Analysis of Variance"

Some total variability in *Y*:

1. Part explained by group membership
1. Part remains unexplained ("error" or "residual")

$F$-statistic is the ratio of the two.


## Assumptions of ANOVA

1. The measurments in every group represent a random sample from the corresponding population 
1. The variable is normally distributed in each population 
1. The variance is the same in all populations

- ANOVA is very robust to violations of these assumptions
- Central limit theorem!
- Equal sample sizes helps with violations of equal variances
- ANOVA is the *Maximum Likelihood Solution* given these assumptions


## Does bright light treatment alleviate jet lag symptoms?

<div class="columns-2">

<center>
<img src="https://i.imgur.com/X8kUEJq.jpg" width="100%" />
<br />
<br />
<br />

</center>

- 3 groups
- No light (control)
- Bright light in eyes
- Bright light in knees
- Outcome
- Shift in circadian pattern (hours)

</div>


## Does bright light treatment alleviate jet lag symptoms?

```{r message=FALSE}
JL <- read_csv("../data/JetLag.csv") %>% 
  mutate(Treatment = factor(Treatment))
```

```{r echo=FALSE, fig.height = 3.5}
set.seed(2)
ggplot(JL, aes(x = Treatment, y = Shift)) +
  geom_point(position = position_jitter(width = 0.1)) +
  xlab("Light Treatment") +
  ylab("Shift in Circadian Rhythm (h)")
```


## Compare group means and standard deviations

```{r}
JL %>% group_by(Treatment) %>%
  summarise(mean(Shift), sd(Shift),
            .groups = "drop")
```


## Compare group means and standard errors

```{r echo=FALSE}
ggplot(JL, aes(x = Treatment, y = Shift)) +
  geom_point(position = position_jitter(width = 0.1), size = 2) +
  xlab("Light Treatment") +
  ylab("Shift in Circadian Rhythm (h)") +
  stat_summary(fun.data = "mean_se",
               position = position_dodge(width = 0.5),
               colour = "red",
               size = 1)

```


## ANOVA as a Linear Model {.smaller}

```{r}
fm_lm <- lm(Shift ~ Treatment, data = JL)
summary(fm_lm)
```


## Parameter estimates

```
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)
## (Intercept)   -0.30875    0.24888  -1.241  0.22988
## Treatmenteyes -1.24268    0.36433  -3.411  0.00293
## Treatmentknee -0.02696    0.36433  -0.074  0.94178
```

1. `(Intercept)`: Mean for 1st level of factor
1. `Treatmenteyes`: Adjustment from `(Intercept)` for eyes group (`Intercept` + `Treatmenteyes`)
1. `Treatmentknee`: Adjustment from `(Intercept)` for knee group (`Intercept` + `Treatmentknee`)

*t*-tests and *P* values are suggestive of significant differences, but we need to look at the overall ANOVA first.


## ANOVA table from a Linear Model

```{r}
anova(fm_lm)
```

*P* = 0.004 for the overall ANOVA.

It is possible to have a significant parameter estimate and a non-significant overall ANOVA.

- Nothing more to do.
- No post hoc test


## Visualizing ANOVA

<center>
<img src="https://i.imgur.com/dNF4ph0.png" width="100%" />
</center>

<br />
<br />

$$F = \frac{\mbox{Between Group Variation}}{\mbox{Within Group Variation}}$$


## Model Matrix

```{r, eval=FALSE}
fm_lm <- lm(Shift ~ Treatment, data = JL)
```

`model.matrix()` will give you the model for the predictors

```{r, echo=FALSE}
set.seed(5)
mm <- as.data.frame(model.matrix( ~ Treatment, data = JL))
mm$Category <- JL$Treatment
mm$Shift <- JL$Shift
mm <- mm[ , c(5, 1, 2, 3, 4)]
mm <- mm[sample(1:nrow(mm)), ]
rownames(mm) <- seq(1, nrow(mm))
mm
```


## *F* distribution

Two different degrees of freedom:

- Numerator: *k* groups - 1
- Denominator: *N* - *k*

```{r echo=FALSE}
JL %>% group_by(Treatment) %>% tally()
```

## Shape of the *F*-distribution

```{r echo=TRUE}
shade_F(0.05, df1 = 2, df2 = 19, vline = 7.28) +
  geom_vline(xintercept = 7.28, color = "blue")
```


## Parts of an ANOVA table

```{r echo=FALSE}
anova(fm_lm)
```

- `Sum Sq`: Variability accounted for by that part of the ANOVA
- `Mean Sq`: `Sum Sq` / `Df`
- `F value`: `Mean Sq` Treatment / `Mean Sq` Residual
- `Pr(>F)`: *P*-value for the *F*-test of that variable


## Role of variation

Large values of $F$ are more likely to be significant.

$F$ is a ratio:

$$F = \frac{\mbox{MS}_{group}}{\mbox{MS}_{error}}$$

What role does within vs. between group variation have in determining $F$?

## If you perfomed pairwise t-tests for all pairs of groups, how often would you expect to get a significant result by chance?

## If you perfomed pairwise t-tests for all pairs of groups, how often would you expect to get a significant result by chance?

- 1 out of every 20 tests
- $\alpha$ for a single comparison?
- $\alpha$ for a single test?
- $\alpha$ for an experiment?
- What do you feel comfortable with?

*Stay tuned for multiple comparisons*


## Post hoc tests for ANOVA

Significant ANOVA (*P* = 0.004) only says that at least one mean is different.

Many options are available for post hoc (unplanned) comparisons:

- Scheff√© test
- Duncan's multiple range test
- Fisher's least significant difference test
- Newman-Keuls test
- Tukey-Kramer test (Tukey's Honestly Significant
Difference)

We'll use the Tukey-Kramer test (only one you need to know right now)


## Tukey-Kramer test

- Assumes that we have already performed an ANOVA and rejected the null hypothesis
- If the overall ANOVA is not significant, then a post hoc test makes no sense.
- The familywise error rate (FWER) with a Tukey-Kramer test is no larger than $\alpha$.
- FWER is the overall probability of a Type I error
- Tukey-Kramer test makes all the same assumptions as ANOVA.
- Defaults to all pairwise combinations of levels


## Tukey-Kramer test: `multcomp` library {.smaller}

- Alternative but less general: `TukeyHSD()` in R
- Only takes an object from `aov()`

```{r}
tukey <- glht(fm_lm, linfct = mcp(Treatment = "Tukey"))
summary(tukey)
```


## Selective Set of Comparisons {.smaller}

```{r}
post_hoc <- glht(fm_lm,
                 mcp(Treatment = c("eyes - control = 0",
                                   "knee - control = 0")))
summary(post_hoc)
```
