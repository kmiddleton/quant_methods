---
title: "Measures of Central Tendency, part 2"
subtitle: "Quantitative Methods in Life Sciences"
author: 'Elizabeth King, Kevin Middleton, and Lauren Sullivan'
output:
  ioslides_presentation:
    fig_width: 8
    css: styles.css
csl: evolution.csl
bibliography: Multivariate.bib
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rstan)
library(cowplot)
library(bayesplot)

color_scheme_set(scheme = "viridis")
theme_set(theme_cowplot())

undulation_rate <- c(0.9, 1.2, 1.2, 1.3, 1.4, 1.4, 1.6, 2.0)
```

## Frameworks for inference

1. Analytical
2. Maximum likelihood
3. Resampling
4. Bayesian

## Resampling inference of the mean

```{r}
set.seed(36428)
reps <- 100000
resampled_mean <- numeric(length = reps)

for (i in 1:reps) {
  resampled_mean[i] <- mean(sample(undulation_rate,
                                   replace = TRUE))
}
```

## Resampling inference of the mean

```{r echo=FALSE, message = FALSE}
tibble(resampled_mean) %>% 
  ggplot(aes(resampled_mean)) +
  geom_histogram(bins = 20) +
  labs(x = "Resampled Mean", y = "Count") +
  geom_vline(xintercept = mean(resampled_mean), color = "red",
             size = 1.5)
```

## Resampling inference of the mean

```{r}
mean(undulation_rate)
mean(resampled_mean)
```

**Given enough iterations, the resampled mean equals the analytical mean (and equals the maximum likelihood mean).**

## Bayesian vs. ML inference

Maximum likelihood inference:

- Probability of the data, given the parameter estimate
- Parameters are fixed; data varies.
- No prior possible

Bayesian inference:

- Probability of the parameters, given the data
- Data are fixed; parameters vary.
- Prior required

## Bayesian inference of the mean

Ranges of possible maximum likelihood values:

1. $\mu$: $-\infty < \mu < \infty$
2. $\sigma$: $0 < \sigma < \infty$

Drawbacks:

1. $\mu$ can't be negative (no negative undulation rates) and probably isn't a large number
2. $\sigma$ is also probably not huge either

Can we do better? Yes, Bayesian priors.

## Prior for the mean

```{r, echo=FALSE, message=FALSE}
ggplot(tibble(undulation_rate), aes(undulation_rate)) +
  geom_histogram() +
  labs(x = "Undulation Rate (Hz)", y = "Count")
```

## Prior for the mean

Cauchy distribution (location = 0, scale = 3)

```{r, echo=FALSE, message=FALSE}
tibble(
  x = seq(0, 10, length = 100),
  y = dcauchy(x, scale = 3)) %>% 
  ggplot(aes(x, y)) + geom_line() +
  labs(y = "Relative Likelihood", x = "value")
```

## Bayesian model

[stan](http://mc-stan.org/) code:

```{r}
model <- "
  data{
    int<lower=1> N;
    real undulation_rate[N];
  }
  parameters{
    real<lower=0> mu;
    real<lower=0> sigma;
  }
  model{
    mu ~ cauchy(0, 3);
    sigma ~ exponential(1);
    undulation_rate ~ normal(mu, sigma);
  }
"
```

## Sample the Bayesian model

```{r stan_fit, message=FALSE, warning=FALSE, results="hide"}
fm_priors <- stan(
  model_code = model,
  data = list(undulation_rate = undulation_rate,
              N = length(undulation_rate)),
  iter = 1e5,
  chains = 4)
```

## Inspecting the samples

```{r echo=FALSE, message=FALSE, warning=FALSE}
post <- as.array(fm_priors)
mcmc_trace(post, pars = c("mu", "sigma"))
```

## Summarizing the results

```{r, echo=FALSE}
mcmc_dens(as.data.frame(fm_priors),
          pars = c("mu", "sigma"))
```

## Summarizing the results

```{r echo=FALSE}
print(fm_priors, digits = 3)
```

Lower mean than the analytical or ML estimate (`r round(mean(undulation_rate), 3)`) because the prior places less probability on high values.
