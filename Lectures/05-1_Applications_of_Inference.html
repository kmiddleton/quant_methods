<!DOCTYPE html>
<html>
  <head>
    <title>Applications of Inference Frameworks</title>
    <meta charset="utf-8">
    <meta name="author" content="Elizabeth King and Kevin Middleton" />
    <link href="05-1_Applications_of_Inference_files/remark-css-0.0.1/example.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse

# Applications of Inference Frameworks
## Quantitative Methods in Life Sciences
### Elizabeth King and Kevin Middleton
### Last updated: 2017-01-13

---




# Notes

---

# Readings

---

# Three frameworks for inference

1. Analytical
2. Maximum likelihood
3. Bayesian

---

# Inferring a mean

Mean undulation rate for `\(n = 8\)` gliding snakes (http://www.flyingsnake.org/):


```r
undulation_rate &lt;- c(0.9, 1.2, 1.2, 1.3, 1.4, 1.4, 1.6, 2.0)
```

![](http://www.lazerhorse.org/wp-content/uploads/2015/01/Flying-Snake-Chrysopelea.jpg)

What is the mean undulation rate for this sample of flying snakes?

---


```r
ggplot(data_frame(undulation_rate), aes(undulation_rate)) +
  geom_histogram() +
  labs(x = "Undulation Rate (Hz)", y = "Count")
```

&lt;img src="05-1_Applications_of_Inference_files/figure-html/undulation_plot-1.png" width="672" /&gt;

---

# Analytical inference of mean

Arithmetic mean:

`$$\hat{Y} = \frac{\sum_{i=1}^{n}Y_i}{n}$$`

`$$mean~undulation~rate = \frac{\sum_{i=1}^{n}undulation~rate_i}{n}$$`

---

# Analytical inference of mean


```r
sum(undulation_rate) / length(undulation_rate)
```

```
## [1] 1.375
```

---

# Maximum likelihood inference of mean

Define a function to calculate the probability of an observed value `\(Y_i\)` drawn from a [normal distribution](http://mathworld.wolfram.com/NormalDistribution.html) given the mean (`\(\mu\)`) and standard deviation (`\(\sigma\)`). Default to the [standard normal distribution](http://mathworld.wolfram.com/StandardNormalDistribution.html) `\(\mathcal{N}(0,1)\)`.

`$$f\left(Y_i; \mu, \sigma\right) = \frac{1}{\sqrt{2\pi\sigma^{2}}} e^{\frac{-\left(Y_i-\mu\right)^{2}}{2\sigma^{2}}}$$`


```r
normal &lt;- function(Y, mu = 0, sigma = 1) {
  1 / sqrt(2 * pi * sigma ^ 2) * 
    exp((-(Y - mu) ^ 2 / (2 * sigma ^ 2)))
}
```

_Note_: this function is built into R as `dnorm()`.

---

# Test our function


```r
normal(0, mu = 0, sigma = 1)
```

```
## [1] 0.3989423
```

```r
dnorm(0, mean = 0, sd = 1)
```

```
## [1] 0.3989423
```

&lt;img src="05-1_Applications_of_Inference_files/figure-html/normal_plot-1.png" width="672" /&gt;

---

# Calculating a likelihood

If the true mean is 0 and the standard deviation is 1, what is the likelihood of the observed values?

1. This is a model.
2. Calculate the probability of each observation.
3. Overall model likelihood is the product of the individual probabilities.
4. log-likelihood is more tractable, so calculate that.

---

# Model Likelihood (`\(\mathcal{L}\)`)

For a set of `\(Y_i\)` and parameters (`\(\Theta\)`; i.e., mean and standard deviation) the likelihood of the model is the product of their individual probabilities:

`$$\mathcal{L}\left(\left\{ Y_{i}\right\} _{i=1}^{n};\Theta\right) = \prod_{i=1}^{n}\phi\left(Y_{i}; \Theta\right)$$`

Evaluate the likelihood function for different values of `\(\Theta\)` to estimate `\(\mathcal{L}\)` for different sets of `\(\Theta\)`.

- Maximize `\(\mathcal{L}\)` and you will have the maximum likelihood set of parameter estimates.

---

# Model Likelihood (`\(\mathcal{L}\)`)

Probabilities are in the range 0 to 1, so taking the product of a large number of probabilities can result in some very small numbers.

- Computers don't handle really small numbers very well
    - There is a lower limit to the smallest number a computer can keep track of.
    - This is `\(2.2250739\times 10^{-308}\)` in R on the computer that compiled these slides.

Think about computing the likelihood for thousands or millions of observations.


```r
.50^1000
```

```
## [1] 9.332636e-302
```

---

# Model Likelihood (`\(\mathcal{L}\)`)

It's usually easier to minimize the (natural) log of the likelihood function. The log-likelihood is easier to deal with mathematically.

`$$\mathcal{L}\left(\left\{ Y_{i}\right\} _{i=1}^{n};\Theta\right) = \prod_{i=1}^{n}\phi\left(Y_{i}; \Theta\right)$$`

Log both sides of the equation:

`$$\log\left(\mathcal{L}\left(\left\{ Y_{i}\right\} _{i=1}^{n};\Theta\right)\right) = \log\left(\prod_{i=1}^{n}\phi\left(Y_{i};\Theta\right)\right)$$`

---

# Model Likelihood (`\(\mathcal{L}\)`)

Taking advantage of the algebraic rules associated with logs (the log of the products equals the sum of the logs):

`$$\log\left(\mathcal{L}\left(\left\{ Y_{i}\right\} _{i=1}^{n};\Theta\right)\right) = \sum_{i=1}^{n} \log\left(\phi\left(Y_{i};\Theta\right)\right)$$`

So we just need to sum the log-likelihoods to get the overall model likelihood. 

_Note_: `log()` is _natural_ log.

---

# Calculating the log-likelihood for a single combination of `\(\mu\)` and `\(\sigma\)`

If the mean is 0 and the standard deviation is 1, what is the likelihood of the observed values?

Probability for the first observation (`undulation_rate[1]`):


```r
undulation_rate[1]
```

```
## [1] 0.9
```

```r
normal(undulation_rate[1], mu = 0, sigma = 1)
```

```
## [1] 0.2660852
```

---

&lt;img src="05-1_Applications_of_Inference_files/figure-html/unnamed-chunk-7-1.png" width="672" /&gt;

This is only the probability for first observation. We need the likelihoods for all 8 undulation rates to get a model likelihood.

---

# Calculating the log-likelihood for a single combination of `\(\mu\)` and `\(\sigma\)`

Vector of probabilities for all values in `undulation_rate` given `mu = 0` and `sigma = 1`:


```r
(probs &lt;- normal(undulation_rate, mu = 0, sigma = 1))
```

```
## [1] 0.26608525 0.19418605 0.19418605 0.17136859 0.14972747 0.14972747
## [7] 0.11092083 0.05399097
```

Overall likelihood is the product of those probabilities:


```r
(lik &lt;- prod(probs))
```

```
## [1] 2.308476e-07
```

---

# Likelihood to log-likelihood


```r
log(lik)
```

```
## [1] -15.28151
```

```r
sum(log(probs))
```

```
## [1] -15.28151
```

If the true mean is 0 and true standard deviation is 1, then the model log-likelihood is -15.282.

Is there another combination of `\(\mu\)` and `\(\sigma\)` that gives a higher likelihood (= larger log-likelihood)?


```r
sum(log(normal(undulation_rate, mu = 1, sigma = 1)))
```

```
## [1] -8.281508
```

---

# Calculating the log-likelihood for a _range_ of `\(\mu\)` and `\(\sigma\)`

Find the combination of `\(\mu\)` and `\(\sigma\)` that maximizes the log-likelihood of the model for the mean and standard deviation of undulation rates.

Ranges of possible values:

1. Mean (`\(\mu\)`): `\(-\infty &lt; \mu &lt; \infty\)`
2. Standard deviation (`\(\sigma\)`): `\(0 &lt; \sigma &lt; \infty\)`

---

# Grid approximation

For combinations of `\(\mu\)` and `\(\sigma\)`, calculate the model likelihood. Pick the largest log-likelihood as the maximum likelihood estimates.

Set up the grid:


```r
n &lt;- 100
mus &lt;- seq(0.1, 5, length = n)
sigmas &lt;- seq(0.1, 2, length = n)

grid_mu_sigma &lt;- mesh(mus, sigmas)

grid_approx &lt;- data_frame(
  mu = as.numeric(grid_mu_sigma$x),
  sigma = as.numeric(grid_mu_sigma$y),
  log_lik = rep(NA, length = n ^ 2)
)
```

---


```r
grid_approx
```

```
## # A tibble: 10,000 × 3
##           mu sigma log_lik
##        &lt;dbl&gt; &lt;dbl&gt;   &lt;lgl&gt;
## 1  0.1000000   0.1      NA
## 2  0.1494949   0.1      NA
## 3  0.1989899   0.1      NA
## 4  0.2484848   0.1      NA
## 5  0.2979798   0.1      NA
## 6  0.3474747   0.1      NA
## 7  0.3969697   0.1      NA
## 8  0.4464646   0.1      NA
## 9  0.4959596   0.1      NA
## 10 0.5454545   0.1      NA
## # ... with 9,990 more rows
```

---

# Grid approximation


```r
for (i in 1:nrow(grid_approx)) {
  grid_approx[i, 3] &lt;- 
    sum(log(normal(undulation_rate,
                   mu = grid_approx$mu[i],
                   sigma = grid_approx$sigma[i])))
}
head(grid_approx)
```

```
## # A tibble: 6 × 3
##          mu sigma   log_lik
##       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;
## 1 0.1000000   0.1 -675.9308
## 2 0.1494949   0.1 -626.4259
## 3 0.1989899   0.1 -578.8807
## 4 0.2484848   0.1 -533.2954
## 5 0.2979798   0.1 -489.6698
## 6 0.3474747   0.1 -448.0041
```

This approach is coarse and time consuming. For `\(n = 100\)`, there are 10000 comparisons.

---

# Grid approximation

&lt;img src="05-1_Applications_of_Inference_files/figure-html/unnamed-chunk-14-1.png" width="672" /&gt;

---

# Grid approximation

On this grid, the maximum likelihood estimates of `\(\mu\)` and `\(\sigma\)` are:


```r
grid_approx[which.max(grid_approx$log_lik), ]
```

```
##            mu     sigma   log_lik
## 1127 1.386869 0.3111111 -1.813363
```

---

# Maximum likelihood as an optimization problem

&lt;img src="05-1_Applications_of_Inference_files/figure-html/unnamed-chunk-16-1.png" width="672" /&gt;

---

# Maximum likelihood as an optimization problem

Define a function that takes a vector of values to optimize `x` (`\(\mu\)` and `\(\sigma\)`) as well as a set of data `Y` and returns the log-likelihood:


```r
log_lik &lt;- function(x, Y){
  probs &lt;- normal(Y, mu = x[1], sigma = x[2])
  return(sum(log(probs)))
}
```

We can now jointly optimize `\(\mu\)` and `\(\sigma\)`, maximizing the log-likelihood.

---

# Maximum likelihood as an optimization problem


```r
optim(c(0.1, 0.1), # Start at 0.1, 0.1
      log_lik,
      Y = undulation_rate,
      control = list(fnscale = -1,
                     reltol = 10^-100))
```

```
## $par
## [1] 1.3750000 0.3031089
## 
## $value
## [1] -1.802203
## 
## $counts
## function gradient 
##      287       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL
```

---

# Maximum likelihood as an optimization problem


```r
fm &lt;- glm(undulation_rate ~ 1)
coef(fm)
```

```
## (Intercept) 
##       1.375
```

```r
logLik(fm)
```

```
## 'log Lik.' -1.802203 (df=2)
```

---

# Bayesian inference of mean

Ranges of possible maximum likelihood values:

1. `\(\mu\)`: `\(-\infty &lt; \mu &lt; \infty\)`
2. `\(\sigma\)`: `\(0 &lt; \sigma &lt; \infty\)`

Drawbacks:

1. `\(\mu\)` can't be negative (no negative undulation rates) and probably isn't a large number
2. `\(\sigma\)` is also probably not huge either

Can we do better? Yes, Bayesian priors.

---

# Prior for the mean

&lt;img src="05-1_Applications_of_Inference_files/figure-html/unnamed-chunk-18-1.png" width="672" /&gt;

---

# Prior for the mean

Cauchy distribution (location = 0, scale = 2)

&lt;img src="05-1_Applications_of_Inference_files/figure-html/unnamed-chunk-19-1.png" width="672" /&gt;

---

# Bayesian model

[stan](http://mc-stan.org/) code:


```r
model &lt;- "
  data{
    int&lt;lower=1&gt; N;
    real undulation_rate[N];
  }
  parameters{
    real&lt;lower=0&gt; mu;
    real&lt;lower=0&gt; sigma;
  }
  model{
    sigma ~ cauchy(0, 5);
    mu ~ cauchy(0, 2);
    undulation_rate ~ normal(mu, sigma);
  }
"
```

---

# Sample the Bayesian model


```r
fm_priors &lt;- stan(
  model_code = model,
  data = list(undulation_rate = undulation_rate,
              N = length(undulation_rate)),
  iter = 10^4,
  warmup = 10^3,
  seed = 12)
```

```
## 
## SAMPLING FOR MODEL '339acd0649a290bdec1497a993a2112f' NOW (CHAIN 1).
## 
## Chain 1, Iteration:    1 / 10000 [  0%]  (Warmup)
## Chain 1, Iteration: 1000 / 10000 [ 10%]  (Warmup)
## Chain 1, Iteration: 1001 / 10000 [ 10%]  (Sampling)
## Chain 1, Iteration: 2000 / 10000 [ 20%]  (Sampling)
## Chain 1, Iteration: 3000 / 10000 [ 30%]  (Sampling)
## Chain 1, Iteration: 4000 / 10000 [ 40%]  (Sampling)
## Chain 1, Iteration: 5000 / 10000 [ 50%]  (Sampling)
## Chain 1, Iteration: 6000 / 10000 [ 60%]  (Sampling)
## Chain 1, Iteration: 7000 / 10000 [ 70%]  (Sampling)
## Chain 1, Iteration: 8000 / 10000 [ 80%]  (Sampling)
## Chain 1, Iteration: 9000 / 10000 [ 90%]  (Sampling)
## Chain 1, Iteration: 10000 / 10000 [100%]  (Sampling)
##  Elapsed Time: 0.083004 seconds (Warm-up)
##                0.917541 seconds (Sampling)
##                1.00055 seconds (Total)
## 
## 
## SAMPLING FOR MODEL '339acd0649a290bdec1497a993a2112f' NOW (CHAIN 2).
## 
## Chain 2, Iteration:    1 / 10000 [  0%]  (Warmup)
## Chain 2, Iteration: 1000 / 10000 [ 10%]  (Warmup)
## Chain 2, Iteration: 1001 / 10000 [ 10%]  (Sampling)
## Chain 2, Iteration: 2000 / 10000 [ 20%]  (Sampling)
## Chain 2, Iteration: 3000 / 10000 [ 30%]  (Sampling)
## Chain 2, Iteration: 4000 / 10000 [ 40%]  (Sampling)
## Chain 2, Iteration: 5000 / 10000 [ 50%]  (Sampling)
## Chain 2, Iteration: 6000 / 10000 [ 60%]  (Sampling)
## Chain 2, Iteration: 7000 / 10000 [ 70%]  (Sampling)
## Chain 2, Iteration: 8000 / 10000 [ 80%]  (Sampling)
## Chain 2, Iteration: 9000 / 10000 [ 90%]  (Sampling)
## Chain 2, Iteration: 10000 / 10000 [100%]  (Sampling)
##  Elapsed Time: 0.092344 seconds (Warm-up)
##                0.647172 seconds (Sampling)
##                0.739516 seconds (Total)
## 
## 
## SAMPLING FOR MODEL '339acd0649a290bdec1497a993a2112f' NOW (CHAIN 3).
## 
## Chain 3, Iteration:    1 / 10000 [  0%]  (Warmup)
## Chain 3, Iteration: 1000 / 10000 [ 10%]  (Warmup)
## Chain 3, Iteration: 1001 / 10000 [ 10%]  (Sampling)
## Chain 3, Iteration: 2000 / 10000 [ 20%]  (Sampling)
## Chain 3, Iteration: 3000 / 10000 [ 30%]  (Sampling)
## Chain 3, Iteration: 4000 / 10000 [ 40%]  (Sampling)
## Chain 3, Iteration: 5000 / 10000 [ 50%]  (Sampling)
## Chain 3, Iteration: 6000 / 10000 [ 60%]  (Sampling)
## Chain 3, Iteration: 7000 / 10000 [ 70%]  (Sampling)
## Chain 3, Iteration: 8000 / 10000 [ 80%]  (Sampling)
## Chain 3, Iteration: 9000 / 10000 [ 90%]  (Sampling)
## Chain 3, Iteration: 10000 / 10000 [100%]  (Sampling)
##  Elapsed Time: 0.079285 seconds (Warm-up)
##                0.599603 seconds (Sampling)
##                0.678888 seconds (Total)
## 
## 
## SAMPLING FOR MODEL '339acd0649a290bdec1497a993a2112f' NOW (CHAIN 4).
## 
## Chain 4, Iteration:    1 / 10000 [  0%]  (Warmup)
## Chain 4, Iteration: 1000 / 10000 [ 10%]  (Warmup)
## Chain 4, Iteration: 1001 / 10000 [ 10%]  (Sampling)
## Chain 4, Iteration: 2000 / 10000 [ 20%]  (Sampling)
## Chain 4, Iteration: 3000 / 10000 [ 30%]  (Sampling)
## Chain 4, Iteration: 4000 / 10000 [ 40%]  (Sampling)
## Chain 4, Iteration: 5000 / 10000 [ 50%]  (Sampling)
## Chain 4, Iteration: 6000 / 10000 [ 60%]  (Sampling)
## Chain 4, Iteration: 7000 / 10000 [ 70%]  (Sampling)
## Chain 4, Iteration: 8000 / 10000 [ 80%]  (Sampling)
## Chain 4, Iteration: 9000 / 10000 [ 90%]  (Sampling)
## Chain 4, Iteration: 10000 / 10000 [100%]  (Sampling)
##  Elapsed Time: 0.080041 seconds (Warm-up)
##                0.848947 seconds (Sampling)
##                0.928988 seconds (Total)
```

```
## Warning: There were 10 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See
## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
```

```
## Warning: Examine the pairs() plot to diagnose sampling problems
```

---

# Inspecting the samples

&lt;img src="05-1_Applications_of_Inference_files/figure-html/unnamed-chunk-21-1.png" width="672" /&gt;

---

# Summarizing the results

&lt;img src="05-1_Applications_of_Inference_files/figure-html/unnamed-chunk-22-1.png" width="672" /&gt;

---

# Summarizing the results


```
## Inference for Stan model: 339acd0649a290bdec1497a993a2112f.
## 4 chains, each with iter=10000; warmup=1000; thin=1; 
## post-warmup draws per chain=9000, total post-warmup draws=36000.
## 
##        mean se_mean    sd   2.5%   25%   50%   75% 97.5% n_eff  Rhat
## mu    1.365   0.001 0.151  1.058 1.275 1.366 1.454 1.666 14465 1.000
## sigma 0.402   0.001 0.144  0.225 0.306 0.371 0.459 0.764 11001 1.000
## lp__  3.151   0.013 1.169 -0.033 2.707 3.500 3.985 4.288  8687 1.001
## 
## Samples were drawn using NUTS(diag_e) at Fri Jan 13 12:13:10 2017.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).
```

We get a lower mean than the analytical or ML estimate (1.375) because the prior places more probability on lower values.
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github"
});
if (window.HTMLWidgets) slideshow.on('showSlide', function (slide) {setTimeout(function() {window.dispatchEvent(new Event('resize'));}, 100)});</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
